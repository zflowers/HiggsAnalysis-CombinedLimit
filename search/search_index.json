{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction These pages document the RooStats / RooFit - based software tools used for statistical analysis within the Higgs PAG - combine . Combine provides a command line interface to many different statistical techniques available inside RooFit/RooStats used widely inside CMS. The package exists in GIT under https://github.com/cms-analysis/HiggsAnalysis-CombinedLimit For more information about GIT and its usage in CMS, see http://cms-sw.github.io/cmssw/faq.html The code can be checked out from GIT and compiled on top of a CMSSW release that includes a recent RooFit/RooStats Setting up the environment and installation The instructions below are for installation within a CMSSW environment For end users that don't need to commit or do any development You can find the latest releases on github under https://github.com/cms-analysis/HiggsAnalysis-CombinedLimit/releases CC7 release CMSSW_10_2_X - recommended version Setting up the environment (once): export SCRAM_ARCH=slc7_amd64_gcc700 cmsrel CMSSW_10_2_13 cd CMSSW_10_2_13/src cmsenv git clone https://github.com/cms-analysis/HiggsAnalysis-CombinedLimit.git HiggsAnalysis/CombinedLimit cd HiggsAnalysis/CombinedLimit Update to a recommended tag - currently the recommended tag is v8.2.0 : see release notes cd $CMSSW_BASE/src/HiggsAnalysis/CombinedLimit git fetch origin git checkout v8.2.0 scramv1 b clean; scramv1 b # always make a clean build SLC6/CC7 release CMSSW_8_1_X Setting up the environment (once): # For CC7: export SCRAM_ARCH=slc7_amd64_gcc530 # For SLC6: export SCRAM_ARCH=slc6_amd64_gcc530 cmsrel CMSSW_8_1_0 cd CMSSW_8_1_0/src cmsenv git clone https://github.com/cms-analysis/HiggsAnalysis-CombinedLimit.git HiggsAnalysis/CombinedLimit cd HiggsAnalysis/CombinedLimit Update to a reccomended tag - currently the reccomended tag is v7.0.13 : cd $CMSSW_BASE/src/HiggsAnalysis/CombinedLimit git fetch origin git checkout v7.0.13 scramv1 b clean; scramv1 b # always make a clean build Standalone version The standalone version can be easily compiled using the \\verb@cvmfs@ as it relies on dependencies which are already installed at /cvmfs/cms.cern.ch/ . Access to /cvmfs/cms.cern.ch/ can be obtained from lxplus machines or via CernVM , by adding the CMS group to the CVMFS Configuration. A minimal CernVM working context setup can be found in the CernVM Marketplace under Experimental/HiggsCombine or at https://cernvm-online.cern.ch/context/view/9ee5960ce4b143f5829e72bbbb26d382 . At least 2GB of disk space should be reserved on the virtual machine for Combine to work properly. In case you do not want to use the cvmfs area, you will need to adapt the location of the dependencies listed in both the Makefile and env_standalone.sh files. git clone https://github.com/cms-analysis/HiggsAnalysis-CombinedLimit.git HiggsAnalysis/CombinedLimit cd HiggsAnalysis/CombinedLimit/ git fetch origin git checkout v8.1.0 . env_standalone.sh make You will need to source env_standalone.sh each time you want to use the package, or add it to your login. What has changed between tags? You can generate a diff of any two tags (eg for v7.0.8 and v7.0.6 ) by using following the url: https://github.com/cms-analysis/HiggsAnalysis-CombinedLimit/compare/v7.0.6...v7.0.7 Replace the tag names in the url to any tags you which to compare. For developers We use the Fork and Pull model for development: each user creates a copy of the repository on github, commits their requests there and then sends pull requests for the administrators to merge. Prerequisites Register on github, as needed anyway for CMSSW development: http://cms-sw.github.io/cmssw/faq.html Register your SSH key on github: https://help.github.com/articles/generating-ssh-keys 1 Fork the repository to create your copy of it: https://github.com/cms-analysis/HiggsAnalysis-CombinedLimit/fork (more documentation at https://help.github.com/articles/fork-a-repo ) You will now be able to browse your fork of the repository from https://github.com/your-github-user-name/HiggsAnalysis-CombinedLimit Recommended way to develop a feature (in a branch) # get the updates of the master branch of the remote repository git fetch upstream # branch straight off the upstream master git checkout -b feature_name_branch upstream/81x-root606 # implement the feature # commit, etc # before publishing: # get the updates of the master branch of the remote repository git fetch upstream # if you're ready to integrate the upstream changes into your repository do git rebase upstream/81x-root606 # fix any conflicts git push origin feature_name_branch And proceed to make a pull request from the branch you created. Committing changes to your repository git add .... git commit -m \"....\" git push You can now make a pull request to the repository. Combine Tool An additional tool for submitting combine jobs to batch/crab, developed originally for HiggsToTauTau. Since the repository contains a certain amount of analysis-specific code, the following scripts can be used to clone it with a sparse checkout for just the core CombineHarvester/CombineTools subpackage, speeding up the checkout and compile times: git clone via ssh: bash <(curl -s https://raw.githubusercontent.com/cms-analysis/CombineHarvester/master/CombineTools/scripts/sparse-checkout-ssh.sh) git clone via https: bash <(curl -s https://raw.githubusercontent.com/cms-analysis/CombineHarvester/master/CombineTools/scripts/sparse-checkout-https.sh) make sure to run scram to compile the CombineTools package. See the CombineHarvester documentation pages for more details on using this tool and additional features available in the full package. Standalone version of combine Combine is also released as a standalone package, still meant to be run on SLC7 machines. Please note that while the CMSSW version is maintained regularly, the same might not be true for the standalone version. For CMS members, we advise to use the CMSSW version when possible. To compile the standalone version on suitable machines, please run: git clone https://github.com/cms-analysis/HiggsAnalysis-CombinedLimit.git HiggsAnalysis/CombinedLimit cd HiggsAnalysis/CombinedLimit/ git fetch origin git checkout v8.2.0 . env_standalone.sh make You will need to source env_standalone.sh each time you want to use the package, or add it to your login. Available machines for standalone combine The standalone version can be easily compiled via CVMFS as it relies on dependencies which are already installed at /cvmfs/cms.cern.ch/ . Access to /cvmfs/cms.cern.ch/ can be obtained from lxplus machines or via CernVM . The only requirement will be to add the CMS group to the CVMFS configuration as shown in the picture At least 2GB of disk space should be reserved on the virtual machine for combine to work properly. A minimal CernVM working context setup can be found in the CernVM Marketplace under Experimental/HiggsCombine . To use this predefined context, first locally launch the CernVM (eg you can use the .ova with VirtualBox, by downloading from here and launching the downloaded file. You can click on \"pair an instance of CernVM\" from the cernvm-online dashboard, which displays a PIN. In the VirtualBox terminal, pair the virtual machine with this PIN code (enter in the terminal using #PIN eg #123456 . After this, you will be asked again for username (use user ) and then a password (use hcomb ). In case you do not want to use the cvmfs area, you will need to adapt the location of the dependencies listed in both the Makefile and env_standalone.sh files.","title":"Home"},{"location":"#introduction","text":"These pages document the RooStats / RooFit - based software tools used for statistical analysis within the Higgs PAG - combine . Combine provides a command line interface to many different statistical techniques available inside RooFit/RooStats used widely inside CMS. The package exists in GIT under https://github.com/cms-analysis/HiggsAnalysis-CombinedLimit For more information about GIT and its usage in CMS, see http://cms-sw.github.io/cmssw/faq.html The code can be checked out from GIT and compiled on top of a CMSSW release that includes a recent RooFit/RooStats","title":"Introduction"},{"location":"#setting-up-the-environment-and-installation","text":"The instructions below are for installation within a CMSSW environment","title":"Setting up the environment and installation"},{"location":"#for-end-users-that-dont-need-to-commit-or-do-any-development","text":"You can find the latest releases on github under https://github.com/cms-analysis/HiggsAnalysis-CombinedLimit/releases","title":"For end users that don't need to commit or do any development"},{"location":"#cc7-release-cmssw_10_2_x-recommended-version","text":"Setting up the environment (once): export SCRAM_ARCH=slc7_amd64_gcc700 cmsrel CMSSW_10_2_13 cd CMSSW_10_2_13/src cmsenv git clone https://github.com/cms-analysis/HiggsAnalysis-CombinedLimit.git HiggsAnalysis/CombinedLimit cd HiggsAnalysis/CombinedLimit Update to a recommended tag - currently the recommended tag is v8.2.0 : see release notes cd $CMSSW_BASE/src/HiggsAnalysis/CombinedLimit git fetch origin git checkout v8.2.0 scramv1 b clean; scramv1 b # always make a clean build","title":"CC7 release CMSSW_10_2_X - recommended version"},{"location":"#slc6cc7-release-cmssw_8_1_x","text":"Setting up the environment (once): # For CC7: export SCRAM_ARCH=slc7_amd64_gcc530 # For SLC6: export SCRAM_ARCH=slc6_amd64_gcc530 cmsrel CMSSW_8_1_0 cd CMSSW_8_1_0/src cmsenv git clone https://github.com/cms-analysis/HiggsAnalysis-CombinedLimit.git HiggsAnalysis/CombinedLimit cd HiggsAnalysis/CombinedLimit Update to a reccomended tag - currently the reccomended tag is v7.0.13 : cd $CMSSW_BASE/src/HiggsAnalysis/CombinedLimit git fetch origin git checkout v7.0.13 scramv1 b clean; scramv1 b # always make a clean build","title":"SLC6/CC7 release CMSSW_8_1_X"},{"location":"#standalone-version","text":"The standalone version can be easily compiled using the \\verb@cvmfs@ as it relies on dependencies which are already installed at /cvmfs/cms.cern.ch/ . Access to /cvmfs/cms.cern.ch/ can be obtained from lxplus machines or via CernVM , by adding the CMS group to the CVMFS Configuration. A minimal CernVM working context setup can be found in the CernVM Marketplace under Experimental/HiggsCombine or at https://cernvm-online.cern.ch/context/view/9ee5960ce4b143f5829e72bbbb26d382 . At least 2GB of disk space should be reserved on the virtual machine for Combine to work properly. In case you do not want to use the cvmfs area, you will need to adapt the location of the dependencies listed in both the Makefile and env_standalone.sh files. git clone https://github.com/cms-analysis/HiggsAnalysis-CombinedLimit.git HiggsAnalysis/CombinedLimit cd HiggsAnalysis/CombinedLimit/ git fetch origin git checkout v8.1.0 . env_standalone.sh make You will need to source env_standalone.sh each time you want to use the package, or add it to your login.","title":"Standalone version"},{"location":"#what-has-changed-between-tags","text":"You can generate a diff of any two tags (eg for v7.0.8 and v7.0.6 ) by using following the url: https://github.com/cms-analysis/HiggsAnalysis-CombinedLimit/compare/v7.0.6...v7.0.7 Replace the tag names in the url to any tags you which to compare.","title":"What has changed between tags?"},{"location":"#for-developers","text":"We use the Fork and Pull model for development: each user creates a copy of the repository on github, commits their requests there and then sends pull requests for the administrators to merge. Prerequisites Register on github, as needed anyway for CMSSW development: http://cms-sw.github.io/cmssw/faq.html Register your SSH key on github: https://help.github.com/articles/generating-ssh-keys 1 Fork the repository to create your copy of it: https://github.com/cms-analysis/HiggsAnalysis-CombinedLimit/fork (more documentation at https://help.github.com/articles/fork-a-repo ) You will now be able to browse your fork of the repository from https://github.com/your-github-user-name/HiggsAnalysis-CombinedLimit","title":"For developers"},{"location":"#recommended-way-to-develop-a-feature-in-a-branch","text":"# get the updates of the master branch of the remote repository git fetch upstream # branch straight off the upstream master git checkout -b feature_name_branch upstream/81x-root606 # implement the feature # commit, etc # before publishing: # get the updates of the master branch of the remote repository git fetch upstream # if you're ready to integrate the upstream changes into your repository do git rebase upstream/81x-root606 # fix any conflicts git push origin feature_name_branch And proceed to make a pull request from the branch you created.","title":"Recommended way to develop a feature (in a branch)"},{"location":"#committing-changes-to-your-repository","text":"git add .... git commit -m \"....\" git push You can now make a pull request to the repository.","title":"Committing changes to your repository"},{"location":"#combine-tool","text":"An additional tool for submitting combine jobs to batch/crab, developed originally for HiggsToTauTau. Since the repository contains a certain amount of analysis-specific code, the following scripts can be used to clone it with a sparse checkout for just the core CombineHarvester/CombineTools subpackage, speeding up the checkout and compile times: git clone via ssh: bash <(curl -s https://raw.githubusercontent.com/cms-analysis/CombineHarvester/master/CombineTools/scripts/sparse-checkout-ssh.sh) git clone via https: bash <(curl -s https://raw.githubusercontent.com/cms-analysis/CombineHarvester/master/CombineTools/scripts/sparse-checkout-https.sh) make sure to run scram to compile the CombineTools package. See the CombineHarvester documentation pages for more details on using this tool and additional features available in the full package.","title":"Combine Tool"},{"location":"#standalone-version-of-combine","text":"Combine is also released as a standalone package, still meant to be run on SLC7 machines. Please note that while the CMSSW version is maintained regularly, the same might not be true for the standalone version. For CMS members, we advise to use the CMSSW version when possible. To compile the standalone version on suitable machines, please run: git clone https://github.com/cms-analysis/HiggsAnalysis-CombinedLimit.git HiggsAnalysis/CombinedLimit cd HiggsAnalysis/CombinedLimit/ git fetch origin git checkout v8.2.0 . env_standalone.sh make You will need to source env_standalone.sh each time you want to use the package, or add it to your login.","title":"Standalone version of combine"},{"location":"#available-machines-for-standalone-combine","text":"The standalone version can be easily compiled via CVMFS as it relies on dependencies which are already installed at /cvmfs/cms.cern.ch/ . Access to /cvmfs/cms.cern.ch/ can be obtained from lxplus machines or via CernVM . The only requirement will be to add the CMS group to the CVMFS configuration as shown in the picture At least 2GB of disk space should be reserved on the virtual machine for combine to work properly. A minimal CernVM working context setup can be found in the CernVM Marketplace under Experimental/HiggsCombine . To use this predefined context, first locally launch the CernVM (eg you can use the .ova with VirtualBox, by downloading from here and launching the downloaded file. You can click on \"pair an instance of CernVM\" from the cernvm-online dashboard, which displays a PIN. In the VirtualBox terminal, pair the virtual machine with this PIN code (enter in the terminal using #PIN eg #123456 . After this, you will be asked again for username (use user ) and then a password (use hcomb ). In case you do not want to use the cvmfs area, you will need to adapt the location of the dependencies listed in both the Makefile and env_standalone.sh files.","title":"Available machines for standalone combine"},{"location":"full-documentation/","text":"Full documentation Common options Choice of the prior for bayesian methods For methods based on bayesian inference (BayesianSimple, MarkovChainMC) you can specify a prior for the signal strength. This is controlled through option --prior , and the values that are currently allowed are: uniform : flat prior 1/sqrt(r) : prior proportional to 1/sqrt(r) , where r is the signal strength; for a counting experiment with a single channel and no systematics, this is Jeffrey's prior. Note that you might have to put the 1/sqrt(r) within quotes because for some shells the braces have a special meaning. If no prior is specified, a flat prior is assumed. Algorithm-specific options #ProfiLe ProfileLikelihood algorithm The ProfileLikelihood algorithm has only two options besides the common ones: the choice of the minimizer algorith (Minuit2 or Minuit), and the choice of the tolerance. If you see that the limit fails with one of the minimizer, try the other. Sometimes this problem happens due to numerical instabilities even if the model itself looks perfectly well behaved. If neither of the two succeeds, another possible way of circumventing the instability is sometimes to change the range of r by a small amount, or to change slightly one number in the model (e.g. in one simple counting experiment a test was failing when assuming \u0394B/B = 0.3, but was succeeding for \u0394B/B = 0.301 and \u0394B/B = 0.299, giving almost the same result) In case you experience numerical instabilities, e.g. failures or bogus results, you could be able to work around the problem by performing the minimization multiple times and requiring the results to be consistent. This can be done using the options below. maxTries : how many times the program should attempt to find a minimum at most (plausible range: 20-100) tries : how many times the program should succeed in computing a limit (plausible range: 5-10) maxOutlierFraction : maximum fraction of bad results within the tries ones (plausible: 0.15-0.30; default 0.25) maxOutliers : maximum number of bogus results before giving up for this point (plausible values: 2-5; default 3) preFit : don't try to profile the likelihood if Minuit already gave errors in finding the minimum. (suggested) BayesianSimple algorithm The BayesianSimple algorithm doesn't have parameters besides the ones specified above under the \"Common options\" section. #MarkoV MarkovChainMC algorithm This algorithm has many configurable parameters, and you're encouraged to experiment with those because the default configuration might not be the best for you (or might not even work for you at all) *Iterations, burn-in, tries* Three parameters control how the MCMC integration is performed: the number of tries (option --tries ): the algorithm will run multiple times with different ransom seeds and report as result the truncated mean and rms of the different results. The default value is 10, which should be ok for a quick computation, but for something more accurate you might want to increase this number even up to ~200. the number of iterations (option -i ) determines how many points are proposed to fill a single Markov Chain. The default value is 10k, and a plausible range is between 5k (for quick checks) and 20-30k for lengthy calculations. Usually beyond 30k you get a better tradeoff in time vs accuracy by increasing the number of chains (option tries ) the number of burn-in steps (option -b ) is the number of points that are removed from the beginning of the chain before using it to compute the limit. IThe default is 200. If your chain is very long, you might want to try increase this a bit (e.g. to some hundreds). Instead going below 50 is probably dangerous. Proposal The option --proposal controls the way new points are proposed to fill in the MC chain. uniform : pick points at random. This works well if you have very few nuisance parameters (or none at all), but normally fails if you have many. gaus : Use a product of independent gaussians one for each nuisance parameter; the sigma of the gaussian for each variable is 1/5 of the range of the variable (this can be controlled using the parameter propHelperWidthRangeDivisor ). This proposal appears to work well for a reasonable number of nuisances (up to ~15), provided that the range of the nuisance parameters is reasonable, like \u00b15\u03c3. It does not work without systematics. =ortho ( default ): (previously known as test ) This proposalis similar to the multi-gaussian proposal but at every step only a single coordinate of the point is varied, so that the acceptance of the chain is high even for a large number of nuisances (i.e. more than 20). fit : Run a fit and use the uncertainty matrix from HESSE to construct a proposal (or the one from MINOS if the option --runMinos is specified). This sometimes work fine, but sometimes gives biased results, so we don't recommend it in general. If you believe there's something going wrong, e.g. if your chain remains stuck after accepting only a few events, the option debugProposal can be used to have a printout of the first N proposed points to see what's going on (e.g. if you have some region of the phase space with probability zero, the gaus and fit proposal can get stuck there forever) #HybridNew HybridNew algorithm Type of limit By default, HybridNew computes hybrid bayesian-frequentist limits. If one specifies the command line option --freqentist then it will instead compute the fully frequentist limits. Rule (option --rule ) The rule defines how the distribution of the test statistics is used to compute a limit. When using the CL<sub>s+b</sub> rule the limit to the value of the signal strength for which 95% of the pseudo-experiments give a result more signal-like than the current one, <nobr> P(x < x<sub>obs</sub>|r*s+b) = 1 - CL </nobr>. For the more conservative CL<sub>s</sub> rule, the limit is set by the condition <nobr> P(x < x<sub>obs</sub>|r*s+b) /P(x < x<sub>obs</sub>|b) = 1 - CL </nobr> . The default rule is CL<sub>s</sub> . Test statistics (option --testStat ) The test statistics is the measure of how signal-like or background-like is an observation. The following test statistics are provided: Simple Likelihood Ratio (option value LEP or SLR ): The LEP-like ratio of likelihoods ( L(x|r*s+b,\u03b8) / L(x|b, \u03b8) ) where numerator and denominator are evaluated for the reference values of the nuisance parameters \u03b8. Ratio of Profiled Likelihoods (option value TEV or ROPL ): The Tevatron-like ratio of profiled likelihoods, in which before computing each of the likelihoods is maximised as function of the nuisance parameters ( max<sub>\u03b8</sub> L(x|r*s+b,\u03b8) / max<sub>\u03b8</sub> L(x|b, \u03b8) ). Profile Likelihood modified for upper limits (option value LHC or MPL ): The LHC-like (or Atlas-like) profiled likelihood in which the maximization of the likelihood is done also in the signal strength ( max<sub>\u03b8</sub> L(x|r*s+b,\u03b8) / max<sub>r', \u03b8</sub> L(x|r'*s+b,\u03b8) ), with the constraints 0 \u2264 r' \u2264 r where the upper bound is applied to force the method to always give an upper limit and not a two-sided interval. Profile Likelihood (not modified for upper limits) (option value PL ): The traditional profiled likelihood in which the maximization of the likelihood is done also in the signal strength ( max<sub>\u03b8</sub>L(x|r*s+b,\u03b8) / max<sub>r', \u03b8</sub>L(x|x|r'*s+b,\u03b8) ), with just the physical constraints 0 \u2264 r' This test statistics can give two-sided limits, as it starts decreasing when the number of observed events is above the expectations from the signal+background hypothesis. The default value when computing hybrid bayesian-frequentist limits is LEP . The default value when computing frequentist limits is LHC . Accuracy The search for the limit is performed using an adaptive algorithm, terminating when the estimate of the limit value is below some limit or when the precision cannot be futher improved with the specified options. The options controlling this behaviour are: rAbsAcc , rRelAcc : define the accuracy on the limit at which the search stops. The default values are 0.1 and 0.05 respectively, meaning that the search is stopped when \u0394r < 0.1 or \u0394r/r < 0.05. clsAcc : this determines the absolute accuracy up to which the CLs values are computed when searching for the limit. The default is 0.5%. Raising the accuracy above this value will increase significantly the time to run the algorithm, as you need N<sup>2</sup> more toys to improve the accuracy by a factor <sub>N</sub>; you can consider enlarging this value if you're computing limits with a larger CL (e.g. 90% or 68%) Note that if you're using the CL<sub>s+b</sub> rule then this parameter will control the uncertainty on CL<sub>s+b</sub> , not on CL<sub>s</sub> . T or toysH : controls the minimum number of toys that are generated for each point. The default value of 500 should be ok when computing the limit with 90-95% CL. You can decrease this number if you're computing limits at 68% CL, or increase it if you're using 99% CL. Computing significances When computing the significances, there is no adaptive generation. You can control the number of toys with option T or toysH= and the option iterations (shortcut -i , default 1): the default of (1 iteration)\u00d7(500 toys) is not enough to probe a significances above ~2. We suggest that you uncrease the number of iterations instead of the number of toys, since the increase in time is linear with the iterations but non-linear with the toys. In order to compute the significance in multiple jobs, proceed as follows: Run N different jobs with the same inputs but different random seed (option -s ), specifying the additional option --saveHybridResult . Use hadd to merge the output root files in a single one. The program will complain with messages like Cannot merge object type, name: HybridCalculator _result which can be safely ignored. Compute the significance from the merged file running again but with options --readHybridResults= and --toysFile=<merged.root> Caveat: there is no check within the program that you're using consistent inputs for the different jobs. Simple hypotesis testing : Sometimes you just want to compute the p-values of the background-only hypotesis and of the signal plus background hypotesis for a fixed value of the signal strength. This can be done specifying the option singlePoint <value> which will set the signal strength to that value and run the hypothesis test. It will generate toys until the required accuracy is met (see above for parameter clsAcc ). You can turn off adaptive generation setting clsAcc to zero, and then it will generate the toys once (or multiple times if you set option iterations to a value larger than 1). Just like for significance, you can run multiple times with different seeds and options --saveHybridResult , combine the output files with hadd and then compute the final result with --readHybridResult --toysFile=merged.root Performance issues The current hybrid code requires a lot of cpu resources. You can speed up the processing by using multiple cores (option fork , default value is 1). Note that even with fork set to 1, toy generation is done in a separate thread to avoid memory leaks. If you want to run in a single thread, e.g. to be able to read the debug output during generation, you should set the option to zero. If running multi-threaded on the cern batch cluster, you should declare it to the bsub command when submitting the jobs: e.g. for a job that uses four cores you should use bsub -n 4 -R \"'span[hosts=1]'\" ... #HybridNewGrid HybridNew algorithm usage for complex models or expected limits: grids If your model is complex, or you need to know the limit accurately, or you want expected limits, then running the computation in a single job might not be feasible. The alternative approach is to compute a grid of distributions of the test statistics for various values of the signal strength, a task that is easy to parallelize, and then use the that grid to compute the observed limit (and also the expected ones). This requires you to have some knowledge of where the limit should be, which you can gain e.g. from the ProfileLikelihood method Creating the grid: manual way The procedure to do this manually would be like the procedure for significances or simple hypothesis testing described previously: for each value r_i of the cross section, you write out one file with the distribution of the test statistics using combine card.txt -M HybridNew [--freq] [other options] -s seed_i --singlePoint r_i --saveToys --saveHybridResult and then you can merge all the output files for the different r_i with hadd . The [other options] should include --clsAcc 0 to switch off adaptive sampling, and you can tune the CPU time by working on the parameters -T and --iterations . It is important that you use different seed_i values for each point; if you don't care about exact reproducibility, you can just use --seed -1 and the code will take care of randomizing itself properly. Creating the grid: automated way, using CRAB Please note that the following is intended for use with crab2. For producing the grid with crab3, please see the instructions here Once you have a sense of the time needed for each toy, and of the range to consider, you can use the script makeGridUsingCrab.py to run the toys to create the grid in parallel either on LXBATCH or on regular T2s (or anything else that CRAB can digest). The procedure is very simple: makeGridUsingCrab.py card.txt minimum maximum -n points [other options] -o name This will create a crab cfg file name.cfg and a script name.sh , and possibly a binary workspace name.workspace.root . You can then just create and submit the jobs from that cfg file, and merge the output rootfiles with hadd=(note: =hadd will complain with messages like Cannot merge object type, name: HybridCalculator _result which can be safely ignored). The other options, that you can get executing the program with --help are: -T : same as in combine -r : use a random seed in each job (suggested) -I n : run only on 1/n of the points in each job (suggested if you want to have many points) -t , -j : choose the total number of toys and of jobs (can change later from the crab cfg file) --lsf , --queue ... : use lxbatch with the specific queue (can change later from the crab cfg file) Note that you can merge also the output of multiple crab submissions, if you have used random seeds. Using the grid for observed limits combine mydatcard.txt -M HybridNew [--freq] --grid=mygrid.root All other parameters controlling toys, accuracy and so on are meaningless in this context. Note that it might still take a while if you have many points and the test statistics is slow to evaluate. Add the option --saveGrid to save the value of the observed CLs at each grid point in the output tree. Using the grid for expected limits combine mydatcard.txt -M HybridNew [--freq] --grid=mygrid.root --expectedFromGrid 0.5 0.5 gives you the median. use 0.16/0.84 to get the endpoints of 68% interval, 0.025/0.975 to get the 95% one). Add the option --saveGrid to save the value of the expected quantile CLs at each grid point in the output tree. Plotting the test-statistics distributions The distribution of the test-statistic under the signal plus background and background only hypotheses can be plotted at each value of the grid using the following; python test/plotTestStatCLs.py --input mygrid.root --poi r --val all --mass MASS The output root file will contain a plot for each point found in the grid. FeldmanCousins The F-C method is used to compute an interval with the specified confidence level. If you run the model without special options, it will report the upper limit to the signal strength. If you want instead the lower end of the interval, just run it with option lowerLimit . The algorithm will search for a limit with an iterative procedure until the specified absolute or relative accuracy is met, as controlled by the parameters rAbsAcc , =rRelAcc . The default values are 0.1 and 0.05 respectively, meaning that the search is stopped when \u0394r < 0.1 or \u0394r/r < 0.05. The number of toys generated is adaptive. You can increase it by a factor using option toysFactor a value of 2 or 4 is suggested if you want to compute the limit with high accuracy. Running under CRAB The instructions below are for use with crab2 . For instructions on how to use the grid for toy based studies or complicated model scans under crab3 , follow the instructions given here . Running many toy MC for the limit calculation may be conveniently split among the different available GRID resources using CRAB. Examples of how to run on the GRID via CRAB are provided in the files: [[https://github.com/cms-analysis/HiggsAnalysis-CombinedLimit/blob/master/test/combine_crab.sh][combine_crab.sh]] [[https://github.com/cms-analysis/HiggsAnalysis-CombinedLimit/blob/master/test/combine_crab.cfg][combine_crab.cfg]] Preparing the ROOT workspace The first thing to do is to convert the datacards and possibly the shape model into a ROOT workspace. This model will be shipped to Worker Nodes for GRID processing. This is done via the utility script text2workspace.py . For instance: ../scripts/text2workspace.py ../data/benchmarks/simple-counting/counting-B0-Obs0-StatOnly.txt -b -o model.root Shell script for GRID Worker Nodes CRAB is designed mainly to provide automatic cmsRun job splitting providing the number of jobs one wants to run, and the number of 'events' in total one wants to process.The total number of toy MC we want to run. The maximum number of events is passed to the application to be executed via the variable $MaxEvents . In our case, we will use for convenience $MaxEvents as the number of toy MC to run per job. The script [[https://github.com/cms-analysis/HiggsAnalysis-CombinedLimit/blob/master/test/combine_crab.sh][combine_crab.sh]] runs the combiner code with the proper options, and prepares the output to be retrieved after the run completion on the Worker Nodes. It takes as argument the job indes ( $1 ), which we use as random seed. The main elements there are running the combiner and packing the output for final retrieval: echo \"job number: seed # i with n toys\" ./combine model.root -t n toys\" ./combine model.root -t n -sn -s i with <span class=\"arithmatex\"><span class=\"MathJax_Preview\">n toys\" ./combine model.root -t</span><script type=\"math/tex\">n toys\" ./combine model.root -t n -sn -s i >& log.txt mv *.root outputToy/ mv log.txt outputToy/ echo \"pack the results\" tar cvfz outputToy.tgz outputToy/ CRAB configuration script Then, the script = [[https://github.com/cms-analysis/HiggsAnalysis-CombinedLimit/blob/master/test/combine_crab.cfg][combine_crab.cfg]]= sets how many jobs to run and how many toys per job. And finally, which files to ship to the Worker Nodes (the executable itself and the ROOT workspace), and which file to retrieve (the packed output): [CRAB] jobtype = cmssw scheduler = glite [CMSSW] output_file = outputToy.tgz datasetpath=None pset=None total_number_of_events=100 number_of_jobs=10 [USER] script_exe = combine_crab.sh additional_input_files = combine, model.root return_data = 1 Please, notice that you have to ship to Worker Nodes the executable itself, so, before running you have to do: ln -s ../../../../bin/slc5_amd64_gcc434/combine combine GRID submission via CRAB CRAB submission can be now performed as usually done for other CMSSW analysis application: create your jobs: crab -create -cfg combine_crab.cfg submit your jobs: crab -submit monitor your jobs' status: crab -status after jobs are finished, retrieve the output: crab -getoutput The output consists in several files outputToy_n_m_xyz.tgz that contain the output of each job, including the log files, that can be combined and analyzed to obtain the final result. Useful Links and tutorials Tutorial Sessions 1st tutorial 17th Nov 2015 . 2nd tutorial 30th Nov 2016 . Worked examples from Higgs analyses Further Reading and Advanced usage tutorials Conventions to be used when preparing inputs for Higgs combinations CMS AN-2011/298: Procedure for the LHC Higgs boson search combination in summer 2011 (describing some of the methods used in combine ) Combine-based packages SWGuideHiggs2TauLimits ATGCRooStats CombineHarvester Contacts Hypernews forum : hn-cms-higgs-combination https://hypernews.cern.ch/HyperNews/CMS/get/higgs-combination.html #ReviewStatus Review status Reviewer/Editor and Date (copy from screen) Comments Main.GiovanniPetrucciani - 14-Dec-2010 created template page Main.GiovanniPetrucciani - 21-Mar-2011 new tag Main.LucaLista - 04-May-2011 added CRAB submission Main.GiovanniPetrucciani - 10-Jun-2011 Frequentist limits Main.GiovanniPetrucciani - 10-Jun-2011 Frequentist grids and expected limits Main.GiovanniPetrucciani Main.GiovanniPetrucciani","title":"Full documentation"},{"location":"full-documentation/#full-documentation","text":"","title":"Full documentation"},{"location":"full-documentation/#common-options","text":"","title":"Common options"},{"location":"full-documentation/#choice-of-the-prior-for-bayesian-methods","text":"For methods based on bayesian inference (BayesianSimple, MarkovChainMC) you can specify a prior for the signal strength. This is controlled through option --prior , and the values that are currently allowed are: uniform : flat prior 1/sqrt(r) : prior proportional to 1/sqrt(r) , where r is the signal strength; for a counting experiment with a single channel and no systematics, this is Jeffrey's prior. Note that you might have to put the 1/sqrt(r) within quotes because for some shells the braces have a special meaning. If no prior is specified, a flat prior is assumed.","title":"Choice of the prior for bayesian methods"},{"location":"full-documentation/#algorithm-specific-options","text":"#ProfiLe","title":"Algorithm-specific options"},{"location":"full-documentation/#profilelikelihood-algorithm","text":"The ProfileLikelihood algorithm has only two options besides the common ones: the choice of the minimizer algorith (Minuit2 or Minuit), and the choice of the tolerance. If you see that the limit fails with one of the minimizer, try the other. Sometimes this problem happens due to numerical instabilities even if the model itself looks perfectly well behaved. If neither of the two succeeds, another possible way of circumventing the instability is sometimes to change the range of r by a small amount, or to change slightly one number in the model (e.g. in one simple counting experiment a test was failing when assuming \u0394B/B = 0.3, but was succeeding for \u0394B/B = 0.301 and \u0394B/B = 0.299, giving almost the same result) In case you experience numerical instabilities, e.g. failures or bogus results, you could be able to work around the problem by performing the minimization multiple times and requiring the results to be consistent. This can be done using the options below. maxTries : how many times the program should attempt to find a minimum at most (plausible range: 20-100) tries : how many times the program should succeed in computing a limit (plausible range: 5-10) maxOutlierFraction : maximum fraction of bad results within the tries ones (plausible: 0.15-0.30; default 0.25) maxOutliers : maximum number of bogus results before giving up for this point (plausible values: 2-5; default 3) preFit : don't try to profile the likelihood if Minuit already gave errors in finding the minimum. (suggested)","title":"ProfileLikelihood algorithm"},{"location":"full-documentation/#bayesiansimple-algorithm","text":"The BayesianSimple algorithm doesn't have parameters besides the ones specified above under the \"Common options\" section. #MarkoV","title":"BayesianSimple algorithm"},{"location":"full-documentation/#markovchainmc-algorithm","text":"This algorithm has many configurable parameters, and you're encouraged to experiment with those because the default configuration might not be the best for you (or might not even work for you at all) *Iterations, burn-in, tries* Three parameters control how the MCMC integration is performed: the number of tries (option --tries ): the algorithm will run multiple times with different ransom seeds and report as result the truncated mean and rms of the different results. The default value is 10, which should be ok for a quick computation, but for something more accurate you might want to increase this number even up to ~200. the number of iterations (option -i ) determines how many points are proposed to fill a single Markov Chain. The default value is 10k, and a plausible range is between 5k (for quick checks) and 20-30k for lengthy calculations. Usually beyond 30k you get a better tradeoff in time vs accuracy by increasing the number of chains (option tries ) the number of burn-in steps (option -b ) is the number of points that are removed from the beginning of the chain before using it to compute the limit. IThe default is 200. If your chain is very long, you might want to try increase this a bit (e.g. to some hundreds). Instead going below 50 is probably dangerous. Proposal The option --proposal controls the way new points are proposed to fill in the MC chain. uniform : pick points at random. This works well if you have very few nuisance parameters (or none at all), but normally fails if you have many. gaus : Use a product of independent gaussians one for each nuisance parameter; the sigma of the gaussian for each variable is 1/5 of the range of the variable (this can be controlled using the parameter propHelperWidthRangeDivisor ). This proposal appears to work well for a reasonable number of nuisances (up to ~15), provided that the range of the nuisance parameters is reasonable, like \u00b15\u03c3. It does not work without systematics. =ortho ( default ): (previously known as test ) This proposalis similar to the multi-gaussian proposal but at every step only a single coordinate of the point is varied, so that the acceptance of the chain is high even for a large number of nuisances (i.e. more than 20). fit : Run a fit and use the uncertainty matrix from HESSE to construct a proposal (or the one from MINOS if the option --runMinos is specified). This sometimes work fine, but sometimes gives biased results, so we don't recommend it in general. If you believe there's something going wrong, e.g. if your chain remains stuck after accepting only a few events, the option debugProposal can be used to have a printout of the first N proposed points to see what's going on (e.g. if you have some region of the phase space with probability zero, the gaus and fit proposal can get stuck there forever) #HybridNew","title":"MarkovChainMC algorithm"},{"location":"full-documentation/#hybridnew-algorithm","text":"Type of limit By default, HybridNew computes hybrid bayesian-frequentist limits. If one specifies the command line option --freqentist then it will instead compute the fully frequentist limits. Rule (option --rule ) The rule defines how the distribution of the test statistics is used to compute a limit. When using the CL<sub>s+b</sub> rule the limit to the value of the signal strength for which 95% of the pseudo-experiments give a result more signal-like than the current one, <nobr> P(x < x<sub>obs</sub>|r*s+b) = 1 - CL </nobr>. For the more conservative CL<sub>s</sub> rule, the limit is set by the condition <nobr> P(x < x<sub>obs</sub>|r*s+b) /P(x < x<sub>obs</sub>|b) = 1 - CL </nobr> . The default rule is CL<sub>s</sub> . Test statistics (option --testStat ) The test statistics is the measure of how signal-like or background-like is an observation. The following test statistics are provided: Simple Likelihood Ratio (option value LEP or SLR ): The LEP-like ratio of likelihoods ( L(x|r*s+b,\u03b8) / L(x|b, \u03b8) ) where numerator and denominator are evaluated for the reference values of the nuisance parameters \u03b8. Ratio of Profiled Likelihoods (option value TEV or ROPL ): The Tevatron-like ratio of profiled likelihoods, in which before computing each of the likelihoods is maximised as function of the nuisance parameters ( max<sub>\u03b8</sub> L(x|r*s+b,\u03b8) / max<sub>\u03b8</sub> L(x|b, \u03b8) ). Profile Likelihood modified for upper limits (option value LHC or MPL ): The LHC-like (or Atlas-like) profiled likelihood in which the maximization of the likelihood is done also in the signal strength ( max<sub>\u03b8</sub> L(x|r*s+b,\u03b8) / max<sub>r', \u03b8</sub> L(x|r'*s+b,\u03b8) ), with the constraints 0 \u2264 r' \u2264 r where the upper bound is applied to force the method to always give an upper limit and not a two-sided interval. Profile Likelihood (not modified for upper limits) (option value PL ): The traditional profiled likelihood in which the maximization of the likelihood is done also in the signal strength ( max<sub>\u03b8</sub>L(x|r*s+b,\u03b8) / max<sub>r', \u03b8</sub>L(x|x|r'*s+b,\u03b8) ), with just the physical constraints 0 \u2264 r' This test statistics can give two-sided limits, as it starts decreasing when the number of observed events is above the expectations from the signal+background hypothesis. The default value when computing hybrid bayesian-frequentist limits is LEP . The default value when computing frequentist limits is LHC . Accuracy The search for the limit is performed using an adaptive algorithm, terminating when the estimate of the limit value is below some limit or when the precision cannot be futher improved with the specified options. The options controlling this behaviour are: rAbsAcc , rRelAcc : define the accuracy on the limit at which the search stops. The default values are 0.1 and 0.05 respectively, meaning that the search is stopped when \u0394r < 0.1 or \u0394r/r < 0.05. clsAcc : this determines the absolute accuracy up to which the CLs values are computed when searching for the limit. The default is 0.5%. Raising the accuracy above this value will increase significantly the time to run the algorithm, as you need N<sup>2</sup> more toys to improve the accuracy by a factor <sub>N</sub>; you can consider enlarging this value if you're computing limits with a larger CL (e.g. 90% or 68%) Note that if you're using the CL<sub>s+b</sub> rule then this parameter will control the uncertainty on CL<sub>s+b</sub> , not on CL<sub>s</sub> . T or toysH : controls the minimum number of toys that are generated for each point. The default value of 500 should be ok when computing the limit with 90-95% CL. You can decrease this number if you're computing limits at 68% CL, or increase it if you're using 99% CL. Computing significances When computing the significances, there is no adaptive generation. You can control the number of toys with option T or toysH= and the option iterations (shortcut -i , default 1): the default of (1 iteration)\u00d7(500 toys) is not enough to probe a significances above ~2. We suggest that you uncrease the number of iterations instead of the number of toys, since the increase in time is linear with the iterations but non-linear with the toys. In order to compute the significance in multiple jobs, proceed as follows: Run N different jobs with the same inputs but different random seed (option -s ), specifying the additional option --saveHybridResult . Use hadd to merge the output root files in a single one. The program will complain with messages like Cannot merge object type, name: HybridCalculator _result which can be safely ignored. Compute the significance from the merged file running again but with options --readHybridResults= and --toysFile=<merged.root> Caveat: there is no check within the program that you're using consistent inputs for the different jobs. Simple hypotesis testing : Sometimes you just want to compute the p-values of the background-only hypotesis and of the signal plus background hypotesis for a fixed value of the signal strength. This can be done specifying the option singlePoint <value> which will set the signal strength to that value and run the hypothesis test. It will generate toys until the required accuracy is met (see above for parameter clsAcc ). You can turn off adaptive generation setting clsAcc to zero, and then it will generate the toys once (or multiple times if you set option iterations to a value larger than 1). Just like for significance, you can run multiple times with different seeds and options --saveHybridResult , combine the output files with hadd and then compute the final result with --readHybridResult --toysFile=merged.root Performance issues The current hybrid code requires a lot of cpu resources. You can speed up the processing by using multiple cores (option fork , default value is 1). Note that even with fork set to 1, toy generation is done in a separate thread to avoid memory leaks. If you want to run in a single thread, e.g. to be able to read the debug output during generation, you should set the option to zero. If running multi-threaded on the cern batch cluster, you should declare it to the bsub command when submitting the jobs: e.g. for a job that uses four cores you should use bsub -n 4 -R \"'span[hosts=1]'\" ... #HybridNewGrid","title":"HybridNew algorithm"},{"location":"full-documentation/#hybridnew-algorithm-usage-for-complex-models-or-expected-limits-grids","text":"If your model is complex, or you need to know the limit accurately, or you want expected limits, then running the computation in a single job might not be feasible. The alternative approach is to compute a grid of distributions of the test statistics for various values of the signal strength, a task that is easy to parallelize, and then use the that grid to compute the observed limit (and also the expected ones). This requires you to have some knowledge of where the limit should be, which you can gain e.g. from the ProfileLikelihood method Creating the grid: manual way The procedure to do this manually would be like the procedure for significances or simple hypothesis testing described previously: for each value r_i of the cross section, you write out one file with the distribution of the test statistics using combine card.txt -M HybridNew [--freq] [other options] -s seed_i --singlePoint r_i --saveToys --saveHybridResult and then you can merge all the output files for the different r_i with hadd . The [other options] should include --clsAcc 0 to switch off adaptive sampling, and you can tune the CPU time by working on the parameters -T and --iterations . It is important that you use different seed_i values for each point; if you don't care about exact reproducibility, you can just use --seed -1 and the code will take care of randomizing itself properly. Creating the grid: automated way, using CRAB Please note that the following is intended for use with crab2. For producing the grid with crab3, please see the instructions here Once you have a sense of the time needed for each toy, and of the range to consider, you can use the script makeGridUsingCrab.py to run the toys to create the grid in parallel either on LXBATCH or on regular T2s (or anything else that CRAB can digest). The procedure is very simple: makeGridUsingCrab.py card.txt minimum maximum -n points [other options] -o name This will create a crab cfg file name.cfg and a script name.sh , and possibly a binary workspace name.workspace.root . You can then just create and submit the jobs from that cfg file, and merge the output rootfiles with hadd=(note: =hadd will complain with messages like Cannot merge object type, name: HybridCalculator _result which can be safely ignored). The other options, that you can get executing the program with --help are: -T : same as in combine -r : use a random seed in each job (suggested) -I n : run only on 1/n of the points in each job (suggested if you want to have many points) -t , -j : choose the total number of toys and of jobs (can change later from the crab cfg file) --lsf , --queue ... : use lxbatch with the specific queue (can change later from the crab cfg file) Note that you can merge also the output of multiple crab submissions, if you have used random seeds. Using the grid for observed limits combine mydatcard.txt -M HybridNew [--freq] --grid=mygrid.root All other parameters controlling toys, accuracy and so on are meaningless in this context. Note that it might still take a while if you have many points and the test statistics is slow to evaluate. Add the option --saveGrid to save the value of the observed CLs at each grid point in the output tree. Using the grid for expected limits combine mydatcard.txt -M HybridNew [--freq] --grid=mygrid.root --expectedFromGrid 0.5 0.5 gives you the median. use 0.16/0.84 to get the endpoints of 68% interval, 0.025/0.975 to get the 95% one). Add the option --saveGrid to save the value of the expected quantile CLs at each grid point in the output tree. Plotting the test-statistics distributions The distribution of the test-statistic under the signal plus background and background only hypotheses can be plotted at each value of the grid using the following; python test/plotTestStatCLs.py --input mygrid.root --poi r --val all --mass MASS The output root file will contain a plot for each point found in the grid.","title":"HybridNew algorithm usage for complex models or expected limits: grids"},{"location":"full-documentation/#feldmancousins","text":"The F-C method is used to compute an interval with the specified confidence level. If you run the model without special options, it will report the upper limit to the signal strength. If you want instead the lower end of the interval, just run it with option lowerLimit . The algorithm will search for a limit with an iterative procedure until the specified absolute or relative accuracy is met, as controlled by the parameters rAbsAcc , =rRelAcc . The default values are 0.1 and 0.05 respectively, meaning that the search is stopped when \u0394r < 0.1 or \u0394r/r < 0.05. The number of toys generated is adaptive. You can increase it by a factor using option toysFactor a value of 2 or 4 is suggested if you want to compute the limit with high accuracy.","title":"FeldmanCousins"},{"location":"full-documentation/#running-under-crab","text":"The instructions below are for use with crab2 . For instructions on how to use the grid for toy based studies or complicated model scans under crab3 , follow the instructions given here . Running many toy MC for the limit calculation may be conveniently split among the different available GRID resources using CRAB. Examples of how to run on the GRID via CRAB are provided in the files: [[https://github.com/cms-analysis/HiggsAnalysis-CombinedLimit/blob/master/test/combine_crab.sh][combine_crab.sh]] [[https://github.com/cms-analysis/HiggsAnalysis-CombinedLimit/blob/master/test/combine_crab.cfg][combine_crab.cfg]]","title":"Running under CRAB"},{"location":"full-documentation/#preparing-the-root-workspace","text":"The first thing to do is to convert the datacards and possibly the shape model into a ROOT workspace. This model will be shipped to Worker Nodes for GRID processing. This is done via the utility script text2workspace.py . For instance: ../scripts/text2workspace.py ../data/benchmarks/simple-counting/counting-B0-Obs0-StatOnly.txt -b -o model.root","title":"Preparing the ROOT workspace"},{"location":"full-documentation/#shell-script-for-grid-worker-nodes","text":"CRAB is designed mainly to provide automatic cmsRun job splitting providing the number of jobs one wants to run, and the number of 'events' in total one wants to process.The total number of toy MC we want to run. The maximum number of events is passed to the application to be executed via the variable $MaxEvents . In our case, we will use for convenience $MaxEvents as the number of toy MC to run per job. The script [[https://github.com/cms-analysis/HiggsAnalysis-CombinedLimit/blob/master/test/combine_crab.sh][combine_crab.sh]] runs the combiner code with the proper options, and prepares the output to be retrieved after the run completion on the Worker Nodes. It takes as argument the job indes ( $1 ), which we use as random seed. The main elements there are running the combiner and packing the output for final retrieval: echo \"job number: seed # i with n toys\" ./combine model.root -t n toys\" ./combine model.root -t n -sn -s i with <span class=\"arithmatex\"><span class=\"MathJax_Preview\">n toys\" ./combine model.root -t</span><script type=\"math/tex\">n toys\" ./combine model.root -t n -sn -s i >& log.txt mv *.root outputToy/ mv log.txt outputToy/ echo \"pack the results\" tar cvfz outputToy.tgz outputToy/","title":"Shell script for GRID Worker Nodes"},{"location":"full-documentation/#crab-configuration-script","text":"Then, the script = [[https://github.com/cms-analysis/HiggsAnalysis-CombinedLimit/blob/master/test/combine_crab.cfg][combine_crab.cfg]]= sets how many jobs to run and how many toys per job. And finally, which files to ship to the Worker Nodes (the executable itself and the ROOT workspace), and which file to retrieve (the packed output): [CRAB] jobtype = cmssw scheduler = glite [CMSSW] output_file = outputToy.tgz datasetpath=None pset=None total_number_of_events=100 number_of_jobs=10 [USER] script_exe = combine_crab.sh additional_input_files = combine, model.root return_data = 1 Please, notice that you have to ship to Worker Nodes the executable itself, so, before running you have to do: ln -s ../../../../bin/slc5_amd64_gcc434/combine combine","title":"CRAB configuration script"},{"location":"full-documentation/#grid-submission-via-crab","text":"CRAB submission can be now performed as usually done for other CMSSW analysis application: create your jobs: crab -create -cfg combine_crab.cfg submit your jobs: crab -submit monitor your jobs' status: crab -status after jobs are finished, retrieve the output: crab -getoutput The output consists in several files outputToy_n_m_xyz.tgz that contain the output of each job, including the log files, that can be combined and analyzed to obtain the final result.","title":"GRID submission via CRAB"},{"location":"full-documentation/#useful-links-and-tutorials","text":"Tutorial Sessions 1st tutorial 17th Nov 2015 . 2nd tutorial 30th Nov 2016 . Worked examples from Higgs analyses Further Reading and Advanced usage tutorials Conventions to be used when preparing inputs for Higgs combinations CMS AN-2011/298: Procedure for the LHC Higgs boson search combination in summer 2011 (describing some of the methods used in combine )","title":"Useful Links and tutorials"},{"location":"full-documentation/#combine-based-packages","text":"SWGuideHiggs2TauLimits ATGCRooStats CombineHarvester","title":"Combine-based packages"},{"location":"full-documentation/#contacts","text":"Hypernews forum : hn-cms-higgs-combination https://hypernews.cern.ch/HyperNews/CMS/get/higgs-combination.html #ReviewStatus","title":"Contacts"},{"location":"full-documentation/#review-status","text":"Reviewer/Editor and Date (copy from screen) Comments Main.GiovanniPetrucciani - 14-Dec-2010 created template page Main.GiovanniPetrucciani - 21-Mar-2011 new tag Main.LucaLista - 04-May-2011 added CRAB submission Main.GiovanniPetrucciani - 10-Jun-2011 Frequentist limits Main.GiovanniPetrucciani - 10-Jun-2011 Frequentist grids and expected limits Main.GiovanniPetrucciani Main.GiovanniPetrucciani","title":"Review status"},{"location":"new-histogram-models-with-automatic-bin-errors/","text":"","title":"New histogram models with automatic bin errors"},{"location":"releaseNotes/","text":"Release notes CMSSW 10_2_X - v8.0.0 This release contains all of the changes listed for v7.0.13 below. In addition: New documentation pages , using the mkdocs framework. The documentation source is included in the repository as simple markdown files . Users are welcome to make additions and corrections as pull requests to this repo. It is now possible to include additional constraint terms for regularisiation when unfolding using combine. Detailed documentation for this is given here . The option -S 0 to remove all systematic uncertainties has been removed. Instead, to freeze all constrained nuisance parameters the option --freezeParameters allConstrainedNuisances should be used, which replaces the previous shortcut of --freezeParameters all . The possibility to use some old method names has now been fully removed. When setting the -M option, FitDiagnostics , AsymptoticLimits and Significance must be used instead of, respectively, MaxLikelihoodFit , Asymptotic and ProfileLikelihood . CMSSW 8_1_X - v7.0.13 Nuisance edit selections for bins, processes or systematic names now require a complete string match. For example, nuisance edit add procA binA [...] will no longer match procAB and binAB . Note that regex selections can still be used to match multiple labels, but again are now required to match the full strings. Nuisance parameters can now be frozen using attributes that have been assigned to the corresponding RooRealVars. Syntax is --freezeWithAttributes attr1,attr2,...,attrN . For Higgs analyses: added YR4 cross sections, branching ratios and partial width uncertainties in data/lhc-hxswg/sm/ , as used in HIG-17-031 [EXPERIMENTAL] For binned analyses using autoMCStats a faster implementation of the vertical template morphing for shape uncertainties can be enabled at runtime with the option --X-rtd FAST_VERTICAL_MORPH . Any results using this flag should be validated carefully against the default.","title":"Release notes"},{"location":"releaseNotes/#release-notes","text":"","title":"Release notes"},{"location":"releaseNotes/#cmssw-10_2_x-v800","text":"This release contains all of the changes listed for v7.0.13 below. In addition: New documentation pages , using the mkdocs framework. The documentation source is included in the repository as simple markdown files . Users are welcome to make additions and corrections as pull requests to this repo. It is now possible to include additional constraint terms for regularisiation when unfolding using combine. Detailed documentation for this is given here . The option -S 0 to remove all systematic uncertainties has been removed. Instead, to freeze all constrained nuisance parameters the option --freezeParameters allConstrainedNuisances should be used, which replaces the previous shortcut of --freezeParameters all . The possibility to use some old method names has now been fully removed. When setting the -M option, FitDiagnostics , AsymptoticLimits and Significance must be used instead of, respectively, MaxLikelihoodFit , Asymptotic and ProfileLikelihood .","title":"CMSSW 10_2_X - v8.0.0"},{"location":"releaseNotes/#cmssw-8_1_x-v7013","text":"Nuisance edit selections for bins, processes or systematic names now require a complete string match. For example, nuisance edit add procA binA [...] will no longer match procAB and binAB . Note that regex selections can still be used to match multiple labels, but again are now required to match the full strings. Nuisance parameters can now be frozen using attributes that have been assigned to the corresponding RooRealVars. Syntax is --freezeWithAttributes attr1,attr2,...,attrN . For Higgs analyses: added YR4 cross sections, branching ratios and partial width uncertainties in data/lhc-hxswg/sm/ , as used in HIG-17-031 [EXPERIMENTAL] For binned analyses using autoMCStats a faster implementation of the vertical template morphing for shape uncertainties can be enabled at runtime with the option --X-rtd FAST_VERTICAL_MORPH . Any results using this flag should be validated carefully against the default.","title":"CMSSW 8_1_X - v7.0.13"},{"location":"tutorials-part-2/","text":"Datacard for Shape analyses The datacard has to be supplemented with two extensions: 1 a new block of lines defining how channels and processes are mapped into shapes 1 the block for systematics that can contain also rows with shape uncertainties. The expected shape can be parametric or not parametric. In the first case the parametric pdfs have to be given as input to the tool. In the latter case, for each channel, histograms have to be provided for the expected shape of each process. For what concerns data, they have to be provided as input to the tool as a histogram to perform a binned shape analysis and as a tree to perform an unbinned shape analysis. Not parametric shapes and uncertainties For each channel, histograms have to be provided for the observed shape and for the expected shape of each process. Within each channel, all histograms must have the same binning. The normalization of the data histogram must correspond to the number of observed events The normalization of the expected histograms must match the expected yields The combine tool can take as input histograms saved as TH1 or as RooAbsHist in a RooFit workspace (an example of how to create a RooFit workspace and save histograms is available in github ). Shape uncertainties can be taken into account by vertical interpolation of the histograms, like in the HIG-10-002 analysis. The shapes are interpolated quadratically for shifts below 1\u03c3 and linearly beyond. The normalizations are interpolated linearly in log scale just like we do for log-normal uncertainties. For each shape uncertainty and process/channel affected by it, two additional input shapes have to be provided, obtained shifting that parameter up and down by one standard deviation. When building the likelihood, each shape uncertainty is associated to a nuisance parameter taken from a unit gaussian distribution, which is used to interpolate or extrapolate using the specified histograms. For each given source of shape uncertainty, in the part of the datacard containing shape uncertainties (last block), there must be a row _ name %RED%shape effect for each process and channel_ The effect can be \"-\" or 0 for no effect, 1 for normal effect, and possibly something different from 1 to test larger or smaller effects (in that case, the unit gaussian is scaled by that factor before using it as parameter for the interpolation) fre The block of lines defining the mapping (first block in the datacard) contains one or more rows in the form shapes process channel file histogram [ _histogram_with systematics ] In this line process is any one the process names, or * for all processes, or data_obs for the observed data channel is any one the process names, or * for all channels _ file , histogram and _histogram_with systematics_ identify the names of the files and of the histograms within the file, after doing some replacements (if any are found): $PROCESS is replaced with the process name (or \" data_obs \" for the observed data) $CHANNEL is replaced with the channel name $SYSTEMATIC is replaced with the name of the systematic + ( Up, Down ) $MASS is replaced with the higgs mass value which is passed as option in the command line used to run the limit tool The datacard in simple-shapes-TH1.txt is a clear example of how to include shapes in the datacard. In the first block the following line specifies the shape mapping: shapes * * simple-shapes-TH1.root $PROCESS $PROCESS_$SYSTEMATIC The last block concerns the treatment of the systematics affecting shapes. In this part the two uncertainties effecting on the shape are listed. alpha shape - 1 uncertainty on background shape and normalization sigma shape 0.5 - uncertainty on signal resolution. Assume the histogram is a 2 sigma shift, # so divide the unit gaussian by 2 before doing the interpolation There are two options for the interpolation algorithm in the \"shape\" uncertainty. Putting shape will result in a quadratic interpolation (within +/-1 sigma) and a linear extrapolation (beyond +/-1 sigma) of the fraction of events in each bin - i.e the histograms are first normalised before interpolation. Putting shapeN while instead base the interpolation on the logs of the fraction in each bin. The total normalisation is interpolated using an asymmetric log-normal so that the effect of the systematic on both the shape and normalisation are accounted for. The following image shows a comparison of those two algorithms for this datacard. <img alt=\"compare_shape_algo.png\" src=\"%ATTACHURLPATH%/compare_shape_algo.png\" /> In this case there are two processes, signal and background , and two uncertainties affecting background ( alpha ) and signal shape ( sigma ). Within the root file 2 histograms per systematic have to be provided, they are the shape obtained, for the specific process, shifting up and down the parameter associated to the uncertainty: _background alphaUp and _background_alphaDown, _signal sigmaUp and _signal sigmaDown . This is the content of the root file simple-shapes-TH1.root associated to the datacard simple-shapes-TH1.txt : root [0] Attaching file simple-shapes-TH1.root as _file0... root [1] _file0->ls() TFile** simple-shapes-TH1.root TFile* simple-shapes-TH1.root KEY: TH1F signal;1 Histogram of signal__x KEY: TH1F signal_sigmaUp;1 Histogram of signal__x KEY: TH1F signal_sigmaDown;1 Histogram of signal__x KEY: TH1F background;1 Histogram of background__x KEY: TH1F background_alphaUp;1 Histogram of background__x KEY: TH1F background_alphaDown;1 Histogram of background__x KEY: TH1F data_obs;1 Histogram of data_obs__x KEY: TH1F data_sig;1 Histogram of data_sig__x For example, without shape uncertainties you could have just one row with shapes * * shapes.root $CHANNEL/$PROCESS Then for a simple example for two channels \"e\", \"mu\" with three processes \"higgs\", \"zz\", \"top\" you should create a rootfile that contains the following histogram meaning e/data_obs observed data in electron channel e/higgs expected shape for higgs in electron channel e/zz expected shape for ZZ in electron channel e/top expected shape for top in electron channel mu/data_obs observed data in muon channel mu/higgs expected shape for higgs in muon channel mu/zz expected shape for ZZ in muon channel mu/top expected shape for top in muon channel If you also have one uncertainty that affects the shape, e.g. jet energy scale, you should create shape histograms for the jet energy scale shifted up by one sigma, you could for example do one folder for each process and write a like like shapes * * shapes.root $CHANNEL/$PROCESS/nominal $CHANNEL/$PROCESS/$SYSTEMATIC or just attach a postifx to the name of the histogram shapes * * shapes.root $CHANNEL/$PROCESS $CHANNEL/$PROCESS_$SYSTEMATIC Parametric shapes and uncertainties If you want to combine a channel with shapes with one that is a simple counting experiment, you have to declare some fake shapes in the datacard of the counting experiment. This can be done simply by adding a line to the datacard shapes process channel %RED%FAKE%ENDCOLOR% As of tag T01-06-00 (note the initial T instead of V , since it's still only for testing) Shapes: choice-yes If doing toy mc generation, must run with --generateBinnedWorkaround (or -U / --unbinned ), due to a bug in RooFit Shape uncertainties: choice-yes shapes with vertical shape morphing (quadratic up to 1 sigma, then linear) exponential morphing on the normalization (just like for an asymmetric log-normal uncertainty) one can select linear morphing ( shapeL ) or multiplicative morphing ( shapeN ) as well, but with some caveats: only one morphing algorithm can be used for a given shape multiplicative morphing is applied to the normalized shapes, and then the normalization is interpolated separately. also, note that this is much slower than quadratic or linear morphing (factor 10-100) if you get results that look unreasonable, it could be that the truncation effects due to some part of the shape becoming negative are sizable. you can partially recover from this problem forcing roofit to re-normalize explicitly the function (just replace shape with ==shape*==; note, will be much slower), but it's likely that you need some other approach to handle the shape systematics This includes also a few other features not in this twiki: Support for RooFit shapes (specifying <tt> workspaceName : objectName </tt> in place of histogramName ): Binned datasets (RooDataHist) for data and for mc processes: choice-yes Unbinned datasets (RooDataSet) for data: choice-yes Arbitrary shapes (RooAbsPdf) for mc processes: choice-yes choice-yes Parametric shape uncertainties can be specified among the other systematics in a line <tt> name param mean uncertainty [range] </tt>. The uncertainty can be either the sigma of a gaussian or -xx/+yy (with no spaces) for a bifurcated gaussian. The range is optional. if present, it should be [hi,lo] (with no spaces). Normalization will always be taken from text datacard Unbinned templates (RooDataSet) for mc processes : choice-yes (but not tested) arbitrary shapes must have unique parameter names and they should match with the names of the systematics Support for plain TTrees as inputs instead of RooDataSet: choice-yes (but not tested) Mixing of histograms with RooFit shapes is not supported. Binned shape analysis See the 2014 Data Analysis School tutorial. Unbinned shape analysis This example is taken from 2014 Data Analysis School and use H->gg datacards as an example. This is an example of an unbinned, parametric analysis. In some cases, it can be convenient to describe the expected signal and background shapes in terms of analytical functions rather than templates; a typical example are the searches where the signal is apparent as a narrow peak over a smooth continuum background. In this context, uncertainties affecting the shapes of the signal and backgrounds can be implemented naturally as uncertainties on the parameters of those analytical functions. It is also possible to adapt an agnostic approach in which the parameters of the background model are left freely floating in the fit to the data, i.e. only requiring the background to be well described by a smooth function. Technically, this is implemented by means of the RooFit package, that allows writing generic probability density functions, and saving them into ROOT files. The pdfs can be either taken from RooFit's standard library of functions (e.g. Gaussians, polynomials, ...) or hand-coded in C++, and combined together to form even more complex shapes. A prototypical case for this kind of analysis is H\u2192\u03b3\u03b3 analysis. For this excercise, we will use a datacard that contains only the 8 TeV data for four event categories: cat0 and cat1 are categories of untagged events containing good quality diphotons, the former of the two with a higer purity but lower event yield obtained preferentially selecting high p<sub>T</sub> diphotons; cat4 and cat5 are categories of di-jet events with different levels of tightness (and so also different level of signal contamination from gluon fusion). The datacard is the following: imax 4 number of bins jmax 5 number of processes minus 1 kmax * number of nuisance parameters ---------------------------------------------------------------------------------------------------------------------------------- shapes WH cat0 hgg.inputsig_8TeV_MVA.root wsig_8TeV:hggpdfrel_wh_cat0 shapes ZH cat0 hgg.inputsig_8TeV_MVA.root wsig_8TeV:hggpdfrel_zh_cat0 shapes bkg_mass cat0 hgg.inputbkgdata_8TeV_MVA.root cms_hgg_workspace:pdf_data_pol_model_8TeV_cat0 shapes data_obs cat0 hgg.inputbkgdata_8TeV_MVA.root cms_hgg_workspace:roohist_data_mass_cat0 shapes ggH cat0 hgg.inputsig_8TeV_MVA.root wsig_8TeV:hggpdfrel_ggh_cat0 shapes qqH cat0 hgg.inputsig_8TeV_MVA.root wsig_8TeV:hggpdfrel_vbf_cat0 shapes ttH cat0 hgg.inputsig_8TeV_MVA.root wsig_8TeV:hggpdfrel_tth_cat0 [... same as above for cat1, cat4, cat5 ...] ---------------------------------------------------------------------------------------------------------------------------------- bin cat0 cat1 cat4 cat5 observation -1.0 -1.0 -1.0 -1.0 ---------------------------------------------------------------------------------------------------------------------------------- bin cat0 cat0 cat0 cat0 cat0 cat0 cat1 cat1 cat1 cat1 cat1 cat1 cat4 cat4 cat4 cat4 cat4 cat4 cat5 cat5 cat5 cat5 cat5 cat5 process ZH qqH WH ttH ggH bkg_mass ZH qqH WH ttH ggH bkg_mass ZH qqH WH ttH ggH bkg_mass ZH qqH WH ttH ggH bkg_mass process -4 -3 -2 -1 0 1 -4 -3 -2 -1 0 1 -4 -3 -2 -1 0 1 -4 -3 -2 -1 0 1 rate 6867.0000 19620.0000 12753.0000 19620.0000 19620.0000 1.0000 7259.4000 19620.0000 12360.6000 19620.0000 19620.0000 1.0000 7063.2000 19620.0000 12556.8000 19620.0000 19620.0000 1.0000 3924.0000 19620.0000 15696.0000 19620.0000 19620.0000 1.0000 ---------------------------------------------------------------------------------------------------------------------------------- CMS_eff_j lnN 0.999125 0.964688 0.999125 0.998262 0.996483 - 0.999616 0.980982 0.999616 0.99934 0.999012 - 1.02 1.02 1.02 1.02 1.02 - 1.02 1.02 1.02 1.02 1.02 - CMS_hgg_JECmigration lnN - - - - - - - - - - - - 0.853986 0.995971 0.853986 0.846677 0.927283 - 1.025 1.005 1.025 1.025 1.025 - CMS_hgg_UEPSmigration lnN - - - - - - - - - - - - 0.737174 0.991941 0.737174 0.724019 0.86911 - 1.045 1.01 1.045 1.045 1.045 - CMS_hgg_eff_MET lnN - - - - - - - - - - - - - - - - - - - - - - - - CMS_hgg_eff_e lnN - - - - - - - - - - - - - - - - - - - - - - - - CMS_hgg_eff_m lnN - - - - - - - - - - - - - - - - - - - - - - - - CMS_hgg_eff_trig lnN 1.01 1.01 1.01 1.01 1.01 - 1.01 1.01 1.01 1.01 1.01 - 1.01 1.01 1.01 1.01 1.01 - 1.01 1.01 1.01 1.01 1.01 - CMS_hgg_n_id lnN 1.034/0.958 1.039/0.949 1.034/0.958 1.053/0.915 1.035/0.958 - 1.042/0.936 1.038/0.948 1.042/0.936 1.053/0.909 1.038/0.954 - 1.016/0.972 1.022/0.963 1.016/0.972 1.017/0.967 1.019/0.964 - 1.024/0.959 1.030/0.948 1.024/0.959 1.014/0.975 1.023/0.962 - CMS_hgg_n_pdf_1 lnN - 0.998/0.996 - - 1.002/0.998 - - 0.992/0.999 - - 1.001/1.000 - - 0.999/0.998 - - 1.003/0.999 - - 0.996/1.000 - - 1.002/0.999 - CMS_hgg_n_pdf_10 lnN - 1.028/1.004 - - 0.998/0.998 - - 1.051/0.997 - - 1.004/0.999 - - 1.020/1.000 - - 1.000/0.998 - - 1.010/0.999 - - 0.999/1.000 - [... and more rows like this ...] CMS_hgg_n_sc_gf lnN - - - - 0.842/1.123 - - - - - 0.976/1.031 - - - - - 0.858/1.095 - - - - - 0.880/1.083 - CMS_hgg_n_sc_vbf lnN - 0.993/1.010 - - - - - 0.994/0.994 - - - - - 0.997/1.001 - - - - - 0.999/0.996 - - - - CMS_hgg_n_sigmae lnN 0.950/1.099 0.943/1.114 0.950/1.099 0.956/1.089 0.944/1.112 - 0.954/1.093 0.948/1.104 0.954/1.093 0.971/1.057 0.919/1.165 - 0.996/1.006 0.992/1.016 0.996/1.006 0.995/1.010 0.994/1.012 - 0.991/1.019 0.985/1.031 0.991/1.019 0.995/1.010 0.988/1.026 - CMS_id_eff_eb lnN 1.01999 1.020047 1.01999 1.02002 1.020022 - 1.018535 1.019332 1.018535 1.018642 1.019654 - 1.017762 1.017952 1.017762 1.018824 1.01812 - 1.016723 1.016872 1.016723 1.01884 1.017172 - CMS_id_eff_ee lnN 1.000284 1.000137 1.000284 1.000206 1.0002 - 1.004035 1.001979 1.004035 1.003759 1.001148 - 1.006058 1.005556 1.006058 1.003308 1.005127 - 1.008752 1.008351 1.008752 1.003254 1.007586 - JEC lnN 0.995187 0.938204 0.995187 0.990442 0.980656 - 0.997891 0.966719 0.997891 0.996368 0.994567 - 1.11 1.035 1.11 1.11 1.11 - 1.11 1.035 1.11 1.11 1.11 - QCDscale_VH lnN 0.982/1.021 - 0.982/1.021 - - - 0.982/1.021 - 0.982/1.021 - - - 0.982/1.021 - 0.982/1.021 - - - 0.982/1.021 - 0.982/1.021 - - - QCDscale_ggH lnN - - - - 0.918/1.076 - - - - - 0.918/1.076 - - - - - 0.918/1.076 - - - - - 0.918/1.076 - QCDscale_qqH lnN - 0.992/1.003 - - - - - 0.992/1.003 - - - - - 0.992/1.003 - - - - - 0.992/1.003 - - - - QCDscale_ttH lnN - - - 0.906/1.041 - - - - - 0.906/1.041 - - - - - 0.906/1.041 - - - - - 0.906/1.041 - - UEPS lnN 0.988624 0.858751 0.988624 0.977408 0.954278 - 0.995014 0.923929 0.995014 0.991416 0.987158 - 1.26 1.08 1.26 1.26 1.26 - 1.26 1.08 1.26 1.26 1.26 - lumi_8TeV lnN 1.044 1.044 1.044 1.044 1.044 - 1.044 1.044 1.044 1.044 1.044 - 1.044 1.044 1.044 1.044 1.044 - 1.044 1.044 1.044 1.044 1.044 - pdf_gg lnN - - - 0.920/1.080 0.930/1.076 - - - - 0.920/1.080 0.930/1.076 - - - - 0.920/1.080 0.930/1.076 - - - - 0.920/1.080 0.930/1.076 - pdf_qqbar lnN 0.958/1.042 0.972/1.026 0.958/1.042 - - - 0.958/1.042 0.972/1.026 0.958/1.042 - - - 0.958/1.042 0.972/1.026 0.958/1.042 - - - 0.958/1.042 0.972/1.026 0.958/1.042 - - - vtxEff lnN 0.991/1.025 0.989/1.030 0.991/1.025 0.993/1.020 0.989/1.030 - 0.990/1.026 0.990/1.027 0.990/1.026 0.994/1.016 0.984/1.042 - 1.000/0.999 0.997/1.007 1.000/0.999 1.000/1.003 0.998/1.006 - 0.999/1.004 0.998/1.006 0.999/1.004 1.000/1.000 0.997/1.007 - CMS_hgg_nuissancedeltamcat4 param 0.0 0.001458 CMS_hgg_nuissancedeltafracright_8TeV param 1.0 0.002000 CMS_hgg_nuissancedeltamcat1 param 0.0 0.001470 CMS_hgg_nuissancedeltamcat0 param 0.0 0.001530 CMS_hgg_nuissancedeltasmearcat4 param 0.0 0.001122 CMS_hgg_nuissancedeltasmearcat1 param 0.0 0.001167 CMS_hgg_nuissancedeltasmearcat0 param 0.0 0.001230 CMS_hgg_globalscale param 0.0 0.004717 The first difference compared to the template datacard is in the shapes line; let's take for example these two lines shapes ggH cat0 hgg.inputsig_8TeV_MVA.root wsig_8TeV:hggpdfrel_ggh_cat0 shapes data_obs cat0 hgg.inputbkgdata_8TeV_MVA.root cms_hgg_workspace:roohist_data_mass_cat0 In the datacard using templates, the column after the file name would have been the name of the histogram. Here, instead, we found two names, separated by a colon ( : ): the first part identifies the name of the RooWorkspace containing the pdf, and the second part the name of the RooAbsPdf inside it (or, for the observed data, the RooAbsData ). Let's inspect this workspace, starting with the data %CODE{\"cpp\"}% TFile *fDat = TFile::Open(\"hgg.inputbkgdata_8TeV_MVA.root\"); RooAbsData *data = cms_hgg_workspace-&gt;data(\"roohist_data_mass_cat0\"); data-&gt;Print(\"\") // --&gt; RooDataHist::roohist_data_mass_cat0[CMS_hgg_mass] = 160 bins (1449 weights) // so, we have a binned dataset, whose variable is called CMS_hgg_mass: RooRealVar *mass = cms_hgg_workspace-&gt;var(\"CMS_hgg_mass\"); mass-&gt;Print(\"\"); // RooRealVar::CMS_hgg_mass = 140 L(100 - 180) // we can make a plot of the dataset with the following RooPlot *plot = mass-&gt;frame(); data-&gt;plotOn(plot); plot-&gt;Draw(); %ENDCODE% <img alt=\"datacards_Hgg_data_cat0.png\" src=\"%ATTACHURLPATH%/datacards_Hgg_data_cat0.png\" /> Now let's look also at the signal %CODE{\"cpp\"}% TFile *sig = TFile::Open(\"hgg.inputsig_8TeV_MVA.root\"); RooWorkspace *wsig_8TeV = (RooWorkspace*)sig-&gt;Get(\"wsig_8TeV\") // necessary to Get the workspace in ROOT6 to avoid reloading RooAbsPdf *ggH = wsig_8TeV-&gt;pdf(\"hggpdfrel_ggh_cat0\"); ggH-&gt;Print(\"\"); // --&gt; RooAddPdf::hggpdfrel_ggh_cat0[ hist_func_frac_g0_ggh_cat0 * hgg_gaus_g0_ggh_cat0 + hist_func_frac_g1_ggh_cat0 * hgg_gaus_g1_ggh_cat0 + const_func_frac_g2_ggh_cat0 * hgg_gaus_g2_ggh_cat0 ] = 0.604097 // this appears to be a linear combination of multiple gaussian pdfs (hgg_gaus_g0_ggh_cat0, hgg_gaus_g1_ggh_cat0, ...) with coefficents hist_func_frac_g0_ggh_cat0, hist_func_frac_g1_ggh_cat0 // let's get the list of the parameters that describe it RooArgSet *params = ggH-&gt;getParameters(*data); params-&gt;Print(\"\"); // --&gt; RooArgSet::parameters = (CMS_hgg_globalscale,CMS_hgg_nuissancedeltamcat0,CMS_hgg_nuissancedeltasmearcat0,MH) // MH is a special parameter, which combine and text2workspace set to the Higgs mass hypothesis. // now since we're just looking into the input workspace, we can set it by hand wsig_8TeV-&gt;var(\"MH\")-&gt;setVal(125.7); // Now we can make a plot of the pdf, and show also the contributions from the different gaussians from which it is composed RooPlot *plot = wsig_8TeV-&gt;var(\"CMS_hgg_mass\")-&gt;frame(); ggH-&gt;plotOn(plot); ggH-&gt;plotOn(plot, RooFit::Components(\"hgg_gaus_g0_ggh_cat0\"), RooFit::LineColor(kRed)); ggH-&gt;plotOn(plot, RooFit::Components(\"hgg_gaus_g1_ggh_cat0\"), RooFit::LineColor(209)); ggH-&gt;plotOn(plot, RooFit::Components(\"hgg_gaus_g2_ggh_cat0\"), RooFit::LineColor(222)); plot-&gt;Draw(); %ENDCODE% <img alt=\"datacards_Hgg_ggH_cat0.png\" src=\"%ATTACHURLPATH%/datacards_Hgg_ggH_cat0.png\" /> Parametric signal normalization There is also another feature of the H\u2192\u03b3\u03b3 datacard that should catch your eye quicky: the event yields are remarkably strange: the the signal yield is 19620.0000 events for ggH, qqH and ttH, the background yield is 1.0 , and observed event yield is -1.0 . Let's start with the simple case: an event yield of -1 just instructs text2workspace and combine to take the yield from the corresponding dataset in the input rootfile, avoiding the need of writing it in the text datacard, but also making the datacard less human-readable. Incidentally, this feature can be used also for datacards that use root histograms like the H\u2192\u03c4\u03c4 example above. Now, the signal and background yields: 19k signal events from each production mode with just one background event would be really nice to have, but of course it can't be true. The way the H\u2192\u03b3\u03b3 works is by relying on an additional feature of text2workspace: in addition to providing generic shapes for the signals and backgrounds, it is also possible to provide generic functions that describe the expected signal and background yields. This additional functions are multiplied by the number in the rate column of the datacard to obtain the final expected event yield. This feature allows e.g. to use the very same datacard to describe all possible different Higgs boson mass hypotheses by just parameterizing properly the expected yield as function of the MH variable. At present, this feature is not indicated by any special line the datacard: simply, whenever a RooAbsPdf is loaded, text2workspace will search also for a RooAbsReal object with the same name but a _norm postfix, and if present it will use it to scale the event yield. WARNING As with all parameters in the workspace, if this RooAbsReal object is not set constant, (i.e is a RooRealVar which you have not set constant or a function of non-constant vars) it will be floating in the fit. This is especially problematic for the signal as the default parameter added to models for limits/p-values r (the floating signal strength) will be degenerate with this _norm parameter. In this H\u2192\u03b3&gamma example, the object has been setup to be constant to avoid this. Note: The newest version of combine will not accept RooExtendedPdfs as an input anymore. This is to alleviate a bug that lead to improper treatment of normalization when using multiple RooExtendedPdfs to describe a single process. Simply follow the above instructions and combine will create the appropriate extended pdf as long as the name of the RooAbsReal is pdfname_norm. Make sure the variable is not set to constant if you intend to have the normalization float. Armed with this new piece of knowledge, we can now determine what is the expected signal yield for ggH in category 0: %CODE{\"cpp\"}% TFile *sig = TFile::Open(\"hgg.inputsig_8TeV_MVA.root\"); RooWorkspace *wsig_8TeV = (RooWorkspace*)sig-&gt;Get(\"wsig_8TeV\") // necessary to Get the workspace in ROOT6 to avoid reloading wsig_8TeV-&gt;var(\"MH\")-&gt;setVal(125.7); RooAbsPdf *ggH = wsig_8TeV-&gt;pdf(\"hggpdfrel_ggh_cat0\"); RooAbsReal *ggH_norm = wsig_8TeV-&gt;function(\"hggpdfrel_ggh_cat0_norm\"); cout &lt;&lt; ggH_norm-&gt;getVal()*19620.0000 &lt;&lt; endl; // --&gt; 12.3902 %ENDCODE% This approach can also be used to make a background freely floating, by associating to them as normalization term a floating RooRealVar. However, this can be achieved in a more transparent way by putting instead a normalization uncertainty on that background using a flat pdf: e.g. to leave the background floating between 50% and 200% of its input prediction, a lnU systematic can be used with \u03ba 2 ( ==lnU= has a syntax like lnN , but produces a uniform pdf between 1/\u03ba and \u03ba rather than a log-normal; see SWGuideHiggsAnalysisCombinedLimit ) Shape uncertainties using parameters The part of the H\u2192\u03b3\u03b3 datacard related to the systematics starts with many lines of log-normals that should already be familiar, except possibly for the notation with two numbers separated by a slash (e.g. 0.950/1.099). This notation is used for asymmetrical uncertainties: a log-normal with 0.950/1.099 means that at -1\u03c3 the yield is scaled down by a factor 0.95, while at +1\u03c3 the yield is scaled up by a factor 1.099. The last part of the datacard contains some lines that use a different syntax, e.g. CMS_hgg_globalscale param 0.0 0.004717 These lines encode uncertainties on the parameters of the signal and background pdfs. The example line quoted here informs text2workspace that the parameter CMS_hgg_globalscale is to be assigned a Gaussian uncertainty of \u00b10.004717 around its mean value of zero (0.0). One can change the mean value from 0 to 1 (or really any value, if one so chooses) if the parameter in question is multiplicative instead of additive. The effect can be visualized from RooFit, e.g. %CODE{\"cpp\"}% TFile *sig = TFile::Open(\"hgg.inputsig_8TeV_MVA.root\"); RooWorkspace *wsig_8TeV = (RooWorkspace*)sig-&gt;Get(\"wsig_8TeV\") // necessary to Get the workspace in ROOT6 to avoid reloading wsig_8TeV-&gt;var(\"MH\")-&gt;setVal(125.7); RooAbsPdf *ggH = wsig_8TeV-&gt;pdf(\"hggpdfrel_ggh_cat0\"); // prepare the canvas RooPlot *plot = wsig_8TeV-&gt;var(\"CMS_hgg_mass\")-&gt;frame(); // plot nominal pdf ggH-&gt;plotOn(plot, RooFit::LineColor(kBlack)); // plot minus 3 sigma pdf wsig_8TeV-&gt;var(\"CMS_hgg_globalscale\")-&gt;setVal(-3*0.004717); ggH-&gt;plotOn(plot, RooFit::LineColor(kBlue)); // plot plus 3 sigma pdf wsig_8TeV-&gt;var(\"CMS_hgg_globalscale\")-&gt;setVal(+3*0.004717); ggH-&gt;plotOn(plot, RooFit::LineColor(kRed)); plot-&gt;Draw(); %ENDCODE% <img alt=\"datacards_Hgg_ggH_cat0_syst.png\" src=\"%ATTACHURLPATH%/datacards_Hgg_ggH_cat0_syst.png\" /> Note that if one wants to specify a parameter that is freely floating across its given range, and not gaussian constrained, the following syntax is used: CMS_my_bg_param1 flatParam The Hgg and HZg analyses use this syntax when adding in the parameters that correspond to their background shapes. #ParametricModelAndBinnedData Caveat on using parametric pdfs with binned datasets Users should be aware of a feature that affects the use of parametric pdfs together with binned datasets. RooFit uses the integral of the pdf, computed analytically (or numerically, but disregarding the binning), to normalize it, but then computes the expected event yield in each bin evaluating only the pdf at the bin center. This means that if the variation of the pdf is sizeable within the bin then there is a mismatch between the sum of the event yields per bin and the pdf normalization, and that can cause a bias in the fits (more properly, the bias is there if the contribution of the second derivative integrated on the bin size is not negligible, since for linear functions evaluating them at the bin center is correct). So, it is recommended to use bins that are significantly finer than the characteristic scale of the pdfs - which would anyway be the recommended thing even in the absence of this feature. Obviously, this caveat does not apply to analyses using templates (they're constant across each bin, so there's no bias), or using unbinned datasets. Analysis with more generic models Multidimensional fits Feldman-Cousins regions The F-C procedure for a generic model is: use as test statistics the profile likelihood q(x) = - 2 ln L(data|x)/L(data|x-hat) where x is a point in the parameter space, and x-hat are the point corresponding to the best fit (nuisance parameters are profiled both at numerator and at denominator) for each point x : compute the observed test statistics q<sub>obs</sub>(x) compute the expected distribution of q(x) under the hypothesis of x. accept the point in the region if P(q(x) < q<sub>obs</sub>(x) | x) < CL In combine, you can perform this test on each individual point (param1,param2,...) = (value1,value2,...) by doing combine workspace.root -M HybridNew --freq --testStat=PL --rule=CLsplusb --singlePoint param1=value1,param2=value2,param3=value3,... [other options of HybridNew] The point belongs to your confidence region if CL<sub>s+b</sub> is larger than 1-CL (e.g. 0.3173 for a 1-sigma region, CL=0.6827). Imposing physical boundaries (such as requiring mu>0) can be achieved by setting the ranges of the physics model parameters using --setPhysicsModelParameterRanges param1=param1_min,param1_max:param2=param2_min,param2_max .... . If there is no upper/lower boundary, just set that value to something far from the region of interest. As in general for HybridNew, you can split the task into multiple tasks and then merge the outputs, as described in the HybridNew chapter . For uni-dimensional models only, and if the parameter behaves like a cross-section, the code is somewhat able to do interpolation and determine the values of your parameter on the contour (just like it does for the limits). In that case, the syntax is the same as per the CLs limits with HybridNew chapter except that you want --testStat=PL --rule=CLsplusb . Extracting Contours There is a tool for extracting confidence intervals and 2D contours from the output of HybridNew located in test/makeFCcontour.py providing the option --saveToys was included when running HybridNew. I can be run taking as input, the toys files (or several of them) as, ./makeFCcontour.py toysfile1.root toysfile2.root .... [options] -out outputfile.root The tool has two modes (1D and 2D). For the 1D, add the option --d1 and the name of the parameter of interest --xvar poi_name . For each confidence interval desired, add any confidence level of interest using --cl 0.68,0.95... The intervals corresponding to each confidence level will be printed to the terminal. The output file will contain a graph of the parameter of interest (x) vs 1-CL<sub>s+b</sub> used to compute the intervals. To extract 2D contours, the names of each parameter must be given --xvar poi_x --yvar poi_y . The output will be a root file containing a 2D histogram of the confidence level (1-CL<sub>s+b</sub>) for each point which can be used to draw 2D contours. There will also be a histogram containing the number of toys found for each point. There are several options for reducing the running time (such as setting limits on the region of interest or the minimum number of toys required for a point to be included) Finally, adding the option --storeToys will add histograms in for each point to the output file of the test-statistic distribution. This will increase the momory usage however as all of the toys will be stored. Signal Hypothesis separation In some cases, instead of separating a signal from a background, you might want to separate a signal of one type from a signal of another type (e.g. scalar vs pseudo-scalar Higgs boson). This is documented at SWGuideHiggsCombinationSignalSeparation Throwing post-fit toys From here : #build workspace for mu-mh fit text2workspace.py ../cards/hgg_datacard_mva_comb_bernsteins.txt -m 125 -P HiggsAnalysis.CombinedLimit.PhysicsModel:floatingHiggsMass --PO higgsMassRange=105,155 -o testmasshggcomb.root #perform s+b fit and save workspace+snapshot combine testmasshggcomb.root -m 125 -M MultiDimFit --saveWorkspace --verbose 9 -n mumhfit #throw post-fit toy with b from s+b(floating mu,mh) fit, s with r=1.0, m=best fit MH, using nuisance values and constraints re-centered on s+b(floating mu-mh) fit values (aka frequentist post-fit expected) #and compute post-fit expected mu uncertainty profiling MH combine higgsCombinemumhfit.MultiDimFit.mH125.root --snapshotName MultiDimFit -M MultiDimFit --verbose 9 -n randomtest --toysFrequentist --bypassFrequentistFit -t -1 --expectSignal=1 -P r --floatOtherPOIs=1 --algo singles #throw post-fit toy with b from s+b(floating mu,mh) fit, s with r=1.0, m=128.0, using nuisance values and constraints re-centered on s+b(floating mu-mh) fit values (aka frequentist post-fit expected) #and compute post-fit expected significance (with MH fixed at 128 implicitly) combine higgsCombinemumhfit.MultiDimFit.mH125.root -m 128 --snapshotName MultiDimFit -M ProfileLikelihood --significance --verbose 9 -n randomtest --toysFrequentist --bypassFrequentistFit --overrideSnapshotMass -t -1 --expectSignal=1 --redefineSignalPOIs r --freezeNuisances MH #throw post-fit toy with b from s+b(floating mu,mh) fit, s with r=0.0, using nuisance values and constraints re-centered on s+b(floating mu-mh) fit values (aka frequentist post-fit expected) #and compute post-fit expected and observed asymptotic limit (with MH fixed at 128 implicitly) combine higgsCombinemumhfit.MultiDimFit.mH125.root -m 128 --snapshotName MultiDimFit -M Asymptotic --verbose 9 -n randomtest --bypassFrequentistFit --overrideSnapshotMass--redefineSignalPOIs r --freezeNuisances MH Modifying parameters of interest on the command line Normally, the parameters of interest of a model are defined by the PhysicsModel used by text2workspace. However, combine provides command line options to redefine on the fly what are the parameters of interest (useful also if the workspaces are built from external tools and not text2workspace), for setting the values of the parameters and their ranges. The MultiDimFit method already provides conventient handles to specify which parameter to analyze and how to deal with the others (floating or fixed), but these commands are more general as they apply to all statistical methods and they can affect any kind of parameters in the model, not only those that were pre-defined as parameters of interest (the wording \"physics model parameter\" or \"nuisance\" in the names of the options is there only because that's typically what they are applied to, not because their functionality is restricted to parameters of that kind) --setPhysicsModelParameters name=value[,name2=value2,...] sets the starting values of the parameters, useful e.g. when generating toy MC or when also setting the parameters as fixed. --setPhysicsModelParameterRanges name=min,max[:name2=min2,max2:...] sets the ranges of the parameters (useful e.g. for scanning in MultiDimFit, or for Bayesian integration) --redefineSignalPOIs name[,name2,...] redefines the set of parameters of interest. if the parameters where constant in the input workspace, they are re-defined to be floating. nuisances promoted to parameters of interest are removed from the list of nuisances, and thus they are not randomized in methods that randomize nuisances (e.g. HybridNew in non-frequentist mode, or BayesianToyMC, or in toy generation with -t but without --toysFreq ). This doesn't have any impact on algorithms that don't randomize nuisances (e.g. fits, Asymptotic, or HybridNew in fequentist mode) or on algorithms that treat all parameters in the same way (e.g. MarkovChainMC). Note that constraint terms for the nuisances are dropped after promotion to a POI using --redefineSignalPOI . To produce a likelihood scan for a nuisance parameter, using MultiDimFit with --algo grid , you should instead use the --poi option which will not cause the loss of the constraint term when scanning. parameters of interest of the input workspace that are not selected by this command become unconstrained nuisance parameters, but they are not added to the list of nuisances so they will not be randomized (see above) --freezeNuisances <name> sets the given parameters to constant A combination of the MultiSignalModel (defined above) and redefineSignalPOIs can be used to alter the way a datacard is interpreted to transform one signal or ordinary background process into a background with freely floating normalization, or with a fixed but different normalization (useful e.g. for cross-checks): First, use the MultiSignalModel to create a workspace in which that process has its own associated signal strenght parameter (e.g. r_B ) different from that of the nomal signal (e.g. r ) Then, fits or upper limits on r can be obtained running combine with --redefineSignalPOIs r so that r_B becomes freely floating. setPhysicsModelParameterRanges can be used to redefine the range to make the process freely float only in some range (this is equivalent to adding a flat uncertainty to it, lnU or unif ) while freezeNuisances and setPhysicsModelParameters can instead be used to pin it to some value. fits and likelihood scans of r_B can be used to check whether the information on this process from the data in the signal region is consistent with the a-priori prediction for it within the respective uncertainties (and freezeNuisances can be used to shut off the uncertainties on the a-priori prediction if one wants to see only the uncertainty from the data) Advanced Tutorials Follow this link for advanced tutorials and non-standard uses of combine","title":"Tutorials part 2"},{"location":"tutorials-part-2/#datacard-for-shape-analyses","text":"The datacard has to be supplemented with two extensions: 1 a new block of lines defining how channels and processes are mapped into shapes 1 the block for systematics that can contain also rows with shape uncertainties. The expected shape can be parametric or not parametric. In the first case the parametric pdfs have to be given as input to the tool. In the latter case, for each channel, histograms have to be provided for the expected shape of each process. For what concerns data, they have to be provided as input to the tool as a histogram to perform a binned shape analysis and as a tree to perform an unbinned shape analysis.","title":"Datacard for Shape analyses"},{"location":"tutorials-part-2/#not-parametric-shapes-and-uncertainties","text":"For each channel, histograms have to be provided for the observed shape and for the expected shape of each process. Within each channel, all histograms must have the same binning. The normalization of the data histogram must correspond to the number of observed events The normalization of the expected histograms must match the expected yields The combine tool can take as input histograms saved as TH1 or as RooAbsHist in a RooFit workspace (an example of how to create a RooFit workspace and save histograms is available in github ). Shape uncertainties can be taken into account by vertical interpolation of the histograms, like in the HIG-10-002 analysis. The shapes are interpolated quadratically for shifts below 1\u03c3 and linearly beyond. The normalizations are interpolated linearly in log scale just like we do for log-normal uncertainties. For each shape uncertainty and process/channel affected by it, two additional input shapes have to be provided, obtained shifting that parameter up and down by one standard deviation. When building the likelihood, each shape uncertainty is associated to a nuisance parameter taken from a unit gaussian distribution, which is used to interpolate or extrapolate using the specified histograms. For each given source of shape uncertainty, in the part of the datacard containing shape uncertainties (last block), there must be a row _ name %RED%shape effect for each process and channel_ The effect can be \"-\" or 0 for no effect, 1 for normal effect, and possibly something different from 1 to test larger or smaller effects (in that case, the unit gaussian is scaled by that factor before using it as parameter for the interpolation) fre The block of lines defining the mapping (first block in the datacard) contains one or more rows in the form shapes process channel file histogram [ _histogram_with systematics ] In this line process is any one the process names, or * for all processes, or data_obs for the observed data channel is any one the process names, or * for all channels _ file , histogram and _histogram_with systematics_ identify the names of the files and of the histograms within the file, after doing some replacements (if any are found): $PROCESS is replaced with the process name (or \" data_obs \" for the observed data) $CHANNEL is replaced with the channel name $SYSTEMATIC is replaced with the name of the systematic + ( Up, Down ) $MASS is replaced with the higgs mass value which is passed as option in the command line used to run the limit tool The datacard in simple-shapes-TH1.txt is a clear example of how to include shapes in the datacard. In the first block the following line specifies the shape mapping: shapes * * simple-shapes-TH1.root $PROCESS $PROCESS_$SYSTEMATIC The last block concerns the treatment of the systematics affecting shapes. In this part the two uncertainties effecting on the shape are listed. alpha shape - 1 uncertainty on background shape and normalization sigma shape 0.5 - uncertainty on signal resolution. Assume the histogram is a 2 sigma shift, # so divide the unit gaussian by 2 before doing the interpolation There are two options for the interpolation algorithm in the \"shape\" uncertainty. Putting shape will result in a quadratic interpolation (within +/-1 sigma) and a linear extrapolation (beyond +/-1 sigma) of the fraction of events in each bin - i.e the histograms are first normalised before interpolation. Putting shapeN while instead base the interpolation on the logs of the fraction in each bin. The total normalisation is interpolated using an asymmetric log-normal so that the effect of the systematic on both the shape and normalisation are accounted for. The following image shows a comparison of those two algorithms for this datacard. <img alt=\"compare_shape_algo.png\" src=\"%ATTACHURLPATH%/compare_shape_algo.png\" /> In this case there are two processes, signal and background , and two uncertainties affecting background ( alpha ) and signal shape ( sigma ). Within the root file 2 histograms per systematic have to be provided, they are the shape obtained, for the specific process, shifting up and down the parameter associated to the uncertainty: _background alphaUp and _background_alphaDown, _signal sigmaUp and _signal sigmaDown . This is the content of the root file simple-shapes-TH1.root associated to the datacard simple-shapes-TH1.txt : root [0] Attaching file simple-shapes-TH1.root as _file0... root [1] _file0->ls() TFile** simple-shapes-TH1.root TFile* simple-shapes-TH1.root KEY: TH1F signal;1 Histogram of signal__x KEY: TH1F signal_sigmaUp;1 Histogram of signal__x KEY: TH1F signal_sigmaDown;1 Histogram of signal__x KEY: TH1F background;1 Histogram of background__x KEY: TH1F background_alphaUp;1 Histogram of background__x KEY: TH1F background_alphaDown;1 Histogram of background__x KEY: TH1F data_obs;1 Histogram of data_obs__x KEY: TH1F data_sig;1 Histogram of data_sig__x For example, without shape uncertainties you could have just one row with shapes * * shapes.root $CHANNEL/$PROCESS Then for a simple example for two channels \"e\", \"mu\" with three processes \"higgs\", \"zz\", \"top\" you should create a rootfile that contains the following histogram meaning e/data_obs observed data in electron channel e/higgs expected shape for higgs in electron channel e/zz expected shape for ZZ in electron channel e/top expected shape for top in electron channel mu/data_obs observed data in muon channel mu/higgs expected shape for higgs in muon channel mu/zz expected shape for ZZ in muon channel mu/top expected shape for top in muon channel If you also have one uncertainty that affects the shape, e.g. jet energy scale, you should create shape histograms for the jet energy scale shifted up by one sigma, you could for example do one folder for each process and write a like like shapes * * shapes.root $CHANNEL/$PROCESS/nominal $CHANNEL/$PROCESS/$SYSTEMATIC or just attach a postifx to the name of the histogram shapes * * shapes.root $CHANNEL/$PROCESS $CHANNEL/$PROCESS_$SYSTEMATIC Parametric shapes and uncertainties If you want to combine a channel with shapes with one that is a simple counting experiment, you have to declare some fake shapes in the datacard of the counting experiment. This can be done simply by adding a line to the datacard shapes process channel %RED%FAKE%ENDCOLOR% As of tag T01-06-00 (note the initial T instead of V , since it's still only for testing) Shapes: choice-yes If doing toy mc generation, must run with --generateBinnedWorkaround (or -U / --unbinned ), due to a bug in RooFit Shape uncertainties: choice-yes shapes with vertical shape morphing (quadratic up to 1 sigma, then linear) exponential morphing on the normalization (just like for an asymmetric log-normal uncertainty) one can select linear morphing ( shapeL ) or multiplicative morphing ( shapeN ) as well, but with some caveats: only one morphing algorithm can be used for a given shape multiplicative morphing is applied to the normalized shapes, and then the normalization is interpolated separately. also, note that this is much slower than quadratic or linear morphing (factor 10-100) if you get results that look unreasonable, it could be that the truncation effects due to some part of the shape becoming negative are sizable. you can partially recover from this problem forcing roofit to re-normalize explicitly the function (just replace shape with ==shape*==; note, will be much slower), but it's likely that you need some other approach to handle the shape systematics This includes also a few other features not in this twiki: Support for RooFit shapes (specifying <tt> workspaceName : objectName </tt> in place of histogramName ): Binned datasets (RooDataHist) for data and for mc processes: choice-yes Unbinned datasets (RooDataSet) for data: choice-yes Arbitrary shapes (RooAbsPdf) for mc processes: choice-yes choice-yes Parametric shape uncertainties can be specified among the other systematics in a line <tt> name param mean uncertainty [range] </tt>. The uncertainty can be either the sigma of a gaussian or -xx/+yy (with no spaces) for a bifurcated gaussian. The range is optional. if present, it should be [hi,lo] (with no spaces). Normalization will always be taken from text datacard Unbinned templates (RooDataSet) for mc processes : choice-yes (but not tested) arbitrary shapes must have unique parameter names and they should match with the names of the systematics Support for plain TTrees as inputs instead of RooDataSet: choice-yes (but not tested) Mixing of histograms with RooFit shapes is not supported.","title":"Not parametric shapes and uncertainties"},{"location":"tutorials-part-2/#binned-shape-analysis","text":"See the 2014 Data Analysis School tutorial.","title":"Binned shape analysis"},{"location":"tutorials-part-2/#unbinned-shape-analysis","text":"This example is taken from 2014 Data Analysis School and use H->gg datacards as an example. This is an example of an unbinned, parametric analysis. In some cases, it can be convenient to describe the expected signal and background shapes in terms of analytical functions rather than templates; a typical example are the searches where the signal is apparent as a narrow peak over a smooth continuum background. In this context, uncertainties affecting the shapes of the signal and backgrounds can be implemented naturally as uncertainties on the parameters of those analytical functions. It is also possible to adapt an agnostic approach in which the parameters of the background model are left freely floating in the fit to the data, i.e. only requiring the background to be well described by a smooth function. Technically, this is implemented by means of the RooFit package, that allows writing generic probability density functions, and saving them into ROOT files. The pdfs can be either taken from RooFit's standard library of functions (e.g. Gaussians, polynomials, ...) or hand-coded in C++, and combined together to form even more complex shapes. A prototypical case for this kind of analysis is H\u2192\u03b3\u03b3 analysis. For this excercise, we will use a datacard that contains only the 8 TeV data for four event categories: cat0 and cat1 are categories of untagged events containing good quality diphotons, the former of the two with a higer purity but lower event yield obtained preferentially selecting high p<sub>T</sub> diphotons; cat4 and cat5 are categories of di-jet events with different levels of tightness (and so also different level of signal contamination from gluon fusion). The datacard is the following: imax 4 number of bins jmax 5 number of processes minus 1 kmax * number of nuisance parameters ---------------------------------------------------------------------------------------------------------------------------------- shapes WH cat0 hgg.inputsig_8TeV_MVA.root wsig_8TeV:hggpdfrel_wh_cat0 shapes ZH cat0 hgg.inputsig_8TeV_MVA.root wsig_8TeV:hggpdfrel_zh_cat0 shapes bkg_mass cat0 hgg.inputbkgdata_8TeV_MVA.root cms_hgg_workspace:pdf_data_pol_model_8TeV_cat0 shapes data_obs cat0 hgg.inputbkgdata_8TeV_MVA.root cms_hgg_workspace:roohist_data_mass_cat0 shapes ggH cat0 hgg.inputsig_8TeV_MVA.root wsig_8TeV:hggpdfrel_ggh_cat0 shapes qqH cat0 hgg.inputsig_8TeV_MVA.root wsig_8TeV:hggpdfrel_vbf_cat0 shapes ttH cat0 hgg.inputsig_8TeV_MVA.root wsig_8TeV:hggpdfrel_tth_cat0 [... same as above for cat1, cat4, cat5 ...] ---------------------------------------------------------------------------------------------------------------------------------- bin cat0 cat1 cat4 cat5 observation -1.0 -1.0 -1.0 -1.0 ---------------------------------------------------------------------------------------------------------------------------------- bin cat0 cat0 cat0 cat0 cat0 cat0 cat1 cat1 cat1 cat1 cat1 cat1 cat4 cat4 cat4 cat4 cat4 cat4 cat5 cat5 cat5 cat5 cat5 cat5 process ZH qqH WH ttH ggH bkg_mass ZH qqH WH ttH ggH bkg_mass ZH qqH WH ttH ggH bkg_mass ZH qqH WH ttH ggH bkg_mass process -4 -3 -2 -1 0 1 -4 -3 -2 -1 0 1 -4 -3 -2 -1 0 1 -4 -3 -2 -1 0 1 rate 6867.0000 19620.0000 12753.0000 19620.0000 19620.0000 1.0000 7259.4000 19620.0000 12360.6000 19620.0000 19620.0000 1.0000 7063.2000 19620.0000 12556.8000 19620.0000 19620.0000 1.0000 3924.0000 19620.0000 15696.0000 19620.0000 19620.0000 1.0000 ---------------------------------------------------------------------------------------------------------------------------------- CMS_eff_j lnN 0.999125 0.964688 0.999125 0.998262 0.996483 - 0.999616 0.980982 0.999616 0.99934 0.999012 - 1.02 1.02 1.02 1.02 1.02 - 1.02 1.02 1.02 1.02 1.02 - CMS_hgg_JECmigration lnN - - - - - - - - - - - - 0.853986 0.995971 0.853986 0.846677 0.927283 - 1.025 1.005 1.025 1.025 1.025 - CMS_hgg_UEPSmigration lnN - - - - - - - - - - - - 0.737174 0.991941 0.737174 0.724019 0.86911 - 1.045 1.01 1.045 1.045 1.045 - CMS_hgg_eff_MET lnN - - - - - - - - - - - - - - - - - - - - - - - - CMS_hgg_eff_e lnN - - - - - - - - - - - - - - - - - - - - - - - - CMS_hgg_eff_m lnN - - - - - - - - - - - - - - - - - - - - - - - - CMS_hgg_eff_trig lnN 1.01 1.01 1.01 1.01 1.01 - 1.01 1.01 1.01 1.01 1.01 - 1.01 1.01 1.01 1.01 1.01 - 1.01 1.01 1.01 1.01 1.01 - CMS_hgg_n_id lnN 1.034/0.958 1.039/0.949 1.034/0.958 1.053/0.915 1.035/0.958 - 1.042/0.936 1.038/0.948 1.042/0.936 1.053/0.909 1.038/0.954 - 1.016/0.972 1.022/0.963 1.016/0.972 1.017/0.967 1.019/0.964 - 1.024/0.959 1.030/0.948 1.024/0.959 1.014/0.975 1.023/0.962 - CMS_hgg_n_pdf_1 lnN - 0.998/0.996 - - 1.002/0.998 - - 0.992/0.999 - - 1.001/1.000 - - 0.999/0.998 - - 1.003/0.999 - - 0.996/1.000 - - 1.002/0.999 - CMS_hgg_n_pdf_10 lnN - 1.028/1.004 - - 0.998/0.998 - - 1.051/0.997 - - 1.004/0.999 - - 1.020/1.000 - - 1.000/0.998 - - 1.010/0.999 - - 0.999/1.000 - [... and more rows like this ...] CMS_hgg_n_sc_gf lnN - - - - 0.842/1.123 - - - - - 0.976/1.031 - - - - - 0.858/1.095 - - - - - 0.880/1.083 - CMS_hgg_n_sc_vbf lnN - 0.993/1.010 - - - - - 0.994/0.994 - - - - - 0.997/1.001 - - - - - 0.999/0.996 - - - - CMS_hgg_n_sigmae lnN 0.950/1.099 0.943/1.114 0.950/1.099 0.956/1.089 0.944/1.112 - 0.954/1.093 0.948/1.104 0.954/1.093 0.971/1.057 0.919/1.165 - 0.996/1.006 0.992/1.016 0.996/1.006 0.995/1.010 0.994/1.012 - 0.991/1.019 0.985/1.031 0.991/1.019 0.995/1.010 0.988/1.026 - CMS_id_eff_eb lnN 1.01999 1.020047 1.01999 1.02002 1.020022 - 1.018535 1.019332 1.018535 1.018642 1.019654 - 1.017762 1.017952 1.017762 1.018824 1.01812 - 1.016723 1.016872 1.016723 1.01884 1.017172 - CMS_id_eff_ee lnN 1.000284 1.000137 1.000284 1.000206 1.0002 - 1.004035 1.001979 1.004035 1.003759 1.001148 - 1.006058 1.005556 1.006058 1.003308 1.005127 - 1.008752 1.008351 1.008752 1.003254 1.007586 - JEC lnN 0.995187 0.938204 0.995187 0.990442 0.980656 - 0.997891 0.966719 0.997891 0.996368 0.994567 - 1.11 1.035 1.11 1.11 1.11 - 1.11 1.035 1.11 1.11 1.11 - QCDscale_VH lnN 0.982/1.021 - 0.982/1.021 - - - 0.982/1.021 - 0.982/1.021 - - - 0.982/1.021 - 0.982/1.021 - - - 0.982/1.021 - 0.982/1.021 - - - QCDscale_ggH lnN - - - - 0.918/1.076 - - - - - 0.918/1.076 - - - - - 0.918/1.076 - - - - - 0.918/1.076 - QCDscale_qqH lnN - 0.992/1.003 - - - - - 0.992/1.003 - - - - - 0.992/1.003 - - - - - 0.992/1.003 - - - - QCDscale_ttH lnN - - - 0.906/1.041 - - - - - 0.906/1.041 - - - - - 0.906/1.041 - - - - - 0.906/1.041 - - UEPS lnN 0.988624 0.858751 0.988624 0.977408 0.954278 - 0.995014 0.923929 0.995014 0.991416 0.987158 - 1.26 1.08 1.26 1.26 1.26 - 1.26 1.08 1.26 1.26 1.26 - lumi_8TeV lnN 1.044 1.044 1.044 1.044 1.044 - 1.044 1.044 1.044 1.044 1.044 - 1.044 1.044 1.044 1.044 1.044 - 1.044 1.044 1.044 1.044 1.044 - pdf_gg lnN - - - 0.920/1.080 0.930/1.076 - - - - 0.920/1.080 0.930/1.076 - - - - 0.920/1.080 0.930/1.076 - - - - 0.920/1.080 0.930/1.076 - pdf_qqbar lnN 0.958/1.042 0.972/1.026 0.958/1.042 - - - 0.958/1.042 0.972/1.026 0.958/1.042 - - - 0.958/1.042 0.972/1.026 0.958/1.042 - - - 0.958/1.042 0.972/1.026 0.958/1.042 - - - vtxEff lnN 0.991/1.025 0.989/1.030 0.991/1.025 0.993/1.020 0.989/1.030 - 0.990/1.026 0.990/1.027 0.990/1.026 0.994/1.016 0.984/1.042 - 1.000/0.999 0.997/1.007 1.000/0.999 1.000/1.003 0.998/1.006 - 0.999/1.004 0.998/1.006 0.999/1.004 1.000/1.000 0.997/1.007 - CMS_hgg_nuissancedeltamcat4 param 0.0 0.001458 CMS_hgg_nuissancedeltafracright_8TeV param 1.0 0.002000 CMS_hgg_nuissancedeltamcat1 param 0.0 0.001470 CMS_hgg_nuissancedeltamcat0 param 0.0 0.001530 CMS_hgg_nuissancedeltasmearcat4 param 0.0 0.001122 CMS_hgg_nuissancedeltasmearcat1 param 0.0 0.001167 CMS_hgg_nuissancedeltasmearcat0 param 0.0 0.001230 CMS_hgg_globalscale param 0.0 0.004717 The first difference compared to the template datacard is in the shapes line; let's take for example these two lines shapes ggH cat0 hgg.inputsig_8TeV_MVA.root wsig_8TeV:hggpdfrel_ggh_cat0 shapes data_obs cat0 hgg.inputbkgdata_8TeV_MVA.root cms_hgg_workspace:roohist_data_mass_cat0 In the datacard using templates, the column after the file name would have been the name of the histogram. Here, instead, we found two names, separated by a colon ( : ): the first part identifies the name of the RooWorkspace containing the pdf, and the second part the name of the RooAbsPdf inside it (or, for the observed data, the RooAbsData ). Let's inspect this workspace, starting with the data %CODE{\"cpp\"}% TFile *fDat = TFile::Open(\"hgg.inputbkgdata_8TeV_MVA.root\"); RooAbsData *data = cms_hgg_workspace-&gt;data(\"roohist_data_mass_cat0\"); data-&gt;Print(\"\") // --&gt; RooDataHist::roohist_data_mass_cat0[CMS_hgg_mass] = 160 bins (1449 weights) // so, we have a binned dataset, whose variable is called CMS_hgg_mass: RooRealVar *mass = cms_hgg_workspace-&gt;var(\"CMS_hgg_mass\"); mass-&gt;Print(\"\"); // RooRealVar::CMS_hgg_mass = 140 L(100 - 180) // we can make a plot of the dataset with the following RooPlot *plot = mass-&gt;frame(); data-&gt;plotOn(plot); plot-&gt;Draw(); %ENDCODE% <img alt=\"datacards_Hgg_data_cat0.png\" src=\"%ATTACHURLPATH%/datacards_Hgg_data_cat0.png\" /> Now let's look also at the signal %CODE{\"cpp\"}% TFile *sig = TFile::Open(\"hgg.inputsig_8TeV_MVA.root\"); RooWorkspace *wsig_8TeV = (RooWorkspace*)sig-&gt;Get(\"wsig_8TeV\") // necessary to Get the workspace in ROOT6 to avoid reloading RooAbsPdf *ggH = wsig_8TeV-&gt;pdf(\"hggpdfrel_ggh_cat0\"); ggH-&gt;Print(\"\"); // --&gt; RooAddPdf::hggpdfrel_ggh_cat0[ hist_func_frac_g0_ggh_cat0 * hgg_gaus_g0_ggh_cat0 + hist_func_frac_g1_ggh_cat0 * hgg_gaus_g1_ggh_cat0 + const_func_frac_g2_ggh_cat0 * hgg_gaus_g2_ggh_cat0 ] = 0.604097 // this appears to be a linear combination of multiple gaussian pdfs (hgg_gaus_g0_ggh_cat0, hgg_gaus_g1_ggh_cat0, ...) with coefficents hist_func_frac_g0_ggh_cat0, hist_func_frac_g1_ggh_cat0 // let's get the list of the parameters that describe it RooArgSet *params = ggH-&gt;getParameters(*data); params-&gt;Print(\"\"); // --&gt; RooArgSet::parameters = (CMS_hgg_globalscale,CMS_hgg_nuissancedeltamcat0,CMS_hgg_nuissancedeltasmearcat0,MH) // MH is a special parameter, which combine and text2workspace set to the Higgs mass hypothesis. // now since we're just looking into the input workspace, we can set it by hand wsig_8TeV-&gt;var(\"MH\")-&gt;setVal(125.7); // Now we can make a plot of the pdf, and show also the contributions from the different gaussians from which it is composed RooPlot *plot = wsig_8TeV-&gt;var(\"CMS_hgg_mass\")-&gt;frame(); ggH-&gt;plotOn(plot); ggH-&gt;plotOn(plot, RooFit::Components(\"hgg_gaus_g0_ggh_cat0\"), RooFit::LineColor(kRed)); ggH-&gt;plotOn(plot, RooFit::Components(\"hgg_gaus_g1_ggh_cat0\"), RooFit::LineColor(209)); ggH-&gt;plotOn(plot, RooFit::Components(\"hgg_gaus_g2_ggh_cat0\"), RooFit::LineColor(222)); plot-&gt;Draw(); %ENDCODE% <img alt=\"datacards_Hgg_ggH_cat0.png\" src=\"%ATTACHURLPATH%/datacards_Hgg_ggH_cat0.png\" />","title":"Unbinned shape analysis"},{"location":"tutorials-part-2/#parametric-signal-normalization","text":"There is also another feature of the H\u2192\u03b3\u03b3 datacard that should catch your eye quicky: the event yields are remarkably strange: the the signal yield is 19620.0000 events for ggH, qqH and ttH, the background yield is 1.0 , and observed event yield is -1.0 . Let's start with the simple case: an event yield of -1 just instructs text2workspace and combine to take the yield from the corresponding dataset in the input rootfile, avoiding the need of writing it in the text datacard, but also making the datacard less human-readable. Incidentally, this feature can be used also for datacards that use root histograms like the H\u2192\u03c4\u03c4 example above. Now, the signal and background yields: 19k signal events from each production mode with just one background event would be really nice to have, but of course it can't be true. The way the H\u2192\u03b3\u03b3 works is by relying on an additional feature of text2workspace: in addition to providing generic shapes for the signals and backgrounds, it is also possible to provide generic functions that describe the expected signal and background yields. This additional functions are multiplied by the number in the rate column of the datacard to obtain the final expected event yield. This feature allows e.g. to use the very same datacard to describe all possible different Higgs boson mass hypotheses by just parameterizing properly the expected yield as function of the MH variable. At present, this feature is not indicated by any special line the datacard: simply, whenever a RooAbsPdf is loaded, text2workspace will search also for a RooAbsReal object with the same name but a _norm postfix, and if present it will use it to scale the event yield. WARNING As with all parameters in the workspace, if this RooAbsReal object is not set constant, (i.e is a RooRealVar which you have not set constant or a function of non-constant vars) it will be floating in the fit. This is especially problematic for the signal as the default parameter added to models for limits/p-values r (the floating signal strength) will be degenerate with this _norm parameter. In this H\u2192\u03b3&gamma example, the object has been setup to be constant to avoid this. Note: The newest version of combine will not accept RooExtendedPdfs as an input anymore. This is to alleviate a bug that lead to improper treatment of normalization when using multiple RooExtendedPdfs to describe a single process. Simply follow the above instructions and combine will create the appropriate extended pdf as long as the name of the RooAbsReal is pdfname_norm. Make sure the variable is not set to constant if you intend to have the normalization float. Armed with this new piece of knowledge, we can now determine what is the expected signal yield for ggH in category 0: %CODE{\"cpp\"}% TFile *sig = TFile::Open(\"hgg.inputsig_8TeV_MVA.root\"); RooWorkspace *wsig_8TeV = (RooWorkspace*)sig-&gt;Get(\"wsig_8TeV\") // necessary to Get the workspace in ROOT6 to avoid reloading wsig_8TeV-&gt;var(\"MH\")-&gt;setVal(125.7); RooAbsPdf *ggH = wsig_8TeV-&gt;pdf(\"hggpdfrel_ggh_cat0\"); RooAbsReal *ggH_norm = wsig_8TeV-&gt;function(\"hggpdfrel_ggh_cat0_norm\"); cout &lt;&lt; ggH_norm-&gt;getVal()*19620.0000 &lt;&lt; endl; // --&gt; 12.3902 %ENDCODE% This approach can also be used to make a background freely floating, by associating to them as normalization term a floating RooRealVar. However, this can be achieved in a more transparent way by putting instead a normalization uncertainty on that background using a flat pdf: e.g. to leave the background floating between 50% and 200% of its input prediction, a lnU systematic can be used with \u03ba 2 ( ==lnU= has a syntax like lnN , but produces a uniform pdf between 1/\u03ba and \u03ba rather than a log-normal; see SWGuideHiggsAnalysisCombinedLimit )","title":"Parametric signal normalization"},{"location":"tutorials-part-2/#shape-uncertainties-using-parameters","text":"The part of the H\u2192\u03b3\u03b3 datacard related to the systematics starts with many lines of log-normals that should already be familiar, except possibly for the notation with two numbers separated by a slash (e.g. 0.950/1.099). This notation is used for asymmetrical uncertainties: a log-normal with 0.950/1.099 means that at -1\u03c3 the yield is scaled down by a factor 0.95, while at +1\u03c3 the yield is scaled up by a factor 1.099. The last part of the datacard contains some lines that use a different syntax, e.g. CMS_hgg_globalscale param 0.0 0.004717 These lines encode uncertainties on the parameters of the signal and background pdfs. The example line quoted here informs text2workspace that the parameter CMS_hgg_globalscale is to be assigned a Gaussian uncertainty of \u00b10.004717 around its mean value of zero (0.0). One can change the mean value from 0 to 1 (or really any value, if one so chooses) if the parameter in question is multiplicative instead of additive. The effect can be visualized from RooFit, e.g. %CODE{\"cpp\"}% TFile *sig = TFile::Open(\"hgg.inputsig_8TeV_MVA.root\"); RooWorkspace *wsig_8TeV = (RooWorkspace*)sig-&gt;Get(\"wsig_8TeV\") // necessary to Get the workspace in ROOT6 to avoid reloading wsig_8TeV-&gt;var(\"MH\")-&gt;setVal(125.7); RooAbsPdf *ggH = wsig_8TeV-&gt;pdf(\"hggpdfrel_ggh_cat0\"); // prepare the canvas RooPlot *plot = wsig_8TeV-&gt;var(\"CMS_hgg_mass\")-&gt;frame(); // plot nominal pdf ggH-&gt;plotOn(plot, RooFit::LineColor(kBlack)); // plot minus 3 sigma pdf wsig_8TeV-&gt;var(\"CMS_hgg_globalscale\")-&gt;setVal(-3*0.004717); ggH-&gt;plotOn(plot, RooFit::LineColor(kBlue)); // plot plus 3 sigma pdf wsig_8TeV-&gt;var(\"CMS_hgg_globalscale\")-&gt;setVal(+3*0.004717); ggH-&gt;plotOn(plot, RooFit::LineColor(kRed)); plot-&gt;Draw(); %ENDCODE% <img alt=\"datacards_Hgg_ggH_cat0_syst.png\" src=\"%ATTACHURLPATH%/datacards_Hgg_ggH_cat0_syst.png\" /> Note that if one wants to specify a parameter that is freely floating across its given range, and not gaussian constrained, the following syntax is used: CMS_my_bg_param1 flatParam The Hgg and HZg analyses use this syntax when adding in the parameters that correspond to their background shapes. #ParametricModelAndBinnedData","title":"Shape uncertainties using parameters"},{"location":"tutorials-part-2/#caveat-on-using-parametric-pdfs-with-binned-datasets","text":"Users should be aware of a feature that affects the use of parametric pdfs together with binned datasets. RooFit uses the integral of the pdf, computed analytically (or numerically, but disregarding the binning), to normalize it, but then computes the expected event yield in each bin evaluating only the pdf at the bin center. This means that if the variation of the pdf is sizeable within the bin then there is a mismatch between the sum of the event yields per bin and the pdf normalization, and that can cause a bias in the fits (more properly, the bias is there if the contribution of the second derivative integrated on the bin size is not negligible, since for linear functions evaluating them at the bin center is correct). So, it is recommended to use bins that are significantly finer than the characteristic scale of the pdfs - which would anyway be the recommended thing even in the absence of this feature. Obviously, this caveat does not apply to analyses using templates (they're constant across each bin, so there's no bias), or using unbinned datasets.","title":"Caveat on using parametric pdfs with binned datasets"},{"location":"tutorials-part-2/#analysis-with-more-generic-models","text":"","title":"Analysis with more generic models"},{"location":"tutorials-part-2/#multidimensional-fits","text":"","title":"Multidimensional fits"},{"location":"tutorials-part-2/#feldman-cousins-regions","text":"The F-C procedure for a generic model is: use as test statistics the profile likelihood q(x) = - 2 ln L(data|x)/L(data|x-hat) where x is a point in the parameter space, and x-hat are the point corresponding to the best fit (nuisance parameters are profiled both at numerator and at denominator) for each point x : compute the observed test statistics q<sub>obs</sub>(x) compute the expected distribution of q(x) under the hypothesis of x. accept the point in the region if P(q(x) < q<sub>obs</sub>(x) | x) < CL In combine, you can perform this test on each individual point (param1,param2,...) = (value1,value2,...) by doing combine workspace.root -M HybridNew --freq --testStat=PL --rule=CLsplusb --singlePoint param1=value1,param2=value2,param3=value3,... [other options of HybridNew] The point belongs to your confidence region if CL<sub>s+b</sub> is larger than 1-CL (e.g. 0.3173 for a 1-sigma region, CL=0.6827). Imposing physical boundaries (such as requiring mu>0) can be achieved by setting the ranges of the physics model parameters using --setPhysicsModelParameterRanges param1=param1_min,param1_max:param2=param2_min,param2_max .... . If there is no upper/lower boundary, just set that value to something far from the region of interest. As in general for HybridNew, you can split the task into multiple tasks and then merge the outputs, as described in the HybridNew chapter . For uni-dimensional models only, and if the parameter behaves like a cross-section, the code is somewhat able to do interpolation and determine the values of your parameter on the contour (just like it does for the limits). In that case, the syntax is the same as per the CLs limits with HybridNew chapter except that you want --testStat=PL --rule=CLsplusb . Extracting Contours There is a tool for extracting confidence intervals and 2D contours from the output of HybridNew located in test/makeFCcontour.py providing the option --saveToys was included when running HybridNew. I can be run taking as input, the toys files (or several of them) as, ./makeFCcontour.py toysfile1.root toysfile2.root .... [options] -out outputfile.root The tool has two modes (1D and 2D). For the 1D, add the option --d1 and the name of the parameter of interest --xvar poi_name . For each confidence interval desired, add any confidence level of interest using --cl 0.68,0.95... The intervals corresponding to each confidence level will be printed to the terminal. The output file will contain a graph of the parameter of interest (x) vs 1-CL<sub>s+b</sub> used to compute the intervals. To extract 2D contours, the names of each parameter must be given --xvar poi_x --yvar poi_y . The output will be a root file containing a 2D histogram of the confidence level (1-CL<sub>s+b</sub>) for each point which can be used to draw 2D contours. There will also be a histogram containing the number of toys found for each point. There are several options for reducing the running time (such as setting limits on the region of interest or the minimum number of toys required for a point to be included) Finally, adding the option --storeToys will add histograms in for each point to the output file of the test-statistic distribution. This will increase the momory usage however as all of the toys will be stored.","title":"Feldman-Cousins regions"},{"location":"tutorials-part-2/#signal-hypothesis-separation","text":"In some cases, instead of separating a signal from a background, you might want to separate a signal of one type from a signal of another type (e.g. scalar vs pseudo-scalar Higgs boson). This is documented at SWGuideHiggsCombinationSignalSeparation","title":"Signal Hypothesis separation"},{"location":"tutorials-part-2/#throwing-post-fit-toys","text":"From here : #build workspace for mu-mh fit text2workspace.py ../cards/hgg_datacard_mva_comb_bernsteins.txt -m 125 -P HiggsAnalysis.CombinedLimit.PhysicsModel:floatingHiggsMass --PO higgsMassRange=105,155 -o testmasshggcomb.root #perform s+b fit and save workspace+snapshot combine testmasshggcomb.root -m 125 -M MultiDimFit --saveWorkspace --verbose 9 -n mumhfit #throw post-fit toy with b from s+b(floating mu,mh) fit, s with r=1.0, m=best fit MH, using nuisance values and constraints re-centered on s+b(floating mu-mh) fit values (aka frequentist post-fit expected) #and compute post-fit expected mu uncertainty profiling MH combine higgsCombinemumhfit.MultiDimFit.mH125.root --snapshotName MultiDimFit -M MultiDimFit --verbose 9 -n randomtest --toysFrequentist --bypassFrequentistFit -t -1 --expectSignal=1 -P r --floatOtherPOIs=1 --algo singles #throw post-fit toy with b from s+b(floating mu,mh) fit, s with r=1.0, m=128.0, using nuisance values and constraints re-centered on s+b(floating mu-mh) fit values (aka frequentist post-fit expected) #and compute post-fit expected significance (with MH fixed at 128 implicitly) combine higgsCombinemumhfit.MultiDimFit.mH125.root -m 128 --snapshotName MultiDimFit -M ProfileLikelihood --significance --verbose 9 -n randomtest --toysFrequentist --bypassFrequentistFit --overrideSnapshotMass -t -1 --expectSignal=1 --redefineSignalPOIs r --freezeNuisances MH #throw post-fit toy with b from s+b(floating mu,mh) fit, s with r=0.0, using nuisance values and constraints re-centered on s+b(floating mu-mh) fit values (aka frequentist post-fit expected) #and compute post-fit expected and observed asymptotic limit (with MH fixed at 128 implicitly) combine higgsCombinemumhfit.MultiDimFit.mH125.root -m 128 --snapshotName MultiDimFit -M Asymptotic --verbose 9 -n randomtest --bypassFrequentistFit --overrideSnapshotMass--redefineSignalPOIs r --freezeNuisances MH","title":"Throwing post-fit toys"},{"location":"tutorials-part-2/#modifying-parameters-of-interest-on-the-command-line","text":"Normally, the parameters of interest of a model are defined by the PhysicsModel used by text2workspace. However, combine provides command line options to redefine on the fly what are the parameters of interest (useful also if the workspaces are built from external tools and not text2workspace), for setting the values of the parameters and their ranges. The MultiDimFit method already provides conventient handles to specify which parameter to analyze and how to deal with the others (floating or fixed), but these commands are more general as they apply to all statistical methods and they can affect any kind of parameters in the model, not only those that were pre-defined as parameters of interest (the wording \"physics model parameter\" or \"nuisance\" in the names of the options is there only because that's typically what they are applied to, not because their functionality is restricted to parameters of that kind) --setPhysicsModelParameters name=value[,name2=value2,...] sets the starting values of the parameters, useful e.g. when generating toy MC or when also setting the parameters as fixed. --setPhysicsModelParameterRanges name=min,max[:name2=min2,max2:...] sets the ranges of the parameters (useful e.g. for scanning in MultiDimFit, or for Bayesian integration) --redefineSignalPOIs name[,name2,...] redefines the set of parameters of interest. if the parameters where constant in the input workspace, they are re-defined to be floating. nuisances promoted to parameters of interest are removed from the list of nuisances, and thus they are not randomized in methods that randomize nuisances (e.g. HybridNew in non-frequentist mode, or BayesianToyMC, or in toy generation with -t but without --toysFreq ). This doesn't have any impact on algorithms that don't randomize nuisances (e.g. fits, Asymptotic, or HybridNew in fequentist mode) or on algorithms that treat all parameters in the same way (e.g. MarkovChainMC). Note that constraint terms for the nuisances are dropped after promotion to a POI using --redefineSignalPOI . To produce a likelihood scan for a nuisance parameter, using MultiDimFit with --algo grid , you should instead use the --poi option which will not cause the loss of the constraint term when scanning. parameters of interest of the input workspace that are not selected by this command become unconstrained nuisance parameters, but they are not added to the list of nuisances so they will not be randomized (see above) --freezeNuisances <name> sets the given parameters to constant A combination of the MultiSignalModel (defined above) and redefineSignalPOIs can be used to alter the way a datacard is interpreted to transform one signal or ordinary background process into a background with freely floating normalization, or with a fixed but different normalization (useful e.g. for cross-checks): First, use the MultiSignalModel to create a workspace in which that process has its own associated signal strenght parameter (e.g. r_B ) different from that of the nomal signal (e.g. r ) Then, fits or upper limits on r can be obtained running combine with --redefineSignalPOIs r so that r_B becomes freely floating. setPhysicsModelParameterRanges can be used to redefine the range to make the process freely float only in some range (this is equivalent to adding a flat uncertainty to it, lnU or unif ) while freezeNuisances and setPhysicsModelParameters can instead be used to pin it to some value. fits and likelihood scans of r_B can be used to check whether the information on this process from the data in the signal region is consistent with the a-priori prediction for it within the respective uncertainties (and freezeNuisances can be used to shut off the uncertainties on the a-priori prediction if one wants to see only the uncertainty from the data)","title":"Modifying parameters of interest on the command line"},{"location":"tutorials-part-2/#advanced-tutorials","text":"Follow this link for advanced tutorials and non-standard uses of combine","title":"Advanced Tutorials"},{"location":"part1/gettingstarted/","text":"Setting up the environment and installation The instructions below are for installation within a CMSSW environment For end users that don't need to commit or do any development You can find the latest releases on github under https://github.com/cms-analysis/HiggsAnalysis-CombinedLimit/releases ROOT6 SLC6 release CMSSW_8_1_X - recommended version Setting up the environment (once): export SCRAM_ARCH=slc6_amd64_gcc530 cmsrel CMSSW_8_1_0 cd CMSSW_8_1_0/src cmsenv git clone https://github.com/cms-analysis/HiggsAnalysis-CombinedLimit.git HiggsAnalysis/CombinedLimit cd HiggsAnalysis/CombinedLimit Update to a reccomended tag - currently the reccomended tag is v7.0.12 : cd $CMSSW_BASE/src/HiggsAnalysis/CombinedLimit git fetch origin git checkout v7.0.12 scramv1 b clean; scramv1 b # always make a clean build You can generate a diff of any two tags (eg for v7.0.8 and v7.0.6 ) by using following the url: https://github.com/cms-analysis/HiggsAnalysis-CombinedLimit/compare/v7.0.6...v7.0.7 Replace the tag names in the url to any tags you which to compare. For developers We use the Fork and Pull model for development: each user creates a copy of the repository on github, commits their requests there and then sends pull requests for the administrators to merge. Prerequisites Register on github, as needed anyway for CMSSW development: http://cms-sw.github.io/cmssw/faq.html Register your SSH key on github: https://help.github.com/articles/generating-ssh-keys 1 Fork the repository to create your copy of it: https://github.com/cms-analysis/HiggsAnalysis-CombinedLimit/fork (more documentation at https://help.github.com/articles/fork-a-repo ) You will now be able to browse your fork of the repository from https://github.com/your-github-user-name/HiggsAnalysis-CombinedLimit Recommended way to develop a feature (in a branch) # get the updates of the master branch of the remote repository git fetch upstream # branch straight off the upstream master git checkout -b feature_name_branch upstream/81x-root606 # implement the feature # commit, etc # before publishing: # get the updates of the master branch of the remote repository git fetch upstream # if you're ready to integrate the upstream changes into your repository do git rebase upstream/81x-root606 # fix any conflicts git push origin feature_name_branch And proceed to make a pull request from the branch you created. Committing changes to your repository git add .... git commit -m \"....\" git push You can now make a pull request to the repository. Combine Tool An additional tool for submitting combine jobs to batch/crab, developed originally for HiggsToTauTau. Since the repository contains a certain amount of analysis-specific code, the following scripts can be used to clone it with a sparse checkout for just the core CombineHarvester/CombineTools subpackage, speeding up the checkout and compile times: git clone via ssh: bash <(curl -s https://raw.githubusercontent.com/cms-analysis/CombineHarvester/master/CombineTools/scripts/sparse-checkout-ssh.sh) git clone via https: bash <(curl -s https://raw.githubusercontent.com/cms-analysis/CombineHarvester/master/CombineTools/scripts/sparse-checkout-https.sh) make sure to run scram to compile the CombineTools package. See the CombineHarvester documentation pages for more details on using this tool and additional features available in the full package.","title":"Setting up the environment and installation"},{"location":"part1/gettingstarted/#setting-up-the-environment-and-installation","text":"The instructions below are for installation within a CMSSW environment","title":"Setting up the environment and installation"},{"location":"part1/gettingstarted/#for-end-users-that-dont-need-to-commit-or-do-any-development","text":"You can find the latest releases on github under https://github.com/cms-analysis/HiggsAnalysis-CombinedLimit/releases","title":"For end users that don't need to commit or do any development"},{"location":"part1/gettingstarted/#root6-slc6-release-cmssw_8_1_x-recommended-version","text":"Setting up the environment (once): export SCRAM_ARCH=slc6_amd64_gcc530 cmsrel CMSSW_8_1_0 cd CMSSW_8_1_0/src cmsenv git clone https://github.com/cms-analysis/HiggsAnalysis-CombinedLimit.git HiggsAnalysis/CombinedLimit cd HiggsAnalysis/CombinedLimit Update to a reccomended tag - currently the reccomended tag is v7.0.12 : cd $CMSSW_BASE/src/HiggsAnalysis/CombinedLimit git fetch origin git checkout v7.0.12 scramv1 b clean; scramv1 b # always make a clean build You can generate a diff of any two tags (eg for v7.0.8 and v7.0.6 ) by using following the url: https://github.com/cms-analysis/HiggsAnalysis-CombinedLimit/compare/v7.0.6...v7.0.7 Replace the tag names in the url to any tags you which to compare.","title":"ROOT6 SLC6 release CMSSW_8_1_X - recommended version"},{"location":"part1/gettingstarted/#for-developers","text":"We use the Fork and Pull model for development: each user creates a copy of the repository on github, commits their requests there and then sends pull requests for the administrators to merge. Prerequisites Register on github, as needed anyway for CMSSW development: http://cms-sw.github.io/cmssw/faq.html Register your SSH key on github: https://help.github.com/articles/generating-ssh-keys 1 Fork the repository to create your copy of it: https://github.com/cms-analysis/HiggsAnalysis-CombinedLimit/fork (more documentation at https://help.github.com/articles/fork-a-repo ) You will now be able to browse your fork of the repository from https://github.com/your-github-user-name/HiggsAnalysis-CombinedLimit","title":"For developers"},{"location":"part1/gettingstarted/#recommended-way-to-develop-a-feature-in-a-branch","text":"# get the updates of the master branch of the remote repository git fetch upstream # branch straight off the upstream master git checkout -b feature_name_branch upstream/81x-root606 # implement the feature # commit, etc # before publishing: # get the updates of the master branch of the remote repository git fetch upstream # if you're ready to integrate the upstream changes into your repository do git rebase upstream/81x-root606 # fix any conflicts git push origin feature_name_branch And proceed to make a pull request from the branch you created.","title":"Recommended way to develop a feature (in a branch)"},{"location":"part1/gettingstarted/#committing-changes-to-your-repository","text":"git add .... git commit -m \"....\" git push You can now make a pull request to the repository.","title":"Committing changes to your repository"},{"location":"part1/gettingstarted/#combine-tool","text":"An additional tool for submitting combine jobs to batch/crab, developed originally for HiggsToTauTau. Since the repository contains a certain amount of analysis-specific code, the following scripts can be used to clone it with a sparse checkout for just the core CombineHarvester/CombineTools subpackage, speeding up the checkout and compile times: git clone via ssh: bash <(curl -s https://raw.githubusercontent.com/cms-analysis/CombineHarvester/master/CombineTools/scripts/sparse-checkout-ssh.sh) git clone via https: bash <(curl -s https://raw.githubusercontent.com/cms-analysis/CombineHarvester/master/CombineTools/scripts/sparse-checkout-https.sh) make sure to run scram to compile the CombineTools package. See the CombineHarvester documentation pages for more details on using this tool and additional features available in the full package.","title":"Combine Tool"},{"location":"part2/bin-wise-stats/","text":"Automatic statistical uncertainties Introduction The text2workspace.py script is now able to produce a new type of workspace in which bin-wise statistical uncertainties are added automatically. This can be built for shape-based datacards where the inputs are in TH1 format. Datacards that use RooDataHists are not supported. The bin errrors (i.e. values returned by TH1::GetBinError) are used to model the uncertainties. By default the script will attempt to assign a single nuisance parameter to scale the sum of the process yields in each bin, constrained by the total uncertainty, instead of requiring separate parameters, one per process. This is sometimes referred to as the Barlow-Beeston -lite approach, and is useful as it minimises the number of parameters required in the maximum-likelihood fit. A useful description of this approach may be found in section 5 of this report . Usage instructions The following line should be added at the bottom of the datacard, underneath the systematics, to produce a new-style workspace and optionally enable the automatic bin-wise uncertainties: [channel] autoMCStats [threshold] [include-signal = 0] [hist-mode = 1] The first string channel should give the name of the channels (bins) in the datacard for which the new histogram classes should be used. The wildcard * is supported for selecting multiple channels in one go. The value of threshold should be set to a value greater than or equal to zero to enable the creation of automatic bin-wise uncertainties, or -1 to use the new histogram classes without these uncertainties. A positive value sets the threshold on the effective number of unweighted events above which the uncertainty will be modeled with the Barlow-Beeston-lite approach described above. Below the threshold an individual uncertainty per-process will be created. The algorithm is described in more detail below. The last two settings are optional. The first of these, include-signal has a default value of 0 but can be set to 1 as an alternative. By default the total nominal yield and uncertainty used to test the threshold excludes signal processes, as typically the initial signal normalisation is arbitrary, and could unduly lead to a bin being considered well-populated despite poorly populated background templates. Setting this flag will include the signal processes in the uncertainty analysis. Note that this option only affects the logic for creating a single Barlow-Beeston-lite parameter vs. separate per-process parameters - the uncertainties on all signal processes are always included in the actual model! The second flag changes the way the normalisation effect of shape-altering uncertainties is handled. In the default mode ( 1 ) the normalisation is handled separately from the shape morphing via a an asymmetric log-normal term. This is identical to how combine has always handled shape morphing. When set to 2 , the normalisation will be adjusted in the shape morphing directly. Unless there is a strong motivation we encourage users to leave this on the default setting. Description of the algorithm When threshold is set to a number of effective unweighted events greater than or equal to zero, denoted n^{\\text{threshold}} n^{\\text{threshold}} , the following algorithm is applied to each bin: Sum the yields n_{i} n_{i} and uncertainities e_{i} e_{i} of each background process i i in the bin. Note that the n_i n_i and e_i e_i include the nominal effect of any scaling parameters that have been set in the datacard, for example rateParams . n_{\\text{tot}} = \\sum_{i\\,\\in\\,\\text{bkg}}n_i n_{\\text{tot}} = \\sum_{i\\,\\in\\,\\text{bkg}}n_i , e_{\\text{tot}} = \\sqrt{\\sum_{i\\,\\in\\,\\text{bkg}}e_i^{2}} e_{\\text{tot}} = \\sqrt{\\sum_{i\\,\\in\\,\\text{bkg}}e_i^{2}} If e_{\\text{tot}} = 0 e_{\\text{tot}} = 0 , the bin is skipped and no parameters are created. (Though you might want to check why there is no uncertainty on the background prediction in this bin!) The effective number of unweighted events is defined as n_{\\text{tot}}^{\\text{eff}} = n_{\\text{tot}}^{2} / e_{\\text{tot}}^{2} n_{\\text{tot}}^{\\text{eff}} = n_{\\text{tot}}^{2} / e_{\\text{tot}}^{2} , rounded to the nearest integer. If n_{\\text{tot}}^{\\text{eff}} \\leq n^{\\text{threshold}} n_{\\text{tot}}^{\\text{eff}} \\leq n^{\\text{threshold}} : separate uncertainties will be created for each process. Processes where e_{i} = 0 e_{i} = 0 are skipped. If the number of effective events for a given process is lower than n^{\\text{threshold}} n^{\\text{threshold}} a Poisson-constrained parameter will be created. Otherwise a Gaussian-constrained parameter is used. If n_{\\text{tot}}^{\\text{eff}} \\gt n^{\\text{threshold}} n_{\\text{tot}}^{\\text{eff}} \\gt n^{\\text{threshold}} : A single Gaussian-constrained Barlow-Beeston-lite parameter is created that will scale the total yield in the bin. Note that the values of e_{i} e_{i} and therefore e_{tot} e_{tot} will be updated automatically in the model whenever the process normalisations change. A Gaussian-constrained parameter x x has a nominal value of zero and scales the yield as n_{\\text{tot}} + x \\cdot e_{\\text{tot}} n_{\\text{tot}} + x \\cdot e_{\\text{tot}} . The Poisson-constrained parameters are expressed as a yield multiplier with nominal value one: n_{\\text{tot}} \\cdot x n_{\\text{tot}} \\cdot x . The output from text2workspace.py will give details on how each bin has been treated by this alogorithm, for example: Show example output ============================================================ Analysing bin errors for: prop_binhtt_et_6_7TeV Poisson cut-off: 10 Processes excluded for sums: ZH qqH WH ggH ============================================================ Bin Contents Error Notes 0 0.000000 0.000000 total sum 0 0.000000 0.000000 excluding marked processes => Error is zero, ignore ------------------------------------------------------------ 1 0.120983 0.035333 total sum 1 0.120983 0.035333 excluding marked processes 1 12.000000 3.464102 Unweighted events, alpha=0.010082 => Total parameter prop_binhtt_et_6_7TeV_bin1[0.00,-7.00,7.00] to be gaussian constrained ------------------------------------------------------------ 2 0.472198 0.232096 total sum 2 0.472198 0.232096 excluding marked processes 2 4.000000 2.000000 Unweighted events, alpha=0.118049 => Number of weighted events is below poisson threshold ZH 0.000000 0.000000 => Error is zero, ignore ---------------------------------------------------------- W 0.050606 0.029220 3.000000 1.732051 Unweighted events, alpha=0.016869 => Product of prop_binhtt_et_6_7TeV_bin2_W[1.00,0.00,12.15] and const [3] to be poisson constrained ---------------------------------------------------------- ZJ 0.142444 0.140865 1.000000 1.000000 Unweighted events, alpha=0.142444 => Product of prop_binhtt_et_6_7TeV_bin2_ZJ[1.00,0.00,30.85] and const [1] to be poisson constrained ---------------------------------------------------------- Analytic minimisation One significant advantage of the Barlow-Beeston-lite approach is that the maximum likelihood estimate of each nuisance parameter has a simple analytic form that depends only on n_{\\text{tot}} n_{\\text{tot}} , e_{\\text{tot}} e_{\\text{tot}} and the observed number of data events in the relevant bin. Therefore when minimising the negative log-likelihood of the whole model it is possible to remove these parameters from the fit and set them to their best-fit values automatically. For models with large numbers of bins this can reduce the fit time and increase the fit stability. The analytic minimisation is enabled by default starting in combine v8.2.0, you can disable it by adding the option --X-rtd MINIMIZER_no_analytic when running combine. Technical details Up until recently text2workspace.py would only construct the PDF for each channel using a RooAddPdf , i.e. each component process is represented by a separate PDF and normalisation coefficient. However in order to model bin-wise statistical uncertainties the alternative RooRealSumPdf can be more useful, as each process is represented by a RooFit function object instead of a PDF, and we can vary the bin yields directly. As such, a new RooFit histogram class CMSHistFunc is introduced, which offers the same vertical template morphing algorithms offered by the current default histogram PDF, FastVerticalInterpHistPdf2 . Accompanying this is the CMSHistErrorPropagator class. This evaluates a sum of CMSHistFunc objects, each multiplied by a coefficient. It is also able to scale the summed yield of each bin to account for bin-wise statistical uncertainty nuisance parameters. Warning One disadvantage of this new approach comes when evaluating the expectation for individual processes, for example when using the --saveShapes option in the FitDiagnostics mode of combine. The Barlow-Beeston-lite parameters scale the sum of the process yields directly, so extra work is needed in the distribution this total scaling back to each individual process. To achieve this an additional class CMSHistFuncWrapper has been created that, given a particular CMSHistFunc , the CMSHistErrorPropagator will distribute an appropriate fraction of the total yield shift to each bin. As a consequence of the extra computation needed to distribute the yield shifts in this way the evaluation of individual process shapes in --saveShapes can take longer then previously.","title":"Automatic MC statistical uncertainties"},{"location":"part2/bin-wise-stats/#automatic-statistical-uncertainties","text":"","title":"Automatic statistical uncertainties"},{"location":"part2/bin-wise-stats/#introduction","text":"The text2workspace.py script is now able to produce a new type of workspace in which bin-wise statistical uncertainties are added automatically. This can be built for shape-based datacards where the inputs are in TH1 format. Datacards that use RooDataHists are not supported. The bin errrors (i.e. values returned by TH1::GetBinError) are used to model the uncertainties. By default the script will attempt to assign a single nuisance parameter to scale the sum of the process yields in each bin, constrained by the total uncertainty, instead of requiring separate parameters, one per process. This is sometimes referred to as the Barlow-Beeston -lite approach, and is useful as it minimises the number of parameters required in the maximum-likelihood fit. A useful description of this approach may be found in section 5 of this report .","title":"Introduction"},{"location":"part2/bin-wise-stats/#usage-instructions","text":"The following line should be added at the bottom of the datacard, underneath the systematics, to produce a new-style workspace and optionally enable the automatic bin-wise uncertainties: [channel] autoMCStats [threshold] [include-signal = 0] [hist-mode = 1] The first string channel should give the name of the channels (bins) in the datacard for which the new histogram classes should be used. The wildcard * is supported for selecting multiple channels in one go. The value of threshold should be set to a value greater than or equal to zero to enable the creation of automatic bin-wise uncertainties, or -1 to use the new histogram classes without these uncertainties. A positive value sets the threshold on the effective number of unweighted events above which the uncertainty will be modeled with the Barlow-Beeston-lite approach described above. Below the threshold an individual uncertainty per-process will be created. The algorithm is described in more detail below. The last two settings are optional. The first of these, include-signal has a default value of 0 but can be set to 1 as an alternative. By default the total nominal yield and uncertainty used to test the threshold excludes signal processes, as typically the initial signal normalisation is arbitrary, and could unduly lead to a bin being considered well-populated despite poorly populated background templates. Setting this flag will include the signal processes in the uncertainty analysis. Note that this option only affects the logic for creating a single Barlow-Beeston-lite parameter vs. separate per-process parameters - the uncertainties on all signal processes are always included in the actual model! The second flag changes the way the normalisation effect of shape-altering uncertainties is handled. In the default mode ( 1 ) the normalisation is handled separately from the shape morphing via a an asymmetric log-normal term. This is identical to how combine has always handled shape morphing. When set to 2 , the normalisation will be adjusted in the shape morphing directly. Unless there is a strong motivation we encourage users to leave this on the default setting.","title":"Usage instructions"},{"location":"part2/bin-wise-stats/#description-of-the-algorithm","text":"When threshold is set to a number of effective unweighted events greater than or equal to zero, denoted n^{\\text{threshold}} n^{\\text{threshold}} , the following algorithm is applied to each bin: Sum the yields n_{i} n_{i} and uncertainities e_{i} e_{i} of each background process i i in the bin. Note that the n_i n_i and e_i e_i include the nominal effect of any scaling parameters that have been set in the datacard, for example rateParams . n_{\\text{tot}} = \\sum_{i\\,\\in\\,\\text{bkg}}n_i n_{\\text{tot}} = \\sum_{i\\,\\in\\,\\text{bkg}}n_i , e_{\\text{tot}} = \\sqrt{\\sum_{i\\,\\in\\,\\text{bkg}}e_i^{2}} e_{\\text{tot}} = \\sqrt{\\sum_{i\\,\\in\\,\\text{bkg}}e_i^{2}} If e_{\\text{tot}} = 0 e_{\\text{tot}} = 0 , the bin is skipped and no parameters are created. (Though you might want to check why there is no uncertainty on the background prediction in this bin!) The effective number of unweighted events is defined as n_{\\text{tot}}^{\\text{eff}} = n_{\\text{tot}}^{2} / e_{\\text{tot}}^{2} n_{\\text{tot}}^{\\text{eff}} = n_{\\text{tot}}^{2} / e_{\\text{tot}}^{2} , rounded to the nearest integer. If n_{\\text{tot}}^{\\text{eff}} \\leq n^{\\text{threshold}} n_{\\text{tot}}^{\\text{eff}} \\leq n^{\\text{threshold}} : separate uncertainties will be created for each process. Processes where e_{i} = 0 e_{i} = 0 are skipped. If the number of effective events for a given process is lower than n^{\\text{threshold}} n^{\\text{threshold}} a Poisson-constrained parameter will be created. Otherwise a Gaussian-constrained parameter is used. If n_{\\text{tot}}^{\\text{eff}} \\gt n^{\\text{threshold}} n_{\\text{tot}}^{\\text{eff}} \\gt n^{\\text{threshold}} : A single Gaussian-constrained Barlow-Beeston-lite parameter is created that will scale the total yield in the bin. Note that the values of e_{i} e_{i} and therefore e_{tot} e_{tot} will be updated automatically in the model whenever the process normalisations change. A Gaussian-constrained parameter x x has a nominal value of zero and scales the yield as n_{\\text{tot}} + x \\cdot e_{\\text{tot}} n_{\\text{tot}} + x \\cdot e_{\\text{tot}} . The Poisson-constrained parameters are expressed as a yield multiplier with nominal value one: n_{\\text{tot}} \\cdot x n_{\\text{tot}} \\cdot x . The output from text2workspace.py will give details on how each bin has been treated by this alogorithm, for example: Show example output ============================================================ Analysing bin errors for: prop_binhtt_et_6_7TeV Poisson cut-off: 10 Processes excluded for sums: ZH qqH WH ggH ============================================================ Bin Contents Error Notes 0 0.000000 0.000000 total sum 0 0.000000 0.000000 excluding marked processes => Error is zero, ignore ------------------------------------------------------------ 1 0.120983 0.035333 total sum 1 0.120983 0.035333 excluding marked processes 1 12.000000 3.464102 Unweighted events, alpha=0.010082 => Total parameter prop_binhtt_et_6_7TeV_bin1[0.00,-7.00,7.00] to be gaussian constrained ------------------------------------------------------------ 2 0.472198 0.232096 total sum 2 0.472198 0.232096 excluding marked processes 2 4.000000 2.000000 Unweighted events, alpha=0.118049 => Number of weighted events is below poisson threshold ZH 0.000000 0.000000 => Error is zero, ignore ---------------------------------------------------------- W 0.050606 0.029220 3.000000 1.732051 Unweighted events, alpha=0.016869 => Product of prop_binhtt_et_6_7TeV_bin2_W[1.00,0.00,12.15] and const [3] to be poisson constrained ---------------------------------------------------------- ZJ 0.142444 0.140865 1.000000 1.000000 Unweighted events, alpha=0.142444 => Product of prop_binhtt_et_6_7TeV_bin2_ZJ[1.00,0.00,30.85] and const [1] to be poisson constrained ----------------------------------------------------------","title":"Description of the algorithm"},{"location":"part2/bin-wise-stats/#analytic-minimisation","text":"One significant advantage of the Barlow-Beeston-lite approach is that the maximum likelihood estimate of each nuisance parameter has a simple analytic form that depends only on n_{\\text{tot}} n_{\\text{tot}} , e_{\\text{tot}} e_{\\text{tot}} and the observed number of data events in the relevant bin. Therefore when minimising the negative log-likelihood of the whole model it is possible to remove these parameters from the fit and set them to their best-fit values automatically. For models with large numbers of bins this can reduce the fit time and increase the fit stability. The analytic minimisation is enabled by default starting in combine v8.2.0, you can disable it by adding the option --X-rtd MINIMIZER_no_analytic when running combine.","title":"Analytic minimisation"},{"location":"part2/bin-wise-stats/#technical-details","text":"Up until recently text2workspace.py would only construct the PDF for each channel using a RooAddPdf , i.e. each component process is represented by a separate PDF and normalisation coefficient. However in order to model bin-wise statistical uncertainties the alternative RooRealSumPdf can be more useful, as each process is represented by a RooFit function object instead of a PDF, and we can vary the bin yields directly. As such, a new RooFit histogram class CMSHistFunc is introduced, which offers the same vertical template morphing algorithms offered by the current default histogram PDF, FastVerticalInterpHistPdf2 . Accompanying this is the CMSHistErrorPropagator class. This evaluates a sum of CMSHistFunc objects, each multiplied by a coefficient. It is also able to scale the summed yield of each bin to account for bin-wise statistical uncertainty nuisance parameters. Warning One disadvantage of this new approach comes when evaluating the expectation for individual processes, for example when using the --saveShapes option in the FitDiagnostics mode of combine. The Barlow-Beeston-lite parameters scale the sum of the process yields directly, so extra work is needed in the distribution this total scaling back to each individual process. To achieve this an additional class CMSHistFuncWrapper has been created that, given a particular CMSHistFunc , the CMSHistErrorPropagator will distribute an appropriate fraction of the total yield shift to each bin. As a consequence of the extra computation needed to distribute the yield shifts in this way the evaluation of individual process shapes in --saveShapes can take longer then previously.","title":"Technical details"},{"location":"part2/physicsmodels/","text":"Physics Models Combine can be run directly on the text based datacard. However, for more advanced physics models, the internal step to convert the datacard to a binary workspace can be performed by the user. To create a binary workspace starting from a datacard.txt , just do text2workspace.py datacard.txt -o workspace.root By default (without the -o option), the binary workspace will be named datacard.root - i.e the .txt suffix will be replaced by .root . A full set of options for text2workspace can be found by using --help . The default model which will be produced when running text2workspace is one in which all processes identified as signal are multiplied by a common multiplier r . This is all that is needed for simply setting limits or calculating significances. text2workspace will convert the datacard into a pdf which summaries the analysis. For example, lets take a look at the data/tutorials/counting/simple-counting-experiment.txt datacard. # Simple counting experiment, with one signal and one background process # Extremely simplified version of the 35/pb H->WW analysis for mH = 200 GeV, # for 4th generation exclusion (EWK-10-009, arxiv:1102.5429v1) imax 1 number of channels jmax 1 number of backgrounds kmax 2 number of nuisance parameters (sources of systematical uncertainties) ------------ # we have just one channel, in which we observe 0 events bin 1 observation 0 ------------ # now we list the expected events for signal and all backgrounds in that bin # the second 'process' line must have a positive number for backgrounds, and 0 for signal # then we list the independent sources of uncertainties, and give their effect (syst. error) # on each process and bin bin 1 1 process ggh4G Bckg process 0 1 rate 4.76 1.47 ------------ deltaS lnN 1.20 - 20% uncertainty on signal deltaB lnN - 1.50 50% uncertainty on background If we run text2workspace.py on this datacard and take a look at the workspace ( w ) inside the .root file produced, we will find a number of different objects representing the signal, background and observed event rates as well as the nuisance parameters and signal strength r . From these objects, the necessary pdf has been constructed (named model_s ). For this counting experiment we will expect a simple pdf of the form p(n_{\\mathrm{obs}}| r,\\delta_{S},\\delta_{B})\\propto \\dfrac{[r\\cdot n_{S}(\\delta_{S})+n_{B}(\\delta_{B})]^{n_{\\mathrm{obs}}} } {n_{\\mathrm{obs}}!}e^{-[r\\cdot n_{S}(\\delta_{S})+n_{B}(\\delta_{B})]} \\cdot e^{-\\frac{1}{2}(\\delta_{S}- \\delta_{S}^{\\mathrm{In}})^{2}} \\cdot e^{-\\frac{1}{2}(\\delta_{B}- \\delta_{B}^{\\mathrm{In}})^{2}} p(n_{\\mathrm{obs}}| r,\\delta_{S},\\delta_{B})\\propto \\dfrac{[r\\cdot n_{S}(\\delta_{S})+n_{B}(\\delta_{B})]^{n_{\\mathrm{obs}}} } {n_{\\mathrm{obs}}!}e^{-[r\\cdot n_{S}(\\delta_{S})+n_{B}(\\delta_{B})]} \\cdot e^{-\\frac{1}{2}(\\delta_{S}- \\delta_{S}^{\\mathrm{In}})^{2}} \\cdot e^{-\\frac{1}{2}(\\delta_{B}- \\delta_{B}^{\\mathrm{In}})^{2}} where the expected signal and background rates are expressed as functions of the nuisance parameters, n_{S}(\\delta_{S}) = 4.76(1+0.2)^{\\delta_{S}}~ n_{S}(\\delta_{S}) = 4.76(1+0.2)^{\\delta_{S}}~ and ~n_{B}(\\delta_{B}) = 1.47(1+0.5)^{\\delta_{B}} ~n_{B}(\\delta_{B}) = 1.47(1+0.5)^{\\delta_{B}} . The first term represents the usual Poisson expression for observing n_{\\mathrm{obs}} n_{\\mathrm{obs}} events while the second two are the Gaussian constraint terms for the nuisance parameters. In this case {\\delta^{\\mathrm{In}}_S}={\\delta^{\\mathrm{In}}_B}=0 {\\delta^{\\mathrm{In}}_S}={\\delta^{\\mathrm{In}}_B}=0 , and the widths of both Gaussians are 1. A combination of counting experiments (or a binned shape datacard) will look like a product of pdfs of this kind. For a parametric/unbinned analyses, the pdf for each process in each channel is provided instead of the using the Poisson terms and a product is over the bin counts/events. Model building For more complex models, PhysicsModels can be produced. To use a different physics model instead of the default one, use the option -P as in text2workspace.py datacard -P HiggsAnalysis.CombinedLimit.PythonFile:modelName Generic models can be implemented by writing a python class that: defines the model parameters (by default it's just the signal strength modifier r ) defines how signal and background yields depend on the parameters (by default, signal scale linearly with r , backgrounds are constant) potentially also modifies the systematics (e.g. switch off theory uncertainties on cross section when measuring the cross section itself) In the case of SM-like Higgs searches the class should inherit from SMLikeHiggsModel (redefining getHiggsSignalYieldScale ), while beyond that one can inherit from PhysicsModel . You can find some examples in PhysicsModel.py . In the 4-process model ( PhysicsModel:floatingXSHiggs , you will see that each of the 4 dominant Higgs production modes get separate scaling parameters, r_ggH , r_qqH , r_ttH and r_VH (or r_ZH and r_WH ) as defined in, def doParametersOfInterest(self): \"\"\"Create POI and other parameters, and define the POI set.\"\"\" # --- Signal Strength as only POI --- if \"ggH\" in self.modes: self.modelBuilder.doVar(\"r_ggH[1,%s,%s]\" % (self.ggHRange[0], self.ggHRange[1])) if \"qqH\" in self.modes: self.modelBuilder.doVar(\"r_qqH[1,%s,%s]\" % (self.qqHRange[0], self.qqHRange[1])) if \"VH\" in self.modes: self.modelBuilder.doVar(\"r_VH[1,%s,%s]\" % (self.VHRange [0], self.VHRange [1])) if \"WH\" in self.modes: self.modelBuilder.doVar(\"r_WH[1,%s,%s]\" % (self.WHRange [0], self.WHRange [1])) if \"ZH\" in self.modes: self.modelBuilder.doVar(\"r_ZH[1,%s,%s]\" % (self.ZHRange [0], self.ZHRange [1])) if \"ttH\" in self.modes: self.modelBuilder.doVar(\"r_ttH[1,%s,%s]\" % (self.ttHRange[0], self.ttHRange[1])) poi = \",\".join([\"r_\"+m for m in self.modes]) if self.pois: poi = self.pois ... The mapping of which POI scales which process is handled via the following function, def getHiggsSignalYieldScale(self,production,decay, energy): if production == \"ggH\": return (\"r_ggH\" if \"ggH\" in self.modes else 1) if production == \"qqH\": return (\"r_qqH\" if \"qqH\" in self.modes else 1) if production == \"ttH\": return (\"r_ttH\" if \"ttH\" in self.modes else (\"r_ggH\" if self.ttHasggH else 1)) if production in [ \"WH\", \"ZH\", \"VH\" ]: return (\"r_VH\" if \"VH\" in self.modes else 1) raise RuntimeError, \"Unknown production mode '%s'\" % production You should note that text2workspace will look for the python module in PYTHONPATH . If you want to keep your model local, you'll need to add the location of the python file to PYTHONPATH . A number of models used in the LHC Higgs combination paper can be found in LHCHCGModels.py . These can be easily accessed by providing for example -P HiggsAnalysis.CombinedLimit.HiggsCouplings:c7 and others defined un HiggsCouplings.py . Below are some (more generic) example models which also exist in gitHub. MultiSignalModel ready made model for multiple signal processes Combine already contains a model HiggsAnalysis.CombinedLimit.PhysicsModel:multiSignalModel that can be used to assign different signal strengths to multiple processes in a datacard, configurable from the command line. The model is configured passing to text2workspace one or more mappings in the form --PO 'map=bin/process:parameter' bin and process can be arbitrary regular expressions matching the bin names and process names in the datacard Note that mappings are applied both to signals and to background processes; if a line matches multiple mappings, precedence is given to the last one in the order they are in the command line. it is suggested to put quotes around the argument of --PO so that the shell does not try to expand any * signs in the patterns. parameter is the POI to use to scale that process ( name[starting_value,min,max] the first time a parameter is defined, then just name if used more than once) Special values are 1 and 0==; ==0 means to drop the process completely from the card, while 1 means to keep the yield as is in the card with no scaling (as normally done for backgrounds); 1 is the default that is applied to processes that have no mappings, so it's normally not needed, but it may be used either to make the thing explicit, or to override a previous more generic match on the same command line (e.g. --PO 'map=.*/ggH:r[1,0,5]' --PO 'map=bin37/ggH:1' would treat ggH as signal in general, but count it as background in the channel bin37 ) Passing the additional option --PO verbose will set the code to verbose mode, printing out the scaling factors for each process; people are encouraged to use this option to make sure that the processes are being scaled correctly. The MultiSignalModel will define all parameters as parameters of interest, but that can be then changed from the command line of combine, as described in the following sub-section. Some examples, taking as reference the toy datacard test/multiDim/toy-hgg-125.txt : Scale both ggH and qqH with the same signal strength r (that's what the default physics model of combine does for all signals; if they all have the same systematic uncertainties, it is also equivalent to adding up their yields and writing them as a single column in the card) $ text2workspace.py -P HiggsAnalysis.CombinedLimit.PhysicsModel:multiSignalModel --PO verbose --PO 'map=.*/ggH:r[1,0,10]' --PO 'map=.*/qqH:r' toy-hgg-125.txt -o toy-1d.root [...] Will create a POI r with factory r[1,0,10] Mapping r to ['.*/ggH'] patterns Mapping r to ['.*/qqH'] patterns [...] Will scale incl/bkg by 1 Will scale incl/ggH by r Will scale incl/qqH by r Will scale dijet/bkg by 1 Will scale dijet/ggH by r Will scale dijet/qqH by r Define two independent parameters of interest r_ggH and r_qqH $ text2workspace.py -P HiggsAnalysis.CombinedLimit.PhysicsModel:multiSignalModel --PO verbose --PO 'map=.*/ggH:r_ggH[1,0,10]' --PO 'map=.*/qqH:r_qqH[1,0,20]' toy-hgg-125.txt -o toy-2d.root [...] Will create a POI r_ggH with factory r_ggH[1,0,10] Mapping r_ggH to ['.*/ggH'] patterns Will create a POI r_qqH with factory r_qqH[1,0,20] Mapping r_qqH to ['.*/qqH'] patterns [...] Will scale incl/bkg by 1 Will scale incl/ggH by r_ggH Will scale incl/qqH by r_qqH Will scale dijet/bkg by 1 Will scale dijet/ggH by r_ggH Will scale dijet/qqH by r_qqH Fix ggH to SM, define only qqH as parameter $ text2workspace.py -P HiggsAnalysis.CombinedLimit.PhysicsModel:multiSignalModel --PO verbose --PO 'map=.*/ggH:1' --PO 'map=.*/qqH:r_qqH[1,0,20]' toy-hgg-125.txt -o toy-1d-qqH.root [...] Mapping 1 to ['.*/ggH'] patterns Will create a POI r_qqH with factory r_qqH[1,0,20] Mapping r_qqH to ['.*/qqH'] patterns [...] Will scale incl/bkg by 1 Will scale incl/ggH by 1 Will scale incl/qqH by r_qqH Will scale dijet/bkg by 1 Will scale dijet/ggH by 1 Will scale dijet/qqH by r_qqH Drop ggH , and define only qqH as parameter $ text2workspace.py -P HiggsAnalysis.CombinedLimit.PhysicsModel:multiSignalModel --PO verbose --PO 'map=.*/ggH:0' --PO 'map=.*/qqH:r_qqH[1,0,20]' toy-hgg-125.txt -o toy-1d-qqH0-only.root [...] Mapping 0 to ['.*/ggH'] patterns Will create a POI r_qqH with factory r_qqH[1,0,20] Mapping r_qqH to ['.*/qqH'] patterns [...] Will scale incl/bkg by 1 Will scale incl/ggH by 0 Will scale incl/qqH by r_qqH Will scale dijet/bkg by 1 Will scale dijet/ggH by 0 Will scale dijet/qqH by r_qqH Two Hypothesis testing The PhysicsModel that encodes the signal model above is the twoHypothesisHiggs , which assumes that there will exist signal processes with suffix _ALT in the datacard. An example of such a datacard can be found under data/benchmarks/simple-counting/twoSignals-3bin-bigBSyst.txt $ text2workspace.py twoSignals-3bin-bigBSyst.txt -P HiggsAnalysis.CombinedLimit.HiggsJPC:twoHypothesisHiggs -m 125.7 --PO verbose -o jcp_hww.root MH (not there before) will be assumed to be 125.7 Process S will get norm not_x Process S_ALT will get norm x Process S will get norm not_x Process S_ALT will get norm x Process S will get norm not_x Process S_ALT will get norm x The two processes (S and S_ALT) will get different scaling parameters. The LEP-style likelihood for hypothesis testing can now be performed by setting x or not_x to 1 and 0 and comparing two likelihood evaluations. Interference Since there are no such things as negative probability distribution functions, the recommended way to implement this is to start from the expression for the individual amplitudes and the parameter of interest k k , \\mathrm{Yield} = (k * A_{s} + A_{b})^2 = k^2 * A_{s}^2 + k * 2 A_{s} A_{b} + A_{b}^2 = \\mu * S + \\sqrt{\\mu} * I + B \\mathrm{Yield} = (k * A_{s} + A_{b})^2 = k^2 * A_{s}^2 + k * 2 A_{s} A_{b} + A_{b}^2 = \\mu * S + \\sqrt{\\mu} * I + B where \\mu = k^2, ~S = A_{s}^2,~B = Ab^2 \\mu = k^2, ~S = A_{s}^2,~B = Ab^2 and $ S+B+I = (As + Ab)^2$. With some algebra you can work out that, \\mathrm{Yield} = \\sqrt{\\mu} * \\left[S+B+I\\right] + (\\mu-\\sqrt{\\mu}) * \\left[S\\right] + (1-\\sqrt{\\mu}) * \\left[B\\right] \\mathrm{Yield} = \\sqrt{\\mu} * \\left[S+B+I\\right] + (\\mu-\\sqrt{\\mu}) * \\left[S\\right] + (1-\\sqrt{\\mu}) * \\left[B\\right] where square brackets represent the input (histograms as TH1 or RooDataHists ) that one needs to provide. An example of this scheme is implemented in a HiggsWidth and is completely general, since all of the three components above are strictly positive. In this example, the POI is CMS_zz4l_mu and the equations for the three components are scaled (separately for the qqH and ggH processes) as, self.modelBuilder.factory_( \"expr::ggH_s_func(\\\"@0-sqrt(@0)\\\", CMS_zz4l_mu)\") self.modelBuilder.factory_( \"expr::ggH_b_func(\\\"1-sqrt(@0)\\\", CMS_zz4l_mu)\") self.modelBuilder.factory_( \"expr::ggH_sbi_func(\\\"sqrt(@0)\\\", CMS_zz4l_mu)\") self.modelBuilder.factory_( \"expr::qqH_s_func(\\\"@0-sqrt(@0)\\\", CMS_zz4l_mu)\") self.modelBuilder.factory_( \"expr::qqH_b_func(\\\"1-sqrt(@0)\\\", CMS_zz4l_mu)\") self.modelBuilder.factory_( \"expr::qqH_sbi_func(\\\"sqrt(@0)\\\", CMS_zz4l_mu)\")","title":"Physics models"},{"location":"part2/physicsmodels/#physics-models","text":"Combine can be run directly on the text based datacard. However, for more advanced physics models, the internal step to convert the datacard to a binary workspace can be performed by the user. To create a binary workspace starting from a datacard.txt , just do text2workspace.py datacard.txt -o workspace.root By default (without the -o option), the binary workspace will be named datacard.root - i.e the .txt suffix will be replaced by .root . A full set of options for text2workspace can be found by using --help . The default model which will be produced when running text2workspace is one in which all processes identified as signal are multiplied by a common multiplier r . This is all that is needed for simply setting limits or calculating significances. text2workspace will convert the datacard into a pdf which summaries the analysis. For example, lets take a look at the data/tutorials/counting/simple-counting-experiment.txt datacard. # Simple counting experiment, with one signal and one background process # Extremely simplified version of the 35/pb H->WW analysis for mH = 200 GeV, # for 4th generation exclusion (EWK-10-009, arxiv:1102.5429v1) imax 1 number of channels jmax 1 number of backgrounds kmax 2 number of nuisance parameters (sources of systematical uncertainties) ------------ # we have just one channel, in which we observe 0 events bin 1 observation 0 ------------ # now we list the expected events for signal and all backgrounds in that bin # the second 'process' line must have a positive number for backgrounds, and 0 for signal # then we list the independent sources of uncertainties, and give their effect (syst. error) # on each process and bin bin 1 1 process ggh4G Bckg process 0 1 rate 4.76 1.47 ------------ deltaS lnN 1.20 - 20% uncertainty on signal deltaB lnN - 1.50 50% uncertainty on background If we run text2workspace.py on this datacard and take a look at the workspace ( w ) inside the .root file produced, we will find a number of different objects representing the signal, background and observed event rates as well as the nuisance parameters and signal strength r . From these objects, the necessary pdf has been constructed (named model_s ). For this counting experiment we will expect a simple pdf of the form p(n_{\\mathrm{obs}}| r,\\delta_{S},\\delta_{B})\\propto \\dfrac{[r\\cdot n_{S}(\\delta_{S})+n_{B}(\\delta_{B})]^{n_{\\mathrm{obs}}} } {n_{\\mathrm{obs}}!}e^{-[r\\cdot n_{S}(\\delta_{S})+n_{B}(\\delta_{B})]} \\cdot e^{-\\frac{1}{2}(\\delta_{S}- \\delta_{S}^{\\mathrm{In}})^{2}} \\cdot e^{-\\frac{1}{2}(\\delta_{B}- \\delta_{B}^{\\mathrm{In}})^{2}} p(n_{\\mathrm{obs}}| r,\\delta_{S},\\delta_{B})\\propto \\dfrac{[r\\cdot n_{S}(\\delta_{S})+n_{B}(\\delta_{B})]^{n_{\\mathrm{obs}}} } {n_{\\mathrm{obs}}!}e^{-[r\\cdot n_{S}(\\delta_{S})+n_{B}(\\delta_{B})]} \\cdot e^{-\\frac{1}{2}(\\delta_{S}- \\delta_{S}^{\\mathrm{In}})^{2}} \\cdot e^{-\\frac{1}{2}(\\delta_{B}- \\delta_{B}^{\\mathrm{In}})^{2}} where the expected signal and background rates are expressed as functions of the nuisance parameters, n_{S}(\\delta_{S}) = 4.76(1+0.2)^{\\delta_{S}}~ n_{S}(\\delta_{S}) = 4.76(1+0.2)^{\\delta_{S}}~ and ~n_{B}(\\delta_{B}) = 1.47(1+0.5)^{\\delta_{B}} ~n_{B}(\\delta_{B}) = 1.47(1+0.5)^{\\delta_{B}} . The first term represents the usual Poisson expression for observing n_{\\mathrm{obs}} n_{\\mathrm{obs}} events while the second two are the Gaussian constraint terms for the nuisance parameters. In this case {\\delta^{\\mathrm{In}}_S}={\\delta^{\\mathrm{In}}_B}=0 {\\delta^{\\mathrm{In}}_S}={\\delta^{\\mathrm{In}}_B}=0 , and the widths of both Gaussians are 1. A combination of counting experiments (or a binned shape datacard) will look like a product of pdfs of this kind. For a parametric/unbinned analyses, the pdf for each process in each channel is provided instead of the using the Poisson terms and a product is over the bin counts/events.","title":"Physics Models"},{"location":"part2/physicsmodels/#model-building","text":"For more complex models, PhysicsModels can be produced. To use a different physics model instead of the default one, use the option -P as in text2workspace.py datacard -P HiggsAnalysis.CombinedLimit.PythonFile:modelName Generic models can be implemented by writing a python class that: defines the model parameters (by default it's just the signal strength modifier r ) defines how signal and background yields depend on the parameters (by default, signal scale linearly with r , backgrounds are constant) potentially also modifies the systematics (e.g. switch off theory uncertainties on cross section when measuring the cross section itself) In the case of SM-like Higgs searches the class should inherit from SMLikeHiggsModel (redefining getHiggsSignalYieldScale ), while beyond that one can inherit from PhysicsModel . You can find some examples in PhysicsModel.py . In the 4-process model ( PhysicsModel:floatingXSHiggs , you will see that each of the 4 dominant Higgs production modes get separate scaling parameters, r_ggH , r_qqH , r_ttH and r_VH (or r_ZH and r_WH ) as defined in, def doParametersOfInterest(self): \"\"\"Create POI and other parameters, and define the POI set.\"\"\" # --- Signal Strength as only POI --- if \"ggH\" in self.modes: self.modelBuilder.doVar(\"r_ggH[1,%s,%s]\" % (self.ggHRange[0], self.ggHRange[1])) if \"qqH\" in self.modes: self.modelBuilder.doVar(\"r_qqH[1,%s,%s]\" % (self.qqHRange[0], self.qqHRange[1])) if \"VH\" in self.modes: self.modelBuilder.doVar(\"r_VH[1,%s,%s]\" % (self.VHRange [0], self.VHRange [1])) if \"WH\" in self.modes: self.modelBuilder.doVar(\"r_WH[1,%s,%s]\" % (self.WHRange [0], self.WHRange [1])) if \"ZH\" in self.modes: self.modelBuilder.doVar(\"r_ZH[1,%s,%s]\" % (self.ZHRange [0], self.ZHRange [1])) if \"ttH\" in self.modes: self.modelBuilder.doVar(\"r_ttH[1,%s,%s]\" % (self.ttHRange[0], self.ttHRange[1])) poi = \",\".join([\"r_\"+m for m in self.modes]) if self.pois: poi = self.pois ... The mapping of which POI scales which process is handled via the following function, def getHiggsSignalYieldScale(self,production,decay, energy): if production == \"ggH\": return (\"r_ggH\" if \"ggH\" in self.modes else 1) if production == \"qqH\": return (\"r_qqH\" if \"qqH\" in self.modes else 1) if production == \"ttH\": return (\"r_ttH\" if \"ttH\" in self.modes else (\"r_ggH\" if self.ttHasggH else 1)) if production in [ \"WH\", \"ZH\", \"VH\" ]: return (\"r_VH\" if \"VH\" in self.modes else 1) raise RuntimeError, \"Unknown production mode '%s'\" % production You should note that text2workspace will look for the python module in PYTHONPATH . If you want to keep your model local, you'll need to add the location of the python file to PYTHONPATH . A number of models used in the LHC Higgs combination paper can be found in LHCHCGModels.py . These can be easily accessed by providing for example -P HiggsAnalysis.CombinedLimit.HiggsCouplings:c7 and others defined un HiggsCouplings.py . Below are some (more generic) example models which also exist in gitHub.","title":"Model building"},{"location":"part2/physicsmodels/#multisignalmodel-ready-made-model-for-multiple-signal-processes","text":"Combine already contains a model HiggsAnalysis.CombinedLimit.PhysicsModel:multiSignalModel that can be used to assign different signal strengths to multiple processes in a datacard, configurable from the command line. The model is configured passing to text2workspace one or more mappings in the form --PO 'map=bin/process:parameter' bin and process can be arbitrary regular expressions matching the bin names and process names in the datacard Note that mappings are applied both to signals and to background processes; if a line matches multiple mappings, precedence is given to the last one in the order they are in the command line. it is suggested to put quotes around the argument of --PO so that the shell does not try to expand any * signs in the patterns. parameter is the POI to use to scale that process ( name[starting_value,min,max] the first time a parameter is defined, then just name if used more than once) Special values are 1 and 0==; ==0 means to drop the process completely from the card, while 1 means to keep the yield as is in the card with no scaling (as normally done for backgrounds); 1 is the default that is applied to processes that have no mappings, so it's normally not needed, but it may be used either to make the thing explicit, or to override a previous more generic match on the same command line (e.g. --PO 'map=.*/ggH:r[1,0,5]' --PO 'map=bin37/ggH:1' would treat ggH as signal in general, but count it as background in the channel bin37 ) Passing the additional option --PO verbose will set the code to verbose mode, printing out the scaling factors for each process; people are encouraged to use this option to make sure that the processes are being scaled correctly. The MultiSignalModel will define all parameters as parameters of interest, but that can be then changed from the command line of combine, as described in the following sub-section. Some examples, taking as reference the toy datacard test/multiDim/toy-hgg-125.txt : Scale both ggH and qqH with the same signal strength r (that's what the default physics model of combine does for all signals; if they all have the same systematic uncertainties, it is also equivalent to adding up their yields and writing them as a single column in the card) $ text2workspace.py -P HiggsAnalysis.CombinedLimit.PhysicsModel:multiSignalModel --PO verbose --PO 'map=.*/ggH:r[1,0,10]' --PO 'map=.*/qqH:r' toy-hgg-125.txt -o toy-1d.root [...] Will create a POI r with factory r[1,0,10] Mapping r to ['.*/ggH'] patterns Mapping r to ['.*/qqH'] patterns [...] Will scale incl/bkg by 1 Will scale incl/ggH by r Will scale incl/qqH by r Will scale dijet/bkg by 1 Will scale dijet/ggH by r Will scale dijet/qqH by r Define two independent parameters of interest r_ggH and r_qqH $ text2workspace.py -P HiggsAnalysis.CombinedLimit.PhysicsModel:multiSignalModel --PO verbose --PO 'map=.*/ggH:r_ggH[1,0,10]' --PO 'map=.*/qqH:r_qqH[1,0,20]' toy-hgg-125.txt -o toy-2d.root [...] Will create a POI r_ggH with factory r_ggH[1,0,10] Mapping r_ggH to ['.*/ggH'] patterns Will create a POI r_qqH with factory r_qqH[1,0,20] Mapping r_qqH to ['.*/qqH'] patterns [...] Will scale incl/bkg by 1 Will scale incl/ggH by r_ggH Will scale incl/qqH by r_qqH Will scale dijet/bkg by 1 Will scale dijet/ggH by r_ggH Will scale dijet/qqH by r_qqH Fix ggH to SM, define only qqH as parameter $ text2workspace.py -P HiggsAnalysis.CombinedLimit.PhysicsModel:multiSignalModel --PO verbose --PO 'map=.*/ggH:1' --PO 'map=.*/qqH:r_qqH[1,0,20]' toy-hgg-125.txt -o toy-1d-qqH.root [...] Mapping 1 to ['.*/ggH'] patterns Will create a POI r_qqH with factory r_qqH[1,0,20] Mapping r_qqH to ['.*/qqH'] patterns [...] Will scale incl/bkg by 1 Will scale incl/ggH by 1 Will scale incl/qqH by r_qqH Will scale dijet/bkg by 1 Will scale dijet/ggH by 1 Will scale dijet/qqH by r_qqH Drop ggH , and define only qqH as parameter $ text2workspace.py -P HiggsAnalysis.CombinedLimit.PhysicsModel:multiSignalModel --PO verbose --PO 'map=.*/ggH:0' --PO 'map=.*/qqH:r_qqH[1,0,20]' toy-hgg-125.txt -o toy-1d-qqH0-only.root [...] Mapping 0 to ['.*/ggH'] patterns Will create a POI r_qqH with factory r_qqH[1,0,20] Mapping r_qqH to ['.*/qqH'] patterns [...] Will scale incl/bkg by 1 Will scale incl/ggH by 0 Will scale incl/qqH by r_qqH Will scale dijet/bkg by 1 Will scale dijet/ggH by 0 Will scale dijet/qqH by r_qqH","title":"MultiSignalModel ready made model for multiple signal processes"},{"location":"part2/physicsmodels/#two-hypothesis-testing","text":"The PhysicsModel that encodes the signal model above is the twoHypothesisHiggs , which assumes that there will exist signal processes with suffix _ALT in the datacard. An example of such a datacard can be found under data/benchmarks/simple-counting/twoSignals-3bin-bigBSyst.txt $ text2workspace.py twoSignals-3bin-bigBSyst.txt -P HiggsAnalysis.CombinedLimit.HiggsJPC:twoHypothesisHiggs -m 125.7 --PO verbose -o jcp_hww.root MH (not there before) will be assumed to be 125.7 Process S will get norm not_x Process S_ALT will get norm x Process S will get norm not_x Process S_ALT will get norm x Process S will get norm not_x Process S_ALT will get norm x The two processes (S and S_ALT) will get different scaling parameters. The LEP-style likelihood for hypothesis testing can now be performed by setting x or not_x to 1 and 0 and comparing two likelihood evaluations.","title":"Two Hypothesis testing"},{"location":"part2/physicsmodels/#interference","text":"Since there are no such things as negative probability distribution functions, the recommended way to implement this is to start from the expression for the individual amplitudes and the parameter of interest k k , \\mathrm{Yield} = (k * A_{s} + A_{b})^2 = k^2 * A_{s}^2 + k * 2 A_{s} A_{b} + A_{b}^2 = \\mu * S + \\sqrt{\\mu} * I + B \\mathrm{Yield} = (k * A_{s} + A_{b})^2 = k^2 * A_{s}^2 + k * 2 A_{s} A_{b} + A_{b}^2 = \\mu * S + \\sqrt{\\mu} * I + B where \\mu = k^2, ~S = A_{s}^2,~B = Ab^2 \\mu = k^2, ~S = A_{s}^2,~B = Ab^2 and $ S+B+I = (As + Ab)^2$. With some algebra you can work out that, \\mathrm{Yield} = \\sqrt{\\mu} * \\left[S+B+I\\right] + (\\mu-\\sqrt{\\mu}) * \\left[S\\right] + (1-\\sqrt{\\mu}) * \\left[B\\right] \\mathrm{Yield} = \\sqrt{\\mu} * \\left[S+B+I\\right] + (\\mu-\\sqrt{\\mu}) * \\left[S\\right] + (1-\\sqrt{\\mu}) * \\left[B\\right] where square brackets represent the input (histograms as TH1 or RooDataHists ) that one needs to provide. An example of this scheme is implemented in a HiggsWidth and is completely general, since all of the three components above are strictly positive. In this example, the POI is CMS_zz4l_mu and the equations for the three components are scaled (separately for the qqH and ggH processes) as, self.modelBuilder.factory_( \"expr::ggH_s_func(\\\"@0-sqrt(@0)\\\", CMS_zz4l_mu)\") self.modelBuilder.factory_( \"expr::ggH_b_func(\\\"1-sqrt(@0)\\\", CMS_zz4l_mu)\") self.modelBuilder.factory_( \"expr::ggH_sbi_func(\\\"sqrt(@0)\\\", CMS_zz4l_mu)\") self.modelBuilder.factory_( \"expr::qqH_s_func(\\\"@0-sqrt(@0)\\\", CMS_zz4l_mu)\") self.modelBuilder.factory_( \"expr::qqH_b_func(\\\"1-sqrt(@0)\\\", CMS_zz4l_mu)\") self.modelBuilder.factory_( \"expr::qqH_sbi_func(\\\"sqrt(@0)\\\", CMS_zz4l_mu)\")","title":"Interference"},{"location":"part2/settinguptheanalysis/","text":"Preparing the datacard The input to combine, which defines the details of the experiment, is a datacard file is a plain ASCII file. This is true whether the experiment is a simple counting experiment or a shape analysis. A simple counting experiment The file data/tutorials/counting/realistic-counting-experiment.txt shows an example of a counting experiment. The first lines can be used as a description and are not parsed by the program. They have to begin with a \"#\": # Simple counting experiment, with one signal and a few background processes # Simplified version of the 35/pb H->WW analysis for mH = 160 GeV Then one declares the number of observables , imax , that are present in the model used to calculate limits/significances. The number of observables will typically be the number of channels in a counting experiment or the number of bins in a binned shape fit. (If one specifies for imax the value * it means \"figure it out from the rest of the datacard\", but in order to better catch mistakes it's recommended to specify it explicitly) imax 1 number of channels Then one declares the number of background sources to be considered, jmax , and the number of independent sources of systematic uncertainties , kmax : jmax 3 number of backgrounds kmax 5 number of nuisance parameters (sources of systematic uncertainties) In the example there is 1 channel, there are 3 background sources, and there are 5 independent sources of systematic uncertainty. Then there are the lines describing what is actually observed: the number of events observed in each channel. The first line, starting with bin defines the label used for each channel. In the example we have 1 channel, labelled 1 , and in the following line, observation , are listed the events observed, 0 in this example: # we have just one channel, in which we observe 0 events bin bin1 observation 0 Following is the part related to the number of events expected, for each bin and process, arranged in (#channels)*(#processes) columns. bin bin1 bin1 bin1 bin1 process ggH qqWW ggWW others process 0 1 2 3 rate 1.47 0.63 0.06 0.22 The bin line identifies the channel the column is referring to. It goes from 1 to the imax declared above. The first process line contains the labels of the various sources The second process line must have a positive number for backgrounds, and 0 or a negative number for the signals. You should use different process ids for different processes. The last line, rate , tells the expected yield of events in the specified bin and process All bins should be declared in increasing order, and within each bin one should include all processes in increasing order, specifying a 0 for processes that do not contribute. The last section contains the description of the systematic uncertainties: lumi lnN 1.11 - 1.11 - lumi affects both signal and gg->WW (mc-driven). lnN = lognormal xs_ggH lnN 1.16 - - - gg->H cross section + signal efficiency + other minor ones. WW_norm gmN 4 - 0.16 - - WW estimate of 0.64 comes from sidebands: 4 events in sideband times 0.16 (=> ~50% statistical uncertainty) xs_ggWW lnN - - 1.50 - 50% uncertainty on gg->WW cross section bg_others lnN - - - 1.30 30% uncertainty on the rest of the backgrounds the first columns is a label identifying the uncertainty the second column identifies the type of distribution used lnN stands for Log-normal , which is the recommended choice for multiplicative corrections (efficiencies, cross sections, ...). If \u0394x/x is the relative uncertainty on the multiplicative correction, one should put 1+\u0394x/x in the column corresponding to the process and channel. Asymmetric log-normals are instead supported by providing \u03ba down /\u03ba up where \u03ba down is the ratio of the the yield to the nominal value for a -1\u03c3 deviation of the nuisance and \u03ba up is the ratio of thyield to the nominal value for +1\\sigma +1\\sigma deviation. Note that for single-value log-normal with value \\kappa=1+\\Delta x/x \\kappa=1+\\Delta x/x , the yield of the process it is associated with is multiplied by \\kappa^{\\theta} \\kappa^{\\theta} . At \\theta=0 \\theta=0 the nominal yield is retained, at \\theta=1\\sigma \\theta=1\\sigma the yield is multiplied by \\kappa \\kappa and at \\theta=-1\\sigma \\theta=-1\\sigma the yield is multiplied by 1/\\kappa 1/\\kappa . This means that an uncertainty represented as 1.2 does not multiply the nominal yield by 0.8 for \\theta=-1\\sigma \\theta=-1\\sigma ; but by 0.8333. For large uncertainties that have a symmetric effect on the yield it may therefore be desirable to encode them as asymmetric log-normals instead. gmN stands for Gamma , and is the recommended choice for the statistical uncertainty on a background coming from the number of events in a control region (or in a MC sample with limited statistics). If the control region or MC contains N events, and the extrapolation factor from the control region to the signal region is \u03b1 then one shoud put N just after the gmN keyword, and then the value of \u03b1 in the proper column. Also, the yield in the rate row should match with N\u03b1 lnU stands for log-uniform distribution. A value of 1+\u03b5 in the column will imply that the yield of this background is allowed to float freely between x(1+\u03b5) and x/(1+\u03b5) (in particular, if \u03b5 is small, then this is approximately (x-\u0394x,x+\u0394x) with \u03b5=\u0394x/x ) This is normally useful when you want to set a large a-priori uncertainty on a given background and then rely on the correlation between channels to constrain it. Beware that while Gaussian-like uncertainties behave in a similar way under profiling and marginalization, uniform uncertainties do not, so the impact of the uncertainty on the result will depend on how the nuisances are treated. then there are (#channels)*(#processes) columns reporting the relative effect of the systematic uncertainty on the rate of each process in each channel. The columns are aligned with the ones in the previous lines declaring bins, processes and rates. In the example, there are 5 uncertainties: the first uncertainty affects the signal by 11%, and affects the ggWW process by 11% the second uncertainty affects the signal by 16% leaving the backgrounds unaffected the third line specifies that the qqWW background comes from a sideband with 4 observed events and an extrapolation factor of 0.16; the resulting uncertainty on the expected yield is 1/\\sqrt{4+1} 1/\\sqrt{4+1} = 45% the fourth uncertainty does not affect the signal, affects the ggWW background by 50%, leaving the other backgrounds unaffected the last uncertainty does not affect the signal, affects by 30% the others backgrounds, leaving the rest of the backgrounds unaffected Shape analysis The datacard has to be supplemented with two extensions: A new block of lines defining how channels and processes are mapped into shapes The block for systematics that can contain also rows with shape uncertainties. The expected shape can be parametric or not parametric. In the first case the parametric pdfs have to be given as input to the tool. In the latter case, for each channel, histograms have to be provided for the expected shape of each process. For what concerns data, they have to be provided as input to the tool as a histogram to perform a binned shape analysis and as a RooDataSet to perform an unbinned shape analysis. Warning If using RooFit based inputs (RooDataHists/RooDataSets/RooAbsPdfs) then you should be careful to use different RooRealVars as the observable in each category being combined. It is possible to use the same RooRealVar if the observable has the same range (and binning if using binned data) in each category though in most cases it is simpler to avoid doing so. Rates for shape analysis As with the counting experiment, the total nominal rate of a given process must be identified in the rate line of the datacard. However, there are special options for shape based analyses as follows A value of -1 in the rate line indicates to combine to calculate the rate from the input TH1 (via TH1::Integral) or RooDataSet/RooDataHist (via RooAbsData::sumEntries) For parametric shapes (RooAbsPdf), if a parameter is found in the input workspace with the name pdfname _norm the rate will be multiplied by the value of that parameter. Note that since this parameter can be freely floating, the normalization of a shape can be made to freely float this way. This can also be achieved through the use of rateParams Binned shape analysis For each channel, histograms have to be provided for the observed shape and for the expected shape of each process. Within each channel, all histograms must have the same binning. The normalization of the data histogram must correspond to the number of observed events The normalization of the expected histograms must match the expected yields The combine tool can take as input histograms saved as TH1 or as RooAbsHist in a RooFit workspace (an example of how to create a RooFit workspace and save histograms is available in github ). The block of lines defining the mapping (first block in the datacard) contains one or more rows in the form shapes process channel file histogram [histogram_with_systematics] In this line process is any one the process names, or * for all processes, or data_obs for the observed data channel is any one the process names, or * for all channels file , histogram and histogram_with_systematics identify the names of the files and of the histograms within the file, after doing some replacements (if any are found): $PROCESS is replaced with the process name (or \" data_obs \" for the observed data) $CHANNEL is replaced with the channel name $SYSTEMATIC is replaced with the name of the systematic + ( Up, Down ) $MASS is replaced with the higgs mass value which is passed as option in the command line used to run the limit tool In addition, user defined keywords can be included to be replaced. Any word in the datacard $WORD will be replaced by VALUE when including the option --keyword-value WORD=VALUE . The option can be repeated multiple times for multiple keywords. Template shape uncertainties Shape uncertainties can be taken into account by vertical interpolation of the histograms. The shapes (fraction of events f f in each bin) are interpolated using a spline for shifts below +/- 1\u03c3 and linearly outside of that. Specifically, for nuisance parameter values |\\theta|\\leq 1 |\\theta|\\leq 1 f(\\theta) = \\frac{1}{2} \\left( (\\delta^{+}-\\delta^{-})\\theta + \\frac{1}{8}(\\delta^{+}+\\delta^{-})(3\\theta^6 - 10\\theta^4 + 15\\theta^2) \\right) f(\\theta) = \\frac{1}{2} \\left( (\\delta^{+}-\\delta^{-})\\theta + \\frac{1}{8}(\\delta^{+}+\\delta^{-})(3\\theta^6 - 10\\theta^4 + 15\\theta^2) \\right) and for |\\theta|> 1 |\\theta|> 1 ( |\\theta|<-1 |\\theta|<-1 ), f(\\theta) f(\\theta) is a straight line with gradient \\delta^{+} \\delta^{+} ( \\delta^{-} \\delta^{-} ), where \\delta^{+}=f(\\theta=1)-f(\\theta=0) \\delta^{+}=f(\\theta=1)-f(\\theta=0) , and \\delta^{-}=f(\\theta=-1)-f(\\theta=0) \\delta^{-}=f(\\theta=-1)-f(\\theta=0) , derived using the nominal and up/down histograms. and This interpolation is designed so that the values of f(\\theta) f(\\theta) and its derivatives are continuous for all values of \\theta \\theta . The normalizations are interpolated linearly in log scale just like we do for log-normal uncertainties. If the value in a given bin is negative for some value of \\theta \\theta , the value will be truncated at 0. For each shape uncertainty and process/channel affected by it, two additional input shapes have to be provided, obtained shifting that parameter up and down by one standard deviation. When building the likelihood, each shape uncertainty is associated to a nuisance parameter taken from a unit gaussian distribution, which is used to interpolate or extrapolate using the specified histograms. For each given source of shape uncertainty, in the part of the datacard containing shape uncertainties (last block), there must be a row name shape effect_for_each_process_and_channel The effect can be \"-\" or 0 for no effect, 1 for normal effect, and possibly something different from 1 to test larger or smaller effects (in that case, the unit gaussian is scaled by that factor before using it as parameter for the interpolation) The datacard in data/tutorials/shapes/simple-shapes-TH1.txt is a clear example of how to include shapes in the datacard. In the first block the following line specifies the shape mapping: shapes * * simple-shapes-TH1.root $PROCESS $PROCESS_$SYSTEMATIC The last block concerns the treatment of the systematics affecting shapes. In this part the two uncertainties effecting on the shape are listed. alpha shape - 1 uncertainty on background shape and normalization sigma shape 0.5 - uncertainty on signal resolution. Assume the histogram is a 2 sigma shift, # so divide the unit gaussian by 2 before doing the interpolation There are two options for the interpolation algorithm in the \"shape\" uncertainty. Putting shape will result in a of the fraction of events in each bin - i.e the histograms are first normalised before interpolation. Putting shapeN while instead base the interpolation on the logs of the fraction in each bin. For both shape and shapeN , the total normalisation is interpolated using an asymmetric log-normal so that the effect of the systematic on both the shape and normalisation are accounted for. The following image shows a comparison of those two algorithms for this datacard. In this case there are two processes, signal and background , and two uncertainties affecting background ( alpha ) and signal shape ( sigma ). Within the root file 2 histograms per systematic have to be provided, they are the shape obtained, for the specific process, shifting up and down the parameter associated to the uncertainty: background_alphaUp and background_alphaDown , signal_sigmaUp and signal_sigmaDown . This is the content of the root file simple-shapes-TH1.root associated to the datacard data/tutorials/shapes/simple-shapes-TH1.txt : root [0] Attaching file simple-shapes-TH1.root as _file0... root [1] _file0->ls() TFile** simple-shapes-TH1.root TFile* simple-shapes-TH1.root KEY: TH1F signal;1 Histogram of signal__x KEY: TH1F signal_sigmaUp;1 Histogram of signal__x KEY: TH1F signal_sigmaDown;1 Histogram of signal__x KEY: TH1F background;1 Histogram of background__x KEY: TH1F background_alphaUp;1 Histogram of background__x KEY: TH1F background_alphaDown;1 Histogram of background__x KEY: TH1F data_obs;1 Histogram of data_obs__x KEY: TH1F data_sig;1 Histogram of data_sig__x For example, without shape uncertainties you could have just one row with shapes * * shapes.root $CHANNEL/$PROCESS Then for a simple example for two channels \"e\", \"mu\" with three processes \"higgs\", \"zz\", \"top\" you should create a rootfile that contains the following histogram meaning e/data_obs observed data in electron channel e/higgs expected shape for higgs in electron channel e/zz expected shape for ZZ in electron channel e/top expected shape for top in electron channel mu/data_obs observed data in muon channel mu/higgs expected shape for higgs in muon channel mu/zz expected shape for ZZ in muon channel mu/top expected shape for top in muon channel If you also have one uncertainty that affects the shape, e.g. jet energy scale, you should create shape histograms for the jet energy scale shifted up by one sigma, you could for example do one folder for each process and write a like like shapes * * shapes.root $CHANNEL/$PROCESS/nominal $CHANNEL/$PROCESS/$SYSTEMATIC or just attach a postifx to the name of the histogram shapes * * shapes.root $CHANNEL/$PROCESS $CHANNEL/$PROCESS_$SYSTEMATIC Warning If you have a nuisance parameter which has shape effects (using shape ) and rate effects (using lnN ) you should use a single line for the systemstic uncertainty with shape? . This will tell combine to fist look for Up/Down systematic templates for that process and if it doesnt find them, it will interpret the number that you put for the process as a lnN instead. For a detailed example of a template based binned analysis see the H\u2192\u03c4\u03c4 2014 DAS tutorial Unbinned or parametric shape analysis In some cases, it can be convenient to describe the expected signal and background shapes in terms of analytical functions rather than templates; a typical example are the searches where the signal is apparent as a narrow peak over a smooth continuum background. In this context, uncertainties affecting the shapes of the signal and backgrounds can be implemented naturally as uncertainties on the parameters of those analytical functions. It is also possible to adapt an agnostic approach in which the parameters of the background model are left freely floating in the fit to the data, i.e. only requiring the background to be well described by a smooth function. Technically, this is implemented by means of the RooFit package, that allows writing generic probability density functions, and saving them into ROOT files. The pdfs can be either taken from RooFit's standard library of functions (e.g. Gaussians, polynomials, ...) or hand-coded in C++, and combined together to form even more complex shapes. In the datacard using templates, the column after the file name would have been the name of the histogram. For the parametric analysis we need two names to identify the mapping, separated by a colon ( : ). shapes process channel shapes.root workspace_name:pdf_name The first part identifies the name of the input RooWorkspace containing the pdf, and the second part the name of the RooAbsPdf inside it (or, for the observed data, the RooAbsData ). There can be multiple input workspaces, just as there can be multiple input root files. You can use any of the usual RooFit pre-defined pdfs for your signal and background models. Warning If you are using RooAddPdfs in your model in which the coefficients are not defined recursively , combine will not interpret them properly. You can add the option --X-rtd ADDNLL_RECURSIVE=0 to any combine command in order to recover the correct interpretation, however we recommend that you instead redefine your pdf so that the coefficients are recursive (as described on the RooAddPdf documentation ) and keep the total normalisation (i.e extended term) as a separate object as in the case of the tutorial datacard. For example, take a look at the data/tutorials/shapes/simple-shapes-parametric.txt . We see the following line. shapes * * simple-shapes-parametric_input.root w:$PROCESS [...] bin 1 1 process sig bkg which indicates that the input file simple-shapes-parametric_input.root should contain an input workspace ( w ) with pdfs named sig and bkg since these are the names of the two processes in the datacard. Additionally, we expect there to be a dataset named data_obs . If we look at the contents of the workspace inside data/tutorials/shapes/simple-shapes-parametric_input.root , this is indeed what we see... root [1] w->Print() RooWorkspace(w) w contents variables --------- (MH,bkg_norm,cc_a0,cc_a1,cc_a2,j,vogian_sigma,vogian_width) p.d.f.s ------- RooChebychev::bkg[ x=j coefList=(cc_a0,cc_a1,cc_a2) ] = 2.6243 RooVoigtian::sig[ x=j mean=MH width=vogian_width sigma=vogian_sigma ] = 0.000639771 datasets -------- RooDataSet::data_obs(j) In this datacard, the signal is parameterised in terms of the hypothesised mass ( MH ). Combine will use this variable, instead of creating its own, which will be interpreted as the value for -m . For this reason, we should add the option -m 30 (or something else within the observable range) when running combine. You will also see there is a variable named bkg_norm . This is used to normalize the background rate (see the section on Rate parameters below for details). Warning Combine will not accept RooExtendedPdfs as an input. This is to alleviate a bug that lead to improper treatment of normalization when using multiple RooExtendedPdfs to describe a single process. You should instead use RooAbsPdfs and provide the rate as a separate object (see the Rate parameters section). The part of the datacard related to the systematics can include lines with the syntax name param X Y These lines encode uncertainties on the parameters of the signal and background pdfs. The parameter is to be assigned a Gaussian uncertainty of Y around its mean value of X . One can change the mean value from 0 to 1 (or really any value, if one so chooses) if the parameter in question is multiplicative instead of additive. In the data/tutorials/shapes/simple-shapes-parametric.txt datacard, there are lines for one such parametric uncertainty, sigma param 1.0 0.1 meaning there is a parameter already contained in the input workspace called sigma which should be constrained with a Gaussian centered at 1.0 with a width of 0.1. Note that, the exact interpretation (i.e all combine knows is that 1.0 should be the most likely value and 0.1 is its 1\u03c3 uncertainy) of these parameters is left to the user since the signal pdf is constructed externally by you. Asymmetric uncertainties are written as with lnN using the syntax -1\u03c3/+1\u03c3 in the datacard. If one wants to specify a parameter that is freely floating across its given range, and not gaussian constrained, the following syntax is used: name flatParam Though this is not strictly necessary in frequentist methods using profiled likelihoods as combine will still profile these nuisances when performing fits (as is the case for the simple-shapes-parametric.txt datacard). Warning All parameters which are floating or constant in the user's input workspaces will remain floating or constant. Combine will not modify those for you! A full example of a parametric analysis can be found in this H\u2192\u03b3\u03b3 2014 DAS tutorial Caveat on using parametric pdfs with binned datasets Users should be aware of a feature that affects the use of parametric pdfs together with binned datasets. RooFit uses the integral of the pdf, computed analytically (or numerically, but disregarding the binning), to normalize it, but then computes the expected event yield in each bin evaluating only the pdf at the bin center. This means that if the variation of the pdf is sizeable within the bin then there is a mismatch between the sum of the event yields per bin and the pdf normalization, and that can cause a bias in the fits (more properly, the bias is there if the contribution of the second derivative integrated on the bin size is not negligible, since for linear functions evaluating them at the bin center is correct). There are two reccomended ways to work around this ... 1. Use narrow bins It is recommended to use bins that are significantly finer than the characteristic scale of the pdfs - which would anyway be the recommended thing even in the absence of this feature. Obviously, this caveat does not apply to analyses using templates (they're constant across each bin, so there's no bias), or using unbinned datasets. 2. Use a RooParametricShapeBinPdf Another solution (currently implemented for 1-dimensional histograms only) is to use a custom pdf which performs the correct integrals internally as in RooParametricShapeBinPdf Note that this pdf class now allows parameters that are themselves RooAbsReal objects (i.e. functions of other variables). The integrals are handled internally by calling the underlying pdf\u2019s createIntegral() method with named ranges created for each of the bins. This means that if the analytical integrals for the underlying pdf are available, they will be used. The constructor for this class requires a RooAbsReal (eg any RooAbsPdf )along with a list of RooRealVars (the parameters, excluding the observable x x ), RooParametricShapeBinPdf(const char *name, const char *title, RooAbsReal& _pdf, RooAbsReal& _x, RooArgList& _pars, const TH1 &_shape ) Below is a comparison of a fit to a binned dataset containing 1000 events with one observable 0 \\leq x \\leq 100 0 \\leq x \\leq 100 . The fit function is a RooExponential of the form e^{xp} e^{xp} . In the upper plot, the data are binned in 100 evenly spaced bins, while in the lower plot, there are 3 irregular bins. The blue lines show the result of the fit when using the RooExponential directly while the red shows the result when wrapping the pdf inside a RooParametricShapeBinPdf . In the narrow binned case, the two agree well while for wide bins, accounting for the integral over the bin yields a better fit. You should note that using this class will result in slower fits so you should first decide if the added accuracy is enough to justify the reduced efficiency. Beyond simple datacards Datacards can be extended in order to provide additional functionality and flexibility during runtime. These can also allow for the production of more complicated models and performing advanced computation of results beyond limits and significances. Rate parameters The overall rate \"expected\" of a particular process in a particular bin does not necessarily need to be a fixed quantity. Scale factors can be introduced to modify the rate directly in the datacards for ANY type of analysis. This can be achieved using the directive rateParam in the datacard with the following syntax, name rateParam bin process initial_value [min,max] The [min,max] argument is optional and if not included, combine will remove the range of this parameter. This will produce a new parameter in the model (unless it already exists) which multiplies the rate of that particular process in the given bin by its value. You can attach the same rateParam to multiple processes/bins by either using a wild card (eg * will match everything, QCD_* will match everything starting with QCD_ etc.) in the name of the bin and/or process or by repeating the rateParam line in the datacard for different bins/processes with the same name. Warning rateParam is not a shortcut to evaluate the post-fit yield of a process since other nuisances can also change the normalisation . E.g., finding that the rateParam best-fit value is 0.9 does not necessarily imply that the process yield is 0.9 times the initial one. The best is to evaluate the yield taking into account the values of all nuisance parameters using --saveNormalizations . This parameter is by default, freely floating. It is possible to include a Gaussian constraint on any rateParam which is floating (i.e not a formula or spline) by adding a param nuisance line in the datacard with the same name. In addition to rate modifiers which are freely floating, modifiers which are functions of other parameters can be included using the following syntax, name rateParam bin process formula args where args is a comma separated list of the arguments for the string formula . You can include other nuisance parameters in the formula , including ones which are Gaussian constrained (i,e via the param directive.) Below is an example datacard which uses the rateParam directive to implement an ABCD like method in combine. For a more realistic description of it's use for ABCD, see the single-lepton SUSY search implementation described here imax 4 number of channels jmax 0 number of processes -1 kmax * number of nuisance parameters (sources of systematical uncertainties) ------- bin B C D A observation 50 100 500 10 ------- bin B C D A process bkg bkg bkg bkg process 1 1 1 1 rate 1 1 1 1 ------- alpha rateParam A bkg (@0*@1/@2) beta,gamma,delta beta rateParam B bkg 50 gamma rateParam C bkg 100 delta rateParam D bkg 500 For more examples of using rateParam (eg for fitting process normalisations in control regions and signal regions simultaneously) see this 2016 CMS tutorial Finally, any pre-existing RooAbsReal inside some rootfile with a workspace can be imported using the following name rateParam bin process rootfile:workspacename The name should correspond to the name of the object which is being picked up inside the RooWorkspace. A simple example using the SM XS and BR splines available in HiggsAnalysis/CombinedLimit can be found under data/tutorials/rate_params/simple_sm_datacard.txt After running text2workspace.py on your datacard, you can check the normalisation objects using the tool test/printWorkspaceNormalisations.py . See the example below for the data/tutorials/shapes/simple-shapes-parametric.txt datacard. text2workspace.py data/tutorials/shapes/simple-shapes-parametric.txt python test/printWorkspaceNormalisations.py data/tutorials/shapes/simple-shapes-parametric.root ... --------------------------------------------------------------------------- --------------------------------------------------------------------------- Channel - bin1 --------------------------------------------------------------------------- Top-level normalisation for process bkg -> n_exp_final_binbin1_proc_bkg ------------------------------------------------------------------------- RooProduct::n_exp_final_binbin1_proc_bkg[ n_exp_binbin1_proc_bkg * shapeBkg_bkg_bin1__norm ] = 521.163 ... is a product, which contains n_exp_binbin1_proc_bkg RooRealVar::n_exp_binbin1_proc_bkg = 1 C L(-INF - +INF) ------------------------------------------------------------------------- default value = 521.163204829 --------------------------------------------------------------------------- Top-level normalisation for process sig -> n_exp_binbin1_proc_sig ------------------------------------------------------------------------- Dumping ProcessNormalization n_exp_binbin1_proc_sig @ 0x464f700 nominal value: 1 log-normals (1): kappa = 1.1, logKappa = 0.0953102, theta = lumi = 0 asymm log-normals (0): other terms (1): term r (class RooRealVar), value = 1 ------------------------------------------------------------------------- default value = 1.0 This tells us that the normalisation for the background process, named n_exp_final_binbin1_proc_bkg is a product of two objects n_exp_binbin1_proc_bkg * shapeBkg_bkg_bin1__norm . The first object is just from the rate line in the datacard (equal to 1) and the second is a floating parameter. For the signal, the normalisation is called n_exp_binbin1_proc_sig and is a ProcessNormalization object which contains the rate modifications due to the systematic uncertainties. You can see that it also has a \" nominal value \" which again is just from the value given in the rate line of the datacard (again=1). Extra arguments If a parameter is intended to be used and it is not a user defined param or rateParam , it can be picked up by first issuing an extArgs directive before this line in the datacard. The syntax for extArgs is name extArg rootfile:workspacename The string \":RecycleConflictNodes\" can be added at the end of the final argument (i.e. rootfile:workspacename:RecycleConflictNodes) to apply the corresponding RooFit option when the object is imported into the workspace. It is also possible to simply add a RooRealVar using extArg for use in function rateParams with the following name extArg init [min,max] Note that the [min,max] argument is optional and if not included, the code will remove the range of this parameter. Manipulation of Nuisance parameters It can often be useful to modify datacards, or the runtime behavior, without having to modify individual systematics lines. This can be acheived through the following. Nuisance modifiers If a nuisance parameter needs to be renamed for certain processes/channels, it can be done so using a single nuisance edit directive at the end of a datacard nuisance edit rename process channel oldname newname [options] Note that the wildcard ( * ) can be used for either/both of process and channel. This will have the effect that nuisance parameter effecting a given process/channel will be renamed, thereby de-correlating it from other processes/channels. Use options ifexists to skip/avoid error if nuisance not found. This kind of command will only effect nuisances of the type shape[N] , lnN . Instead, if you also want to change the names of param type nuisances, you can use a global version nuisance edit rename oldname newname which will rename all shape[N] , lnN and param nuisances found in one go. You should make sure these commands come after any process/channel specific ones in the datacard. This version does not accept options. Other edits are also supported as follows, nuisance edit add process channel name pdf value [options] -> add a new or add to a nuisance. If options is addq , value will be added in quadrature to this nuisance for this process/channel. If options is overwrite , the nuisance value will be replaced with this value nuisance edit drop process channel name [options] -> remove this nuisance from the process/channel. Use options ifexists to skip/avoid error if nuisance not found. nuisance edit changepdf name newpdf -> change the pdf type of a given nuisance to newpdf . nuisance edit split process channel oldname newname1 newname2 value1 value2 -> split a nuisance line into two separate nuisances called newname1 and newname2 with values value1 and value2 . Will produce two separate lines to that the original nuisance oldname becomes two uncorrelated nuisances. nuisance edit freeze name [options] -> set nuisance to frozen by default. Can be over-ridden in combine command line using --floatNuisances option Use options ifexists to skip/avoid error if nuisance not found. nuisance edit merge process channel name1 name2 -> merge systematic name2 into name1 by adding their values in quadrature and removing name2 . This only works if, for each process and channel included, they go in the same direction. For example, you can add 1.1 to 1.2, but not to 0.9. The above edits (excluding the renaming) support nuisances which are any of shape[N] , lnN , lnU , gmN , param , flatParam , rateParam or discrete types. Groups of nuisances Often it is desirable to freeze one or more nuisances to check the impact they have on limits, likelihood scans, significances etc. However, for large groups of nuisances (eg everything associated to theory) it is easier to define nuisance groups in the datacard. The following line in a datacard will, for example, produce a group of nuisances with the group name theory which contains two parameters, QCDscale and pdf . theory group = QCDscale pdf Multiple groups can be defined in this way. It is also possible to extend nuisance groups in datacards using += in place of = . These groups can be manipulated at runtime (eg for freezing all nuisances associated to a group at runtime, see Running the tool ). You can find more info on groups of nuisances here Note that when using the automatic addition of statistical uncertainties (autoMCStats), the corresponding nuisance parameters are created by text2workspace.py and so do not exist in the datacards. It is therefore not possible to add autoMCStats parameters to groups of nuisances in the way described above. However, text2workspace.py will automatically create a group labelled autoMCStats which contains all autoMCStats parameters. This group is useful for freezing all parameters created by autoMCStats. For freezing subsets of the parameters, for example if the datacard contains two categories, cat_label_1 and cat_label_2 , to only freeze the autoMCStat parameters created for category cat_label_1 the regular expression features can be used. In this example this can be achieved by using --freezeParameters 'rgx{prop_bincat_label_1_bin.*}' . Combination of multiple datacards If you have separate channels each with it's own datacard, it is possible to produce a combined datacard using the script combineCards.py The syntax is simple: combineCards.py Name1=card1.txt Name2=card2.txt .... > card.txt If the input datacards had just one bin each, then the output channels will be called Name1 , Name2 , and so on. Otherwise, a prefix Name1_ ... Name2_ will be added to the bin labels in each datacard. The supplied bin names Name1 , Name2 , etc. must themselves conform to valid C++/python identifier syntax. Warning Systematics which have different names will be assumed to be uncorrelated, and the ones with the same name will be assumed 100% correlated. A systematic correlated across channels must have the same p.d.f. in all cards (i.e. always lnN , or all gmN with same N ) The combineCards.py script will complain if you are trying to combine a shape datacard with a counting datacard. You can however convert a counting datacard in an equivalent shape-based one by adding a line shapes * * FAKE in the datacard after the imax , jmax and kmax section. Alternatively, you can add the option -S in combineCards.py which will do this for you while making the combination. Automatic production of datacards and workspaces For complicated analyses or cases in which multiple datacards are needed (e.g. optimisation studies), you can avoid writing these by hand. The object Datacard defines the analysis and can be created as a python object. The template python script below will produce the same workspace as running textToWorkspace.py (see the section on Physics Models ) on the realistic-counting-experiment.txt datacard. from HiggsAnalysis.CombinedLimit.DatacardParser import * from HiggsAnalysis.CombinedLimit.ModelTools import * from HiggsAnalysis.CombinedLimit.ShapeTools import * from HiggsAnalysis.CombinedLimit.PhysicsModel import * from sys import exit from optparse import OptionParser parser = OptionParser() addDatacardParserOptions(parser) options,args = parser.parse_args() options.bin = True # make a binary workspace DC = Datacard() MB = None ############## Setup the datacard (must be filled in) ########################### DC.bins = ['bin1'] # <type 'list'> DC.obs = {'bin1': 0.0} # <type 'dict'> DC.processes = ['ggH', 'qqWW', 'ggWW', 'others'] # <type 'list'> DC.signals = ['ggH'] # <type 'list'> DC.isSignal = {'qqWW': False, 'ggWW': False, 'ggH': True, 'others': False} # <type 'dict'> DC.keyline = [('bin1', 'ggH', True), ('bin1', 'qqWW', False), ('bin1', 'ggWW', False), ('bin1', 'others', False)] # <type 'list'> DC.exp = {'bin1': {'qqWW': 0.63, 'ggWW': 0.06, 'ggH': 1.47, 'others': 0.22}} # <type 'dict'> DC.systs = [('lumi', False, 'lnN', [], {'bin1': {'qqWW': 0.0, 'ggWW': 1.11, 'ggH': 1.11, 'others': 0.0}}), ('xs_ggH', False, 'lnN', [], {'bin1': {'qqWW': 0.0, 'ggWW': 0.0, 'ggH': 1.16, 'others': 0.0}}), ('WW_norm', False, 'gmN', [4], {'bin1': {'qqWW': 0.16, 'ggWW': 0.0, 'ggH': 0.0, 'others': 0.0}}), ('xs_ggWW', False, 'lnN', [], {'bin1': {'qqWW': 0.0, 'ggWW': 1.5, 'ggH': 0.0, 'others': 0.0}}), ('bg_others', False, 'lnN', [], {'bin1': {'qqWW': 0.0, 'ggWW': 0.0, 'ggH': 0.0, 'others': 1.3}})] # <type 'list'> DC.shapeMap = {} # <type 'dict'> DC.hasShapes = False # <type 'bool'> DC.flatParamNuisances = {} # <type 'dict'> DC.rateParams = {} # <type 'dict'> DC.extArgs = {} # <type 'dict'> DC.rateParamsOrder = set([]) # <type 'set'> DC.frozenNuisances = set([]) # <type 'set'> DC.systematicsShapeMap = {} # <type 'dict'> DC.nuisanceEditLines = [] # <type 'list'> DC.groups = {} # <type 'dict'> DC.discretes = [] # <type 'list'> ###### User defined options ############################################# options.out = \"combine_workspace.root\" # Output workspace name options.fileName = \"./\" # Path to input ROOT files options.verbose = \"1\" # Verbosity ########################################################################## if DC.hasShapes: MB = ShapeBuilder(DC, options) else: MB = CountingModelBuilder(DC, options) # Set physics models MB.setPhysics(defaultModel) MB.doModel() Any existing datacard can be converted into such a template python script by using the --dump-datacard option in text2workspace.py in case a more complicated template is needed. Warning The above is not advised for final results as this script is not easily combined with other analyses so should only be used for internal studies. For the automatic generation of datacards (which are combinable), you should instead use the CombineHarvester package which includes many features for producing complex datacards in a reliable, automated way. Sanity checking the datacard For large combinations with multiple channels/processes etc, the .txt file can get unweildy to read through. There are some simple tools to help check and disseminate the contents of the cards. In order to get a quick view of the systematic uncertainties included in the datacard, you can use the test/systematicsAnalyzer.py tool. This will produce a list of the systematic uncertainties (normalisation and shape), indicating what type they are, which channels/processes they affect and the size of the affect on the normalisation (for shape uncertainties, this will just be the overall uncertaintly on the normalisation information). The default output is a .html file which allows you to expand to give more details about the affect of the systematic for each channel/process. Add the option --format brief to give a simpler summary report direct to the terminal. An example output for the tutorial card data/tutorials/shapes/simple-shapes-TH1.txt is shown below. $ python test/systematicsAnalyzer.py data/tutorials/shapes/simple-shapes-TH1.txt > out.html In case you only have a cut-and-count style card, include the option --noshape . If you have a datacard which uses several rateParams or a Physics model which includes some complicated product of normalisation terms in each process, you can check the values of the normalisation (and which objects in the workspace comprise them) using the test/printWorkspaceNormalisations.py tool. As an example, below is the first few blocks of output for the tutorial card data/tutorials/counting/realistic-multi-channel.txt . $ text2workspace.py data/tutorials/shapes/simple-shapes-parametric.txt -m 30 $ python test/printWorkspaceNormalisations.py data/tutorials/counting/realistic-multi-channel.root --------------------------------------------------------------------------- --------------------------------------------------------------------------- Channel - mu_tau --------------------------------------------------------------------------- Top-level normalisation for process ZTT -> n_exp_binmu_tau_proc_ZTT ------------------------------------------------------------------------- Dumping ProcessNormalization n_exp_binmu_tau_proc_ZTT @ 0x6bbb610 nominal value: 329 log-normals (3): kappa = 1.23, logKappa = 0.207014, theta = tauid = 0 kappa = 1.04, logKappa = 0.0392207, theta = ZtoLL = 0 kappa = 1.04, logKappa = 0.0392207, theta = effic = 0 asymm log-normals (0): other terms (0): ------------------------------------------------------------------------- default value = 329.0 --------------------------------------------------------------------------- Top-level normalisation for process QCD -> n_exp_binmu_tau_proc_QCD ------------------------------------------------------------------------- Dumping ProcessNormalization n_exp_binmu_tau_proc_QCD @ 0x6bbcaa0 nominal value: 259 log-normals (1): kappa = 1.1, logKappa = 0.0953102, theta = QCDmu = 0 asymm log-normals (0): other terms (0): ------------------------------------------------------------------------- default value = 259.0 --------------------------------------------------------------------------- Top-level normalisation for process higgs -> n_exp_binmu_tau_proc_higgs ------------------------------------------------------------------------- Dumping ProcessNormalization n_exp_binmu_tau_proc_higgs @ 0x6bc6390 nominal value: 0.57 log-normals (3): kappa = 1.11, logKappa = 0.10436, theta = lumi = 0 kappa = 1.23, logKappa = 0.207014, theta = tauid = 0 kappa = 1.04, logKappa = 0.0392207, theta = effic = 0 asymm log-normals (0): other terms (1): term r (class RooRealVar), value = 1 ------------------------------------------------------------------------- default value = 0.57 --------------------------------------------------------------------------- --------------------------------------------------------------------------- Channel - e_mu --------------------------------------------------------------------------- Top-level normalisation for process ZTT -> n_exp_bine_mu_proc_ZTT ------------------------------------------------------------------------- Dumping ProcessNormalization n_exp_bine_mu_proc_ZTT @ 0x6bc8910 nominal value: 88 log-normals (2): kappa = 1.04, logKappa = 0.0392207, theta = ZtoLL = 0 kappa = 1.04, logKappa = 0.0392207, theta = effic = 0 asymm log-normals (0): other terms (0): ------------------------------------------------------------------------- default value = 88.0 --------------------------------------------------------------------------- As you can see, for each channel, a report is given for the top-level rate object in the workspace, for each process contributing to that channel. You can also see the various terms which make up that rate. The default value is for the default parameters in the workspace (i.e when running text2workspace , these are the values created as default).","title":"Preparing the datacard"},{"location":"part2/settinguptheanalysis/#preparing-the-datacard","text":"The input to combine, which defines the details of the experiment, is a datacard file is a plain ASCII file. This is true whether the experiment is a simple counting experiment or a shape analysis.","title":"Preparing the datacard"},{"location":"part2/settinguptheanalysis/#a-simple-counting-experiment","text":"The file data/tutorials/counting/realistic-counting-experiment.txt shows an example of a counting experiment. The first lines can be used as a description and are not parsed by the program. They have to begin with a \"#\": # Simple counting experiment, with one signal and a few background processes # Simplified version of the 35/pb H->WW analysis for mH = 160 GeV Then one declares the number of observables , imax , that are present in the model used to calculate limits/significances. The number of observables will typically be the number of channels in a counting experiment or the number of bins in a binned shape fit. (If one specifies for imax the value * it means \"figure it out from the rest of the datacard\", but in order to better catch mistakes it's recommended to specify it explicitly) imax 1 number of channels Then one declares the number of background sources to be considered, jmax , and the number of independent sources of systematic uncertainties , kmax : jmax 3 number of backgrounds kmax 5 number of nuisance parameters (sources of systematic uncertainties) In the example there is 1 channel, there are 3 background sources, and there are 5 independent sources of systematic uncertainty. Then there are the lines describing what is actually observed: the number of events observed in each channel. The first line, starting with bin defines the label used for each channel. In the example we have 1 channel, labelled 1 , and in the following line, observation , are listed the events observed, 0 in this example: # we have just one channel, in which we observe 0 events bin bin1 observation 0 Following is the part related to the number of events expected, for each bin and process, arranged in (#channels)*(#processes) columns. bin bin1 bin1 bin1 bin1 process ggH qqWW ggWW others process 0 1 2 3 rate 1.47 0.63 0.06 0.22 The bin line identifies the channel the column is referring to. It goes from 1 to the imax declared above. The first process line contains the labels of the various sources The second process line must have a positive number for backgrounds, and 0 or a negative number for the signals. You should use different process ids for different processes. The last line, rate , tells the expected yield of events in the specified bin and process All bins should be declared in increasing order, and within each bin one should include all processes in increasing order, specifying a 0 for processes that do not contribute. The last section contains the description of the systematic uncertainties: lumi lnN 1.11 - 1.11 - lumi affects both signal and gg->WW (mc-driven). lnN = lognormal xs_ggH lnN 1.16 - - - gg->H cross section + signal efficiency + other minor ones. WW_norm gmN 4 - 0.16 - - WW estimate of 0.64 comes from sidebands: 4 events in sideband times 0.16 (=> ~50% statistical uncertainty) xs_ggWW lnN - - 1.50 - 50% uncertainty on gg->WW cross section bg_others lnN - - - 1.30 30% uncertainty on the rest of the backgrounds the first columns is a label identifying the uncertainty the second column identifies the type of distribution used lnN stands for Log-normal , which is the recommended choice for multiplicative corrections (efficiencies, cross sections, ...). If \u0394x/x is the relative uncertainty on the multiplicative correction, one should put 1+\u0394x/x in the column corresponding to the process and channel. Asymmetric log-normals are instead supported by providing \u03ba down /\u03ba up where \u03ba down is the ratio of the the yield to the nominal value for a -1\u03c3 deviation of the nuisance and \u03ba up is the ratio of thyield to the nominal value for +1\\sigma +1\\sigma deviation. Note that for single-value log-normal with value \\kappa=1+\\Delta x/x \\kappa=1+\\Delta x/x , the yield of the process it is associated with is multiplied by \\kappa^{\\theta} \\kappa^{\\theta} . At \\theta=0 \\theta=0 the nominal yield is retained, at \\theta=1\\sigma \\theta=1\\sigma the yield is multiplied by \\kappa \\kappa and at \\theta=-1\\sigma \\theta=-1\\sigma the yield is multiplied by 1/\\kappa 1/\\kappa . This means that an uncertainty represented as 1.2 does not multiply the nominal yield by 0.8 for \\theta=-1\\sigma \\theta=-1\\sigma ; but by 0.8333. For large uncertainties that have a symmetric effect on the yield it may therefore be desirable to encode them as asymmetric log-normals instead. gmN stands for Gamma , and is the recommended choice for the statistical uncertainty on a background coming from the number of events in a control region (or in a MC sample with limited statistics). If the control region or MC contains N events, and the extrapolation factor from the control region to the signal region is \u03b1 then one shoud put N just after the gmN keyword, and then the value of \u03b1 in the proper column. Also, the yield in the rate row should match with N\u03b1 lnU stands for log-uniform distribution. A value of 1+\u03b5 in the column will imply that the yield of this background is allowed to float freely between x(1+\u03b5) and x/(1+\u03b5) (in particular, if \u03b5 is small, then this is approximately (x-\u0394x,x+\u0394x) with \u03b5=\u0394x/x ) This is normally useful when you want to set a large a-priori uncertainty on a given background and then rely on the correlation between channels to constrain it. Beware that while Gaussian-like uncertainties behave in a similar way under profiling and marginalization, uniform uncertainties do not, so the impact of the uncertainty on the result will depend on how the nuisances are treated. then there are (#channels)*(#processes) columns reporting the relative effect of the systematic uncertainty on the rate of each process in each channel. The columns are aligned with the ones in the previous lines declaring bins, processes and rates. In the example, there are 5 uncertainties: the first uncertainty affects the signal by 11%, and affects the ggWW process by 11% the second uncertainty affects the signal by 16% leaving the backgrounds unaffected the third line specifies that the qqWW background comes from a sideband with 4 observed events and an extrapolation factor of 0.16; the resulting uncertainty on the expected yield is 1/\\sqrt{4+1} 1/\\sqrt{4+1} = 45% the fourth uncertainty does not affect the signal, affects the ggWW background by 50%, leaving the other backgrounds unaffected the last uncertainty does not affect the signal, affects by 30% the others backgrounds, leaving the rest of the backgrounds unaffected","title":"A simple counting experiment"},{"location":"part2/settinguptheanalysis/#shape-analysis","text":"The datacard has to be supplemented with two extensions: A new block of lines defining how channels and processes are mapped into shapes The block for systematics that can contain also rows with shape uncertainties. The expected shape can be parametric or not parametric. In the first case the parametric pdfs have to be given as input to the tool. In the latter case, for each channel, histograms have to be provided for the expected shape of each process. For what concerns data, they have to be provided as input to the tool as a histogram to perform a binned shape analysis and as a RooDataSet to perform an unbinned shape analysis. Warning If using RooFit based inputs (RooDataHists/RooDataSets/RooAbsPdfs) then you should be careful to use different RooRealVars as the observable in each category being combined. It is possible to use the same RooRealVar if the observable has the same range (and binning if using binned data) in each category though in most cases it is simpler to avoid doing so.","title":"Shape analysis"},{"location":"part2/settinguptheanalysis/#rates-for-shape-analysis","text":"As with the counting experiment, the total nominal rate of a given process must be identified in the rate line of the datacard. However, there are special options for shape based analyses as follows A value of -1 in the rate line indicates to combine to calculate the rate from the input TH1 (via TH1::Integral) or RooDataSet/RooDataHist (via RooAbsData::sumEntries) For parametric shapes (RooAbsPdf), if a parameter is found in the input workspace with the name pdfname _norm the rate will be multiplied by the value of that parameter. Note that since this parameter can be freely floating, the normalization of a shape can be made to freely float this way. This can also be achieved through the use of rateParams","title":"Rates for shape analysis"},{"location":"part2/settinguptheanalysis/#binned-shape-analysis","text":"For each channel, histograms have to be provided for the observed shape and for the expected shape of each process. Within each channel, all histograms must have the same binning. The normalization of the data histogram must correspond to the number of observed events The normalization of the expected histograms must match the expected yields The combine tool can take as input histograms saved as TH1 or as RooAbsHist in a RooFit workspace (an example of how to create a RooFit workspace and save histograms is available in github ). The block of lines defining the mapping (first block in the datacard) contains one or more rows in the form shapes process channel file histogram [histogram_with_systematics] In this line process is any one the process names, or * for all processes, or data_obs for the observed data channel is any one the process names, or * for all channels file , histogram and histogram_with_systematics identify the names of the files and of the histograms within the file, after doing some replacements (if any are found): $PROCESS is replaced with the process name (or \" data_obs \" for the observed data) $CHANNEL is replaced with the channel name $SYSTEMATIC is replaced with the name of the systematic + ( Up, Down ) $MASS is replaced with the higgs mass value which is passed as option in the command line used to run the limit tool In addition, user defined keywords can be included to be replaced. Any word in the datacard $WORD will be replaced by VALUE when including the option --keyword-value WORD=VALUE . The option can be repeated multiple times for multiple keywords.","title":"Binned shape analysis"},{"location":"part2/settinguptheanalysis/#template-shape-uncertainties","text":"Shape uncertainties can be taken into account by vertical interpolation of the histograms. The shapes (fraction of events f f in each bin) are interpolated using a spline for shifts below +/- 1\u03c3 and linearly outside of that. Specifically, for nuisance parameter values |\\theta|\\leq 1 |\\theta|\\leq 1 f(\\theta) = \\frac{1}{2} \\left( (\\delta^{+}-\\delta^{-})\\theta + \\frac{1}{8}(\\delta^{+}+\\delta^{-})(3\\theta^6 - 10\\theta^4 + 15\\theta^2) \\right) f(\\theta) = \\frac{1}{2} \\left( (\\delta^{+}-\\delta^{-})\\theta + \\frac{1}{8}(\\delta^{+}+\\delta^{-})(3\\theta^6 - 10\\theta^4 + 15\\theta^2) \\right) and for |\\theta|> 1 |\\theta|> 1 ( |\\theta|<-1 |\\theta|<-1 ), f(\\theta) f(\\theta) is a straight line with gradient \\delta^{+} \\delta^{+} ( \\delta^{-} \\delta^{-} ), where \\delta^{+}=f(\\theta=1)-f(\\theta=0) \\delta^{+}=f(\\theta=1)-f(\\theta=0) , and \\delta^{-}=f(\\theta=-1)-f(\\theta=0) \\delta^{-}=f(\\theta=-1)-f(\\theta=0) , derived using the nominal and up/down histograms. and This interpolation is designed so that the values of f(\\theta) f(\\theta) and its derivatives are continuous for all values of \\theta \\theta . The normalizations are interpolated linearly in log scale just like we do for log-normal uncertainties. If the value in a given bin is negative for some value of \\theta \\theta , the value will be truncated at 0. For each shape uncertainty and process/channel affected by it, two additional input shapes have to be provided, obtained shifting that parameter up and down by one standard deviation. When building the likelihood, each shape uncertainty is associated to a nuisance parameter taken from a unit gaussian distribution, which is used to interpolate or extrapolate using the specified histograms. For each given source of shape uncertainty, in the part of the datacard containing shape uncertainties (last block), there must be a row name shape effect_for_each_process_and_channel The effect can be \"-\" or 0 for no effect, 1 for normal effect, and possibly something different from 1 to test larger or smaller effects (in that case, the unit gaussian is scaled by that factor before using it as parameter for the interpolation) The datacard in data/tutorials/shapes/simple-shapes-TH1.txt is a clear example of how to include shapes in the datacard. In the first block the following line specifies the shape mapping: shapes * * simple-shapes-TH1.root $PROCESS $PROCESS_$SYSTEMATIC The last block concerns the treatment of the systematics affecting shapes. In this part the two uncertainties effecting on the shape are listed. alpha shape - 1 uncertainty on background shape and normalization sigma shape 0.5 - uncertainty on signal resolution. Assume the histogram is a 2 sigma shift, # so divide the unit gaussian by 2 before doing the interpolation There are two options for the interpolation algorithm in the \"shape\" uncertainty. Putting shape will result in a of the fraction of events in each bin - i.e the histograms are first normalised before interpolation. Putting shapeN while instead base the interpolation on the logs of the fraction in each bin. For both shape and shapeN , the total normalisation is interpolated using an asymmetric log-normal so that the effect of the systematic on both the shape and normalisation are accounted for. The following image shows a comparison of those two algorithms for this datacard. In this case there are two processes, signal and background , and two uncertainties affecting background ( alpha ) and signal shape ( sigma ). Within the root file 2 histograms per systematic have to be provided, they are the shape obtained, for the specific process, shifting up and down the parameter associated to the uncertainty: background_alphaUp and background_alphaDown , signal_sigmaUp and signal_sigmaDown . This is the content of the root file simple-shapes-TH1.root associated to the datacard data/tutorials/shapes/simple-shapes-TH1.txt : root [0] Attaching file simple-shapes-TH1.root as _file0... root [1] _file0->ls() TFile** simple-shapes-TH1.root TFile* simple-shapes-TH1.root KEY: TH1F signal;1 Histogram of signal__x KEY: TH1F signal_sigmaUp;1 Histogram of signal__x KEY: TH1F signal_sigmaDown;1 Histogram of signal__x KEY: TH1F background;1 Histogram of background__x KEY: TH1F background_alphaUp;1 Histogram of background__x KEY: TH1F background_alphaDown;1 Histogram of background__x KEY: TH1F data_obs;1 Histogram of data_obs__x KEY: TH1F data_sig;1 Histogram of data_sig__x For example, without shape uncertainties you could have just one row with shapes * * shapes.root $CHANNEL/$PROCESS Then for a simple example for two channels \"e\", \"mu\" with three processes \"higgs\", \"zz\", \"top\" you should create a rootfile that contains the following histogram meaning e/data_obs observed data in electron channel e/higgs expected shape for higgs in electron channel e/zz expected shape for ZZ in electron channel e/top expected shape for top in electron channel mu/data_obs observed data in muon channel mu/higgs expected shape for higgs in muon channel mu/zz expected shape for ZZ in muon channel mu/top expected shape for top in muon channel If you also have one uncertainty that affects the shape, e.g. jet energy scale, you should create shape histograms for the jet energy scale shifted up by one sigma, you could for example do one folder for each process and write a like like shapes * * shapes.root $CHANNEL/$PROCESS/nominal $CHANNEL/$PROCESS/$SYSTEMATIC or just attach a postifx to the name of the histogram shapes * * shapes.root $CHANNEL/$PROCESS $CHANNEL/$PROCESS_$SYSTEMATIC Warning If you have a nuisance parameter which has shape effects (using shape ) and rate effects (using lnN ) you should use a single line for the systemstic uncertainty with shape? . This will tell combine to fist look for Up/Down systematic templates for that process and if it doesnt find them, it will interpret the number that you put for the process as a lnN instead. For a detailed example of a template based binned analysis see the H\u2192\u03c4\u03c4 2014 DAS tutorial","title":"Template shape uncertainties"},{"location":"part2/settinguptheanalysis/#unbinned-or-parametric-shape-analysis","text":"In some cases, it can be convenient to describe the expected signal and background shapes in terms of analytical functions rather than templates; a typical example are the searches where the signal is apparent as a narrow peak over a smooth continuum background. In this context, uncertainties affecting the shapes of the signal and backgrounds can be implemented naturally as uncertainties on the parameters of those analytical functions. It is also possible to adapt an agnostic approach in which the parameters of the background model are left freely floating in the fit to the data, i.e. only requiring the background to be well described by a smooth function. Technically, this is implemented by means of the RooFit package, that allows writing generic probability density functions, and saving them into ROOT files. The pdfs can be either taken from RooFit's standard library of functions (e.g. Gaussians, polynomials, ...) or hand-coded in C++, and combined together to form even more complex shapes. In the datacard using templates, the column after the file name would have been the name of the histogram. For the parametric analysis we need two names to identify the mapping, separated by a colon ( : ). shapes process channel shapes.root workspace_name:pdf_name The first part identifies the name of the input RooWorkspace containing the pdf, and the second part the name of the RooAbsPdf inside it (or, for the observed data, the RooAbsData ). There can be multiple input workspaces, just as there can be multiple input root files. You can use any of the usual RooFit pre-defined pdfs for your signal and background models. Warning If you are using RooAddPdfs in your model in which the coefficients are not defined recursively , combine will not interpret them properly. You can add the option --X-rtd ADDNLL_RECURSIVE=0 to any combine command in order to recover the correct interpretation, however we recommend that you instead redefine your pdf so that the coefficients are recursive (as described on the RooAddPdf documentation ) and keep the total normalisation (i.e extended term) as a separate object as in the case of the tutorial datacard. For example, take a look at the data/tutorials/shapes/simple-shapes-parametric.txt . We see the following line. shapes * * simple-shapes-parametric_input.root w:$PROCESS [...] bin 1 1 process sig bkg which indicates that the input file simple-shapes-parametric_input.root should contain an input workspace ( w ) with pdfs named sig and bkg since these are the names of the two processes in the datacard. Additionally, we expect there to be a dataset named data_obs . If we look at the contents of the workspace inside data/tutorials/shapes/simple-shapes-parametric_input.root , this is indeed what we see... root [1] w->Print() RooWorkspace(w) w contents variables --------- (MH,bkg_norm,cc_a0,cc_a1,cc_a2,j,vogian_sigma,vogian_width) p.d.f.s ------- RooChebychev::bkg[ x=j coefList=(cc_a0,cc_a1,cc_a2) ] = 2.6243 RooVoigtian::sig[ x=j mean=MH width=vogian_width sigma=vogian_sigma ] = 0.000639771 datasets -------- RooDataSet::data_obs(j) In this datacard, the signal is parameterised in terms of the hypothesised mass ( MH ). Combine will use this variable, instead of creating its own, which will be interpreted as the value for -m . For this reason, we should add the option -m 30 (or something else within the observable range) when running combine. You will also see there is a variable named bkg_norm . This is used to normalize the background rate (see the section on Rate parameters below for details). Warning Combine will not accept RooExtendedPdfs as an input. This is to alleviate a bug that lead to improper treatment of normalization when using multiple RooExtendedPdfs to describe a single process. You should instead use RooAbsPdfs and provide the rate as a separate object (see the Rate parameters section). The part of the datacard related to the systematics can include lines with the syntax name param X Y These lines encode uncertainties on the parameters of the signal and background pdfs. The parameter is to be assigned a Gaussian uncertainty of Y around its mean value of X . One can change the mean value from 0 to 1 (or really any value, if one so chooses) if the parameter in question is multiplicative instead of additive. In the data/tutorials/shapes/simple-shapes-parametric.txt datacard, there are lines for one such parametric uncertainty, sigma param 1.0 0.1 meaning there is a parameter already contained in the input workspace called sigma which should be constrained with a Gaussian centered at 1.0 with a width of 0.1. Note that, the exact interpretation (i.e all combine knows is that 1.0 should be the most likely value and 0.1 is its 1\u03c3 uncertainy) of these parameters is left to the user since the signal pdf is constructed externally by you. Asymmetric uncertainties are written as with lnN using the syntax -1\u03c3/+1\u03c3 in the datacard. If one wants to specify a parameter that is freely floating across its given range, and not gaussian constrained, the following syntax is used: name flatParam Though this is not strictly necessary in frequentist methods using profiled likelihoods as combine will still profile these nuisances when performing fits (as is the case for the simple-shapes-parametric.txt datacard). Warning All parameters which are floating or constant in the user's input workspaces will remain floating or constant. Combine will not modify those for you! A full example of a parametric analysis can be found in this H\u2192\u03b3\u03b3 2014 DAS tutorial","title":"Unbinned or parametric shape analysis"},{"location":"part2/settinguptheanalysis/#caveat-on-using-parametric-pdfs-with-binned-datasets","text":"Users should be aware of a feature that affects the use of parametric pdfs together with binned datasets. RooFit uses the integral of the pdf, computed analytically (or numerically, but disregarding the binning), to normalize it, but then computes the expected event yield in each bin evaluating only the pdf at the bin center. This means that if the variation of the pdf is sizeable within the bin then there is a mismatch between the sum of the event yields per bin and the pdf normalization, and that can cause a bias in the fits (more properly, the bias is there if the contribution of the second derivative integrated on the bin size is not negligible, since for linear functions evaluating them at the bin center is correct). There are two reccomended ways to work around this ... 1. Use narrow bins It is recommended to use bins that are significantly finer than the characteristic scale of the pdfs - which would anyway be the recommended thing even in the absence of this feature. Obviously, this caveat does not apply to analyses using templates (they're constant across each bin, so there's no bias), or using unbinned datasets. 2. Use a RooParametricShapeBinPdf Another solution (currently implemented for 1-dimensional histograms only) is to use a custom pdf which performs the correct integrals internally as in RooParametricShapeBinPdf Note that this pdf class now allows parameters that are themselves RooAbsReal objects (i.e. functions of other variables). The integrals are handled internally by calling the underlying pdf\u2019s createIntegral() method with named ranges created for each of the bins. This means that if the analytical integrals for the underlying pdf are available, they will be used. The constructor for this class requires a RooAbsReal (eg any RooAbsPdf )along with a list of RooRealVars (the parameters, excluding the observable x x ), RooParametricShapeBinPdf(const char *name, const char *title, RooAbsReal& _pdf, RooAbsReal& _x, RooArgList& _pars, const TH1 &_shape ) Below is a comparison of a fit to a binned dataset containing 1000 events with one observable 0 \\leq x \\leq 100 0 \\leq x \\leq 100 . The fit function is a RooExponential of the form e^{xp} e^{xp} . In the upper plot, the data are binned in 100 evenly spaced bins, while in the lower plot, there are 3 irregular bins. The blue lines show the result of the fit when using the RooExponential directly while the red shows the result when wrapping the pdf inside a RooParametricShapeBinPdf . In the narrow binned case, the two agree well while for wide bins, accounting for the integral over the bin yields a better fit. You should note that using this class will result in slower fits so you should first decide if the added accuracy is enough to justify the reduced efficiency.","title":"Caveat on using parametric pdfs with binned datasets"},{"location":"part2/settinguptheanalysis/#beyond-simple-datacards","text":"Datacards can be extended in order to provide additional functionality and flexibility during runtime. These can also allow for the production of more complicated models and performing advanced computation of results beyond limits and significances.","title":"Beyond simple datacards"},{"location":"part2/settinguptheanalysis/#rate-parameters","text":"The overall rate \"expected\" of a particular process in a particular bin does not necessarily need to be a fixed quantity. Scale factors can be introduced to modify the rate directly in the datacards for ANY type of analysis. This can be achieved using the directive rateParam in the datacard with the following syntax, name rateParam bin process initial_value [min,max] The [min,max] argument is optional and if not included, combine will remove the range of this parameter. This will produce a new parameter in the model (unless it already exists) which multiplies the rate of that particular process in the given bin by its value. You can attach the same rateParam to multiple processes/bins by either using a wild card (eg * will match everything, QCD_* will match everything starting with QCD_ etc.) in the name of the bin and/or process or by repeating the rateParam line in the datacard for different bins/processes with the same name. Warning rateParam is not a shortcut to evaluate the post-fit yield of a process since other nuisances can also change the normalisation . E.g., finding that the rateParam best-fit value is 0.9 does not necessarily imply that the process yield is 0.9 times the initial one. The best is to evaluate the yield taking into account the values of all nuisance parameters using --saveNormalizations . This parameter is by default, freely floating. It is possible to include a Gaussian constraint on any rateParam which is floating (i.e not a formula or spline) by adding a param nuisance line in the datacard with the same name. In addition to rate modifiers which are freely floating, modifiers which are functions of other parameters can be included using the following syntax, name rateParam bin process formula args where args is a comma separated list of the arguments for the string formula . You can include other nuisance parameters in the formula , including ones which are Gaussian constrained (i,e via the param directive.) Below is an example datacard which uses the rateParam directive to implement an ABCD like method in combine. For a more realistic description of it's use for ABCD, see the single-lepton SUSY search implementation described here imax 4 number of channels jmax 0 number of processes -1 kmax * number of nuisance parameters (sources of systematical uncertainties) ------- bin B C D A observation 50 100 500 10 ------- bin B C D A process bkg bkg bkg bkg process 1 1 1 1 rate 1 1 1 1 ------- alpha rateParam A bkg (@0*@1/@2) beta,gamma,delta beta rateParam B bkg 50 gamma rateParam C bkg 100 delta rateParam D bkg 500 For more examples of using rateParam (eg for fitting process normalisations in control regions and signal regions simultaneously) see this 2016 CMS tutorial Finally, any pre-existing RooAbsReal inside some rootfile with a workspace can be imported using the following name rateParam bin process rootfile:workspacename The name should correspond to the name of the object which is being picked up inside the RooWorkspace. A simple example using the SM XS and BR splines available in HiggsAnalysis/CombinedLimit can be found under data/tutorials/rate_params/simple_sm_datacard.txt After running text2workspace.py on your datacard, you can check the normalisation objects using the tool test/printWorkspaceNormalisations.py . See the example below for the data/tutorials/shapes/simple-shapes-parametric.txt datacard. text2workspace.py data/tutorials/shapes/simple-shapes-parametric.txt python test/printWorkspaceNormalisations.py data/tutorials/shapes/simple-shapes-parametric.root ... --------------------------------------------------------------------------- --------------------------------------------------------------------------- Channel - bin1 --------------------------------------------------------------------------- Top-level normalisation for process bkg -> n_exp_final_binbin1_proc_bkg ------------------------------------------------------------------------- RooProduct::n_exp_final_binbin1_proc_bkg[ n_exp_binbin1_proc_bkg * shapeBkg_bkg_bin1__norm ] = 521.163 ... is a product, which contains n_exp_binbin1_proc_bkg RooRealVar::n_exp_binbin1_proc_bkg = 1 C L(-INF - +INF) ------------------------------------------------------------------------- default value = 521.163204829 --------------------------------------------------------------------------- Top-level normalisation for process sig -> n_exp_binbin1_proc_sig ------------------------------------------------------------------------- Dumping ProcessNormalization n_exp_binbin1_proc_sig @ 0x464f700 nominal value: 1 log-normals (1): kappa = 1.1, logKappa = 0.0953102, theta = lumi = 0 asymm log-normals (0): other terms (1): term r (class RooRealVar), value = 1 ------------------------------------------------------------------------- default value = 1.0 This tells us that the normalisation for the background process, named n_exp_final_binbin1_proc_bkg is a product of two objects n_exp_binbin1_proc_bkg * shapeBkg_bkg_bin1__norm . The first object is just from the rate line in the datacard (equal to 1) and the second is a floating parameter. For the signal, the normalisation is called n_exp_binbin1_proc_sig and is a ProcessNormalization object which contains the rate modifications due to the systematic uncertainties. You can see that it also has a \" nominal value \" which again is just from the value given in the rate line of the datacard (again=1).","title":"Rate parameters"},{"location":"part2/settinguptheanalysis/#extra-arguments","text":"If a parameter is intended to be used and it is not a user defined param or rateParam , it can be picked up by first issuing an extArgs directive before this line in the datacard. The syntax for extArgs is name extArg rootfile:workspacename The string \":RecycleConflictNodes\" can be added at the end of the final argument (i.e. rootfile:workspacename:RecycleConflictNodes) to apply the corresponding RooFit option when the object is imported into the workspace. It is also possible to simply add a RooRealVar using extArg for use in function rateParams with the following name extArg init [min,max] Note that the [min,max] argument is optional and if not included, the code will remove the range of this parameter.","title":"Extra arguments"},{"location":"part2/settinguptheanalysis/#manipulation-of-nuisance-parameters","text":"It can often be useful to modify datacards, or the runtime behavior, without having to modify individual systematics lines. This can be acheived through the following.","title":"Manipulation of Nuisance parameters"},{"location":"part2/settinguptheanalysis/#nuisance-modifiers","text":"If a nuisance parameter needs to be renamed for certain processes/channels, it can be done so using a single nuisance edit directive at the end of a datacard nuisance edit rename process channel oldname newname [options] Note that the wildcard ( * ) can be used for either/both of process and channel. This will have the effect that nuisance parameter effecting a given process/channel will be renamed, thereby de-correlating it from other processes/channels. Use options ifexists to skip/avoid error if nuisance not found. This kind of command will only effect nuisances of the type shape[N] , lnN . Instead, if you also want to change the names of param type nuisances, you can use a global version nuisance edit rename oldname newname which will rename all shape[N] , lnN and param nuisances found in one go. You should make sure these commands come after any process/channel specific ones in the datacard. This version does not accept options. Other edits are also supported as follows, nuisance edit add process channel name pdf value [options] -> add a new or add to a nuisance. If options is addq , value will be added in quadrature to this nuisance for this process/channel. If options is overwrite , the nuisance value will be replaced with this value nuisance edit drop process channel name [options] -> remove this nuisance from the process/channel. Use options ifexists to skip/avoid error if nuisance not found. nuisance edit changepdf name newpdf -> change the pdf type of a given nuisance to newpdf . nuisance edit split process channel oldname newname1 newname2 value1 value2 -> split a nuisance line into two separate nuisances called newname1 and newname2 with values value1 and value2 . Will produce two separate lines to that the original nuisance oldname becomes two uncorrelated nuisances. nuisance edit freeze name [options] -> set nuisance to frozen by default. Can be over-ridden in combine command line using --floatNuisances option Use options ifexists to skip/avoid error if nuisance not found. nuisance edit merge process channel name1 name2 -> merge systematic name2 into name1 by adding their values in quadrature and removing name2 . This only works if, for each process and channel included, they go in the same direction. For example, you can add 1.1 to 1.2, but not to 0.9. The above edits (excluding the renaming) support nuisances which are any of shape[N] , lnN , lnU , gmN , param , flatParam , rateParam or discrete types.","title":"Nuisance modifiers"},{"location":"part2/settinguptheanalysis/#groups-of-nuisances","text":"Often it is desirable to freeze one or more nuisances to check the impact they have on limits, likelihood scans, significances etc. However, for large groups of nuisances (eg everything associated to theory) it is easier to define nuisance groups in the datacard. The following line in a datacard will, for example, produce a group of nuisances with the group name theory which contains two parameters, QCDscale and pdf . theory group = QCDscale pdf Multiple groups can be defined in this way. It is also possible to extend nuisance groups in datacards using += in place of = . These groups can be manipulated at runtime (eg for freezing all nuisances associated to a group at runtime, see Running the tool ). You can find more info on groups of nuisances here Note that when using the automatic addition of statistical uncertainties (autoMCStats), the corresponding nuisance parameters are created by text2workspace.py and so do not exist in the datacards. It is therefore not possible to add autoMCStats parameters to groups of nuisances in the way described above. However, text2workspace.py will automatically create a group labelled autoMCStats which contains all autoMCStats parameters. This group is useful for freezing all parameters created by autoMCStats. For freezing subsets of the parameters, for example if the datacard contains two categories, cat_label_1 and cat_label_2 , to only freeze the autoMCStat parameters created for category cat_label_1 the regular expression features can be used. In this example this can be achieved by using --freezeParameters 'rgx{prop_bincat_label_1_bin.*}' .","title":"Groups of nuisances"},{"location":"part2/settinguptheanalysis/#combination-of-multiple-datacards","text":"If you have separate channels each with it's own datacard, it is possible to produce a combined datacard using the script combineCards.py The syntax is simple: combineCards.py Name1=card1.txt Name2=card2.txt .... > card.txt If the input datacards had just one bin each, then the output channels will be called Name1 , Name2 , and so on. Otherwise, a prefix Name1_ ... Name2_ will be added to the bin labels in each datacard. The supplied bin names Name1 , Name2 , etc. must themselves conform to valid C++/python identifier syntax. Warning Systematics which have different names will be assumed to be uncorrelated, and the ones with the same name will be assumed 100% correlated. A systematic correlated across channels must have the same p.d.f. in all cards (i.e. always lnN , or all gmN with same N ) The combineCards.py script will complain if you are trying to combine a shape datacard with a counting datacard. You can however convert a counting datacard in an equivalent shape-based one by adding a line shapes * * FAKE in the datacard after the imax , jmax and kmax section. Alternatively, you can add the option -S in combineCards.py which will do this for you while making the combination.","title":"Combination of multiple datacards"},{"location":"part2/settinguptheanalysis/#automatic-production-of-datacards-and-workspaces","text":"For complicated analyses or cases in which multiple datacards are needed (e.g. optimisation studies), you can avoid writing these by hand. The object Datacard defines the analysis and can be created as a python object. The template python script below will produce the same workspace as running textToWorkspace.py (see the section on Physics Models ) on the realistic-counting-experiment.txt datacard. from HiggsAnalysis.CombinedLimit.DatacardParser import * from HiggsAnalysis.CombinedLimit.ModelTools import * from HiggsAnalysis.CombinedLimit.ShapeTools import * from HiggsAnalysis.CombinedLimit.PhysicsModel import * from sys import exit from optparse import OptionParser parser = OptionParser() addDatacardParserOptions(parser) options,args = parser.parse_args() options.bin = True # make a binary workspace DC = Datacard() MB = None ############## Setup the datacard (must be filled in) ########################### DC.bins = ['bin1'] # <type 'list'> DC.obs = {'bin1': 0.0} # <type 'dict'> DC.processes = ['ggH', 'qqWW', 'ggWW', 'others'] # <type 'list'> DC.signals = ['ggH'] # <type 'list'> DC.isSignal = {'qqWW': False, 'ggWW': False, 'ggH': True, 'others': False} # <type 'dict'> DC.keyline = [('bin1', 'ggH', True), ('bin1', 'qqWW', False), ('bin1', 'ggWW', False), ('bin1', 'others', False)] # <type 'list'> DC.exp = {'bin1': {'qqWW': 0.63, 'ggWW': 0.06, 'ggH': 1.47, 'others': 0.22}} # <type 'dict'> DC.systs = [('lumi', False, 'lnN', [], {'bin1': {'qqWW': 0.0, 'ggWW': 1.11, 'ggH': 1.11, 'others': 0.0}}), ('xs_ggH', False, 'lnN', [], {'bin1': {'qqWW': 0.0, 'ggWW': 0.0, 'ggH': 1.16, 'others': 0.0}}), ('WW_norm', False, 'gmN', [4], {'bin1': {'qqWW': 0.16, 'ggWW': 0.0, 'ggH': 0.0, 'others': 0.0}}), ('xs_ggWW', False, 'lnN', [], {'bin1': {'qqWW': 0.0, 'ggWW': 1.5, 'ggH': 0.0, 'others': 0.0}}), ('bg_others', False, 'lnN', [], {'bin1': {'qqWW': 0.0, 'ggWW': 0.0, 'ggH': 0.0, 'others': 1.3}})] # <type 'list'> DC.shapeMap = {} # <type 'dict'> DC.hasShapes = False # <type 'bool'> DC.flatParamNuisances = {} # <type 'dict'> DC.rateParams = {} # <type 'dict'> DC.extArgs = {} # <type 'dict'> DC.rateParamsOrder = set([]) # <type 'set'> DC.frozenNuisances = set([]) # <type 'set'> DC.systematicsShapeMap = {} # <type 'dict'> DC.nuisanceEditLines = [] # <type 'list'> DC.groups = {} # <type 'dict'> DC.discretes = [] # <type 'list'> ###### User defined options ############################################# options.out = \"combine_workspace.root\" # Output workspace name options.fileName = \"./\" # Path to input ROOT files options.verbose = \"1\" # Verbosity ########################################################################## if DC.hasShapes: MB = ShapeBuilder(DC, options) else: MB = CountingModelBuilder(DC, options) # Set physics models MB.setPhysics(defaultModel) MB.doModel() Any existing datacard can be converted into such a template python script by using the --dump-datacard option in text2workspace.py in case a more complicated template is needed. Warning The above is not advised for final results as this script is not easily combined with other analyses so should only be used for internal studies. For the automatic generation of datacards (which are combinable), you should instead use the CombineHarvester package which includes many features for producing complex datacards in a reliable, automated way.","title":"Automatic production of datacards and workspaces"},{"location":"part2/settinguptheanalysis/#sanity-checking-the-datacard","text":"For large combinations with multiple channels/processes etc, the .txt file can get unweildy to read through. There are some simple tools to help check and disseminate the contents of the cards. In order to get a quick view of the systematic uncertainties included in the datacard, you can use the test/systematicsAnalyzer.py tool. This will produce a list of the systematic uncertainties (normalisation and shape), indicating what type they are, which channels/processes they affect and the size of the affect on the normalisation (for shape uncertainties, this will just be the overall uncertaintly on the normalisation information). The default output is a .html file which allows you to expand to give more details about the affect of the systematic for each channel/process. Add the option --format brief to give a simpler summary report direct to the terminal. An example output for the tutorial card data/tutorials/shapes/simple-shapes-TH1.txt is shown below. $ python test/systematicsAnalyzer.py data/tutorials/shapes/simple-shapes-TH1.txt > out.html In case you only have a cut-and-count style card, include the option --noshape . If you have a datacard which uses several rateParams or a Physics model which includes some complicated product of normalisation terms in each process, you can check the values of the normalisation (and which objects in the workspace comprise them) using the test/printWorkspaceNormalisations.py tool. As an example, below is the first few blocks of output for the tutorial card data/tutorials/counting/realistic-multi-channel.txt . $ text2workspace.py data/tutorials/shapes/simple-shapes-parametric.txt -m 30 $ python test/printWorkspaceNormalisations.py data/tutorials/counting/realistic-multi-channel.root --------------------------------------------------------------------------- --------------------------------------------------------------------------- Channel - mu_tau --------------------------------------------------------------------------- Top-level normalisation for process ZTT -> n_exp_binmu_tau_proc_ZTT ------------------------------------------------------------------------- Dumping ProcessNormalization n_exp_binmu_tau_proc_ZTT @ 0x6bbb610 nominal value: 329 log-normals (3): kappa = 1.23, logKappa = 0.207014, theta = tauid = 0 kappa = 1.04, logKappa = 0.0392207, theta = ZtoLL = 0 kappa = 1.04, logKappa = 0.0392207, theta = effic = 0 asymm log-normals (0): other terms (0): ------------------------------------------------------------------------- default value = 329.0 --------------------------------------------------------------------------- Top-level normalisation for process QCD -> n_exp_binmu_tau_proc_QCD ------------------------------------------------------------------------- Dumping ProcessNormalization n_exp_binmu_tau_proc_QCD @ 0x6bbcaa0 nominal value: 259 log-normals (1): kappa = 1.1, logKappa = 0.0953102, theta = QCDmu = 0 asymm log-normals (0): other terms (0): ------------------------------------------------------------------------- default value = 259.0 --------------------------------------------------------------------------- Top-level normalisation for process higgs -> n_exp_binmu_tau_proc_higgs ------------------------------------------------------------------------- Dumping ProcessNormalization n_exp_binmu_tau_proc_higgs @ 0x6bc6390 nominal value: 0.57 log-normals (3): kappa = 1.11, logKappa = 0.10436, theta = lumi = 0 kappa = 1.23, logKappa = 0.207014, theta = tauid = 0 kappa = 1.04, logKappa = 0.0392207, theta = effic = 0 asymm log-normals (0): other terms (1): term r (class RooRealVar), value = 1 ------------------------------------------------------------------------- default value = 0.57 --------------------------------------------------------------------------- --------------------------------------------------------------------------- Channel - e_mu --------------------------------------------------------------------------- Top-level normalisation for process ZTT -> n_exp_bine_mu_proc_ZTT ------------------------------------------------------------------------- Dumping ProcessNormalization n_exp_bine_mu_proc_ZTT @ 0x6bc8910 nominal value: 88 log-normals (2): kappa = 1.04, logKappa = 0.0392207, theta = ZtoLL = 0 kappa = 1.04, logKappa = 0.0392207, theta = effic = 0 asymm log-normals (0): other terms (0): ------------------------------------------------------------------------- default value = 88.0 --------------------------------------------------------------------------- As you can see, for each channel, a report is given for the top-level rate object in the workspace, for each process contributing to that channel. You can also see the various terms which make up that rate. The default value is for the default parameters in the workspace (i.e when running text2workspace , these are the values created as default).","title":"Sanity checking the datacard"},{"location":"part3/commonstatsmethods/","text":"Common Statistical Methods In this section, the most commonly used statistical methods from combine will be covered including specific instructions on how to obtain limits, significances and likelihood scans. For all of these methods, the assumed parameters of interest (POI) is the overall signal strength r (i.e the default PhysicsModel). In general however, the first POI in the list of POIs (as defined by the PhysicsModel) will be taken instead of r which may or may not make sense for a given method ... use your judgment! This section will assume that you are using the default model unless otherwise specified. Asymptotic Frequentist Limits The AsymptoticLimits method allows to compute quickly an estimate of the observed and expected limits, which is fairly accurate when the event yields are not too small and the systematic uncertainties don't play a major role in the result. The limit calculation relies on an asymptotic approximation of the distributions of the LHC test-statistic, which is based on a profile likelihood ratio, under signal and background hypotheses to compute two p-values p_{\\mu}, p_{b} p_{\\mu}, p_{b} and therefore CL_s=p_{\\mu}/(1-p_{b}) CL_s=p_{\\mu}/(1-p_{b}) (see the (see the FAQ section for a description of these) - i.e it is the asymptotic approximation of computing limits with frequentist toys. This method is so commonly used that it is the default method (i.e not specifying -M will run AsymptoticLimits ) A realistic example of datacard for a counting experiment can be found in the HiggsCombination package: data/tutorials/counting/realistic-counting-experiment.txt The method can be run using combine -M AsymptoticLimits realistic-counting-experiment.txt The program will print out the limit on the signal strength r (number of signal events / number of expected signal events) e .g. Observed Limit: r < 1.6297 @ 95% CL , the median expected limit Expected 50.0%: r < 2.3111 and edges of the 68% and 95% ranges for the expected limits. <<< Combine >>> >>> including systematics >>> method used to compute upper limit is AsymptoticLimits [...] -- AsymptoticLimits ( CLs ) -- Observed Limit: r < 1.6281 Expected 2.5%: r < 0.9640 Expected 16.0%: r < 1.4329 Expected 50.0%: r < 2.3281 Expected 84.0%: r < 3.9800 Expected 97.5%: r < 6.6194 Done in 0.01 min (cpu), 0.01 min (real) By default, the limits are calculated using the CL s prescription, as noted in the output, which takes the ratio of p-values under the signal plus background and background only hypothesis. This can be altered to using the strict p-value by using the option --rule CLsplusb (note that CLsplusb is the jargon for calculating the p-value p_{\\mu} p_{\\mu} ). You can also change the confidence level (default is 95%) to 90% using the option --cl 0.9 or any other confidence level. You can find the full list of options for AsymptoticLimits using --help -M AsymptoticLimits . Warning You may find that combine issues a warning that the best fit for the background-only Asimov dataset returns a non-zero value for the signal strength for example; WARNING: Best fit of asimov dataset is at r = 0.220944 (0.011047 times rMax), while it should be at zero If this happens, you should check to make sure that there are no issues with the datacard or the Asimov generation used for your setup. For details on debugging it is recommended that you follow the simple checks used by the HIG PAG here . The program will also create a rootfile higgsCombineTest.AsymptoticLimits.mH120.root containing a root tree limit that contains the limit values and other bookeeping information. The important columns are limit (the limit value) and quantileExpected (-1 for observed limit, 0.5 for median expected limit, 0.16/0.84 for the edges of the 65% interval band of expected limits, 0.025/0.975 for 95%). $ root -l higgsCombineTest.AsymptoticLimits.mH120.root root [0] limit->Scan(\"*\") ************************************************************************************************************************************ * Row * limit * limitErr * mh * syst * iToy * iSeed * iChannel * t_cpu * t_real * quantileE * ************************************************************************************************************************************ * 0 * 0.9639892 * 0 * 120 * 1 * 0 * 123456 * 0 * 0 * 0 * 0.0250000 * * 1 * 1.4329109 * 0 * 120 * 1 * 0 * 123456 * 0 * 0 * 0 * 0.1599999 * * 2 * 2.328125 * 0 * 120 * 1 * 0 * 123456 * 0 * 0 * 0 * 0.5 * * 3 * 3.9799661 * 0 * 120 * 1 * 0 * 123456 * 0 * 0 * 0 * 0.8399999 * * 4 * 6.6194028 * 0 * 120 * 1 * 0 * 123456 * 0 * 0 * 0 * 0.9750000 * * 5 * 1.6281188 * 0.0050568 * 120 * 1 * 0 * 123456 * 0 * 0.0035000 * 0.0055123 * -1 * ************************************************************************************************************************************ Blind limits The AsymptoticLimits calculation follows the frequentist paradigm for calculating expected limits. This means that the routine will first fit the observed data, conditionally for a fixed value of r and set the nuisance parameters to the values obtained in the fit for generating the Asimov data, i.e it calculates the post-fit or a-posteriori expected limit. In order to use the pre-fit nuisance parameters (to calculate an a-priori limit), you must add the option --noFitAsimov or --bypassFrequentistFit . For blinding the results completely (i.e not using the data) you can include the option --run blind . Warning You should never use -t -1 to get blind limits! Splitting points In case your model is particularly complex, you can perform the asymptotic calculation by determining the value of CL s for a set grid of points (in r ) and merging the results. This is done by using the option --singlePoint X for multiple values of X, hadding the output files and reading them back in, combine -M AsymptoticLimits realistic-counting-experiment.txt --singlePoint 0.1 -n 0.1 combine -M AsymptoticLimits realistic-counting-experiment.txt --singlePoint 0.2 -n 0.2 combine -M AsymptoticLimits realistic-counting-experiment.txt --singlePoint 0.3 -n 0.3 ... hadd limits.root higgsCombine*.AsymptoticLimits.* combine -M AsymptoticLimits realistic-counting-experiment.txt --getLimitFromGrid limits.root Asymptotic Significances The significance of a result is calculated using a ratio of profiled likelihoods, one in which the signal strength is set to 0 and the other in which it is free to float, i.e the quantity is -2\\ln[\\mathcal{L}(\\textrm{data}|r=0,\\hat{\\theta}_{0})/\\mathcal{L}(\\textrm{data}|r=\\hat{r},\\hat{\\theta})] -2\\ln[\\mathcal{L}(\\textrm{data}|r=0,\\hat{\\theta}_{0})/\\mathcal{L}(\\textrm{data}|r=\\hat{r},\\hat{\\theta})] , in which the nuisance parameters are profiled separately for r=\\hat{r} r=\\hat{r} and r=0 r=0 . The distribution of this test-statistic can be determined using Wilke's theorem provided the number of events is large enough (i.e in the Asymptotic limit ). The significance (or p-value) can therefore be calculated very quickly and uses the Significance method. It is also possible to calculate the ratio of likelihoods between the freely floating signal strength to that of a fixed signal strength other than 0 , by specifying it with the option --signalForSignificance=X Info This calculation assumes that the signal strength can only be positive (i.e we are not interested in negative signal strengths). This can be altered by including the option --uncapped Compute the observed significance The observed significance is calculated using the Significance method, as combine -M Significance datacard.txt The printed output will report the significance and the p-value, for example, when using the realistic-counting-experiment.txt datacard, you will see <<< Combine >>> >>> including systematics >>> method used is Significance [...] -- Significance -- Significance: 0 (p-value = 0.5) Done in 0.00 min (cpu), 0.01 min (real) which is not surprising since 0 events were observed in that datacard. The output root file will contain the significance value in the branch limit . To store the p-value instead, include the option --pval . These can be converted between one another using the RooFit functions RooFit::PValueToSignificance and RooFit::SignificanceToPValue . You may find it useful to resort to a brute-force fitting algorithm when calculating the significance which scans the nll (repeating fits until a tolerance is reached), bypassing MINOS, which can be activated with the option bruteForce . This can be tuned using the options setBruteForceAlgo , setBruteForceTypeAndAlgo and setBruteForceTolerance . Computing the expected significance The expected significance can be computed from an Asimov dataset of signal+background. There are two options for this a-posteriori expected: will depend on the observed dataset. a-priori expected (the default behavior): does not depend on the observed dataset, and so is a good metric for optimizing an analysis when still blinded. The a-priori expected significance from the Asimov dataset is calculated as combine -M Significance datacard.txt -t -1 --expectSignal=1 In order to produced the a-posteriori expected significance, just generate a post-fit Asimov (i.e add the option --toysFreq in the command above). The output format is the same as for observed signifiances: the variable limit in the tree will be filled with the significance (or with the p-value if you put also the option --pvalue ) Bayesian Limits and Credible regions Bayesian calculation of limits requires the user to assume a particular prior distribution for the parameter of interest (default r ). You can specify the prior using the --prior option, the default is a flat pior in r . Since the Bayesian methods are much less frequently used, the tool will not build the default prior. For running the two methods below, you should include the option --noDefaultPrior=0 . Computing the observed bayesian limit (for simple models) The BayesianSimple method computes a Bayesian limit performing classical numerical integration; very fast and accurate but only works for simple models (a few channels and nuisance parameters). combine -M BayesianSimple simple-counting-experiment.txt --noDefaultPrior=0 [...] -- BayesianSimple -- Limit: r < 0.672292 @ 95% CL Done in 0.04 min (cpu), 0.05 min (real) The output tree will contain a single entry corresponding to the observed 95% upper limit. The confidence level can be modified to 100*X% using --cl X . Computing the observed bayesian limit (for arbitrary models) The MarkovChainMC method computes a Bayesian limit performing a monte-carlo integration. From the statistics point of view it is identical to the BayesianSimple method, only the technical implementation is different. The method is slower, but can also handle complex models. For this method, you can increase the accuracy of the result by increasing the number of markov chains at the expense of a longer running time (option --tries , default is 10). Let's use the realistic counting experiment datacard to test the method To use the MarkovChainMC method, users need to specify this method in the command line, together with the options they want to use. For instance, to set the number of times the algorithm will run with different random seeds, use option --tries : combine -M MarkovChainMC realistic-counting-experiment.txt --tries 100 --noDefaultPrior=0 [...] -- MarkovChainMC -- Limit: r < 2.20438 +/- 0.0144695 @ 95% CL (100 tries) Average chain acceptance: 0.078118 Done in 0.14 min (cpu), 0.15 min (real) Again, the resulting limit tree will contain the result. You can also save the chains using the option --saveChain which will then also be included in the output file. Exclusion regions can be made from the posterior once an ordering principle is defined to decide how to grow the contour (there's infinite possible regions that contain 68% of the posterior pdf...) Below is a simple example script which can be used to plot the posterior distribution from these chains and calculate the smallest such region. Note that in this example we are ignoring the burn-in (but you can add it by just editing for i in range(mychain.numEntries()): to for i in range(200,mychain.numEntries()): eg for a burn-in of 200. import ROOT rmin = 0 rmax = 30 nbins = 100 CL = 0.95 chains = \"higgsCombineTest.MarkovChainMC.blahblahblah.root\" def findSmallestInterval(hist,CL): bins = hist.GetNbinsX() best_i = 1 best_j = 1 bd = bins+1 val = 0; for i in range(1,bins+1): integral = hist.GetBinContent(i) for j in range(i+1,bins+2): integral += hist.GetBinContent(j) if integral > CL : val = integral break if integral > CL and j-i < bd : bd = j-i best_j = j+1 best_i = i val = integral return hist.GetBinLowEdge(best_i), hist.GetBinLowEdge(best_j), val fi_MCMC = ROOT.TFile.Open(chains) # Sum up all of the chains (or we could take the average limit) mychain=0 for k in fi_MCMC.Get(\"toys\").GetListOfKeys(): obj = k.ReadObj if mychain ==0: mychain = k.ReadObj().GetAsDataSet() else : mychain.append(k.ReadObj().GetAsDataSet()) hist = ROOT.TH1F(\"h_post\",\";r;posterior probability\",nbins,rmin,rmax) for i in range(mychain.numEntries()): #for i in range(200,mychain.numEntries()): burn-in of 200 mychain.get(i) hist.Fill(mychain.get(i).getRealValue(\"r\"), mychain.weight()) hist.Scale(1./hist.Integral()) hist.SetLineColor(1) vl,vu,trueCL = findSmallestInterval(hist,CL) histCL = hist.Clone() for b in range(nbins): if histCL.GetBinLowEdge(b+1) < vl or histCL.GetBinLowEdge(b+2)>vu: histCL.SetBinContent(b+1,0) c6a = ROOT.TCanvas() histCL.SetFillColor(ROOT.kAzure-3) histCL.SetFillStyle(1001) hist.Draw() histCL.Draw(\"histFsame\") hist.Draw(\"histsame\") ll = ROOT.TLine(vl,0,vl,2*hist.GetBinContent(hist.FindBin(vl))); ll.SetLineColor(2); ll.SetLineWidth(2) lu = ROOT.TLine(vu,0,vu,2*hist.GetBinContent(hist.FindBin(vu))); lu.SetLineColor(2); lu.SetLineWidth(2) ll.Draw() lu.Draw() print \" %g %% (%g %%) interval (target) = %g < r < %g \"%(trueCL,CL,vl,vu) Running the script on the output file produced for the same datacard (including the --saveChain option) will produce the following output 0.950975 % (0.95 %) interval (target) = 0 < r < 2.2 along with a plot of the posterior shown below. This is the same as the output from combine but the script can also be used to find lower limits (for example) or credible intervals. An example to make contours when ordering by probability density is in bayesContours.cxx , but the implementation is very simplistic, with no clever handling of bin sizes nor any smoothing of statistical fluctuations. The MarkovChainMC algorithm has many configurable parameters, and you're encouraged to experiment with those because the default configuration might not be the best for you (or might not even work for you at all) Iterations, burn-in, tries Three parameters control how the MCMC integration is performed: the number of tries (option --tries ): the algorithm will run multiple times with different ransom seeds and report as result the truncated mean and rms of the different results. The default value is 10, which should be ok for a quick computation, but for something more accurate you might want to increase this number even up to ~200. the number of iterations (option -i ) determines how many points are proposed to fill a single Markov Chain. The default value is 10k, and a plausible range is between 5k (for quick checks) and 20-30k for lengthy calculations. Usually beyond 30k you get a better tradeoff in time vs accuracy by increasing the number of chains (option --tries ) the number of burn-in steps (option -b ) is the number of points that are removed from the beginning of the chain before using it to compute the limit. The default is 200. If your chain is very long, you might want to try increase this a bit (e.g. to some hundreds). Instead going below 50 is probably dangerous. Proposals The option --proposal controls the way new points are proposed to fill in the MC chain. uniform : pick points at random. This works well if you have very few nuisance parameters (or none at all), but normally fails if you have many. gaus : Use a product of independent gaussians one for each nuisance parameter; the sigma of the gaussian for each variable is 1/5 of the range of the variable (this can be controlled using the parameter --propHelperWidthRangeDivisor ). This proposal appears to work well for a reasonable number of nuisances (up to ~15), provided that the range of the nuisance parameters is reasonable, like \u00b15\u03c3. It does not work without systematics. ortho ( default ): This proposalis similar to the multi-gaussian proposal but at every step only a single coordinate of the point is varied, so that the acceptance of the chain is high even for a large number of nuisances (i.e. more than 20). fit : Run a fit and use the uncertainty matrix from HESSE to construct a proposal (or the one from MINOS if the option --runMinos is specified). This sometimes work fine, but sometimes gives biased results, so we don't recommend it in general. If you believe there's something going wrong, e.g. if your chain remains stuck after accepting only a few events, the option --debugProposal can be used to have a printout of the first N proposed points to see what's going on (e.g. if you have some region of the phase space with probability zero, the gaus and fit proposal can get stuck there forever) Computing the expected bayesian limit The expected limit is computed by generating many toy mc observations and compute the limit for each of them. This can be done passing the option -t . E.g. to run 100 toys with the BayesianSimple method, just do combine -M BayesianSimple datacard.txt -t 100 --noDefaultPrior=0 The program will print out the mean and median limit, and the 68% and 95% quantiles of the distributions of the limits. This time, the output root tree will contain one entry per toy . For more heavy methods (eg the MarkovChainMC ) you'll probably want to split this in multiple jobs. To do this, just run combine multiple times specifying a smaller number of toys (can be as low as 1 ) each time using a different seed to initialize the random number generator (option -s if you set it to -1, the starting seed will be initialized randomly at the beginning of the job), then merge the resulting trees with hadd and look at the distribution in the merged file. Multidimensional bayesian credible regions The MarkovChainMC method allows the user to produce the posterior pdf as a function of (in principle) any number of parameter of interest. In order to do so, you first need to create a workspace with more than one parameter, as explained in the physics models section. For example, lets use the toy datacard test/multiDim/toy-hgg-125.txt (counting experiment which vaguely resembles the H\u2192\u03b3\u03b3 analysis at 125 GeV) and convert the datacard into a workspace with 2 parameters, ggH and qqH cross sections using text2workspace with the option -P HiggsAnalysis.CombinedLimit.PhysicsModel:floatingXSHiggs --PO modes=ggH,qqH . Now we just run one (or more) MCMC chain(s) and save them in the output tree.By default, the nuisance parameters will be marginalized (integrated) over their pdfs. You can ignore the complaints about not being able to compute an upper limit (since for more than 1D, this isn't well defined), combine -M MarkovChainMC workspace.root --tries 1 --saveChain -i 1000000 -m 125 -s seed --noDefaultPrior=0 The output of the markov chain is again a RooDataSet of weighted events distributed according to the posterior pdf (after you cut out the burn in part), so it can be used to make histograms or other distributions of the posterior pdf. See as an example bayesPosterior2D.cxx . Below is an example of the output of the macro, $ root -l higgsCombineTest.MarkovChainMC.... .L bayesPosterior2D.cxx bayesPosterior2D(\"bayes2D\",\"Posterior PDF\") Computing Limits with toys The HybridNew method is used to compute either the hybrid bayesian-frequentist limits popularly known as \"CL s of LEP or Tevatron type\" or the fully frequentist limits which are the current recommended method by the LHC Higgs Combination Group. Note that these methods can be resource intensive for complex models. It is possible to define the criterion used for setting limits using --rule CLs (to use the CL s criterion) or --rule CLsplusb (to calculate the limit using p_{\\mu} p_{\\mu} ) and as always the confidence level desired using --cl=X The choice of test-statistic can be made via the option --testStat and different methodologies for treatment of the nuisance parameters are available. While it is possible to mix different test-statistics with different nuisance parameter treatments, this is highly not-reccomended . Instead one should follow one of the following three procedures, LEP-style : --testStat LEP --generateNuisances=1 --fitNuisances=0 The test statistic is defined using the ratio of likelihoods q_{\\mathrm{LEP}}=-2\\ln[\\mathcal{L}(\\mathrm{data}|r=0)/\\mathcal{L}(\\mathrm{data}|r)] q_{\\mathrm{LEP}}=-2\\ln[\\mathcal{L}(\\mathrm{data}|r=0)/\\mathcal{L}(\\mathrm{data}|r)] . The nuisance parameters are fixed to their nominal values for the purpose of evaluating the likelihood, while for generating toys, the nuisance parameters are first randomized within their pdfs before generation of the toy. TEV-style : --testStat TEV --generateNuisances=0 --generateExternalMeasurements=1 --fitNuisances=1 The test statistic is defined using the ratio of likelihoods q_{\\mathrm{TEV}}=-2\\ln[\\mathcal{L}(\\mathrm{data}|r=0,\\hat{\\theta}_{0})/\\mathcal{L}(\\mathrm{data}|r,\\hat{\\theta}_{r})] q_{\\mathrm{TEV}}=-2\\ln[\\mathcal{L}(\\mathrm{data}|r=0,\\hat{\\theta}_{0})/\\mathcal{L}(\\mathrm{data}|r,\\hat{\\theta}_{r})] , in which the nuisance parameters are profiled separately for r=0 r=0 and r r . For the purposes of toy generation, the nuisance parameters are fixed to their post-fit values from the data (conditional on r), while the constraint terms are randomized for the evaluation of the likelihood. LHC-style : --LHCmode LHC-limits , which is the shortcut for --testStat LHC --generateNuisances=0 --generateExternalMeasurements=1 --fitNuisances=1 The test statistic is defined using the ratio of likelihoods q_{r} = -2\\ln[\\mathcal{L}(\\mathrm{data}|r,\\hat{\\theta}_{r})/\\mathcal{L}(\\mathrm{data}|r=\\hat{r},\\hat{\\theta}]) q_{r} = -2\\ln[\\mathcal{L}(\\mathrm{data}|r,\\hat{\\theta}_{r})/\\mathcal{L}(\\mathrm{data}|r=\\hat{r},\\hat{\\theta}]) , in which the nuisance parameters are profiled separately for r=\\hat{r} r=\\hat{r} and r r . The value of q_{r} q_{r} set to 0 when \\hat{r}>r \\hat{r}>r giving a one sided limit. Furthermore, the constraint r>0 r>0 is enforced in the fit. This means that if the unconstrained value of \\hat{r} \\hat{r} would be negative, the test statistic q_{r} q_{r} is evaluated as -2\\ln[\\mathcal{L}(\\mathrm{data}|r,\\hat{\\theta}_{r})/\\mathcal{L}(\\mathrm{data}|r=0,\\hat{\\theta}_{0}]) -2\\ln[\\mathcal{L}(\\mathrm{data}|r,\\hat{\\theta}_{r})/\\mathcal{L}(\\mathrm{data}|r=0,\\hat{\\theta}_{0}]) For the purposes of toy generation, the nuisance parameters are fixed to their post-fit values from the data (conditionally on the value of r ), while the constraint terms are randomized in the evaluation of the likelihood. Warning The recommended style is the LHC-style . Please note that this method is sensitive to the observation in data since the post-fit (after a fit to the data) values of the nuisance parameters (assuming different values of r ) are used when generating the toys. For completely blind limits you can first generate a pre-fit asimov toy dataset (described in the toy data generation section) and use that in place of the data. While the above shortcuts are the common variants, you can also try others. The treatment of the nuisances can be changed to the so-called \"Hybrid-Bayesian\" method which effectively integrates over the nuisance parameters. This can be achieved (with any test-statistic which is not profiled over the nuisances) by setting --generateNuisances=1 --generateExternalMeasurements=0 --fitNuisances=0 . Info Note that (observed and toy) values of the test statistic stored in the instances of RooStats::HypoTestResult when the option --saveHybridResult has been specified, are defined without the factor 2 and therefore are twice as small as the values given by the formulas above. This factor is however included automatically by all plotting script supplied within the Combine package. Simple models For relatively simple models, the observed and expected limits can be calculated interactively. Since the LHC-style is the reccomended procedure for calculating limits using toys, we will use that in this section but the same applies to the other methods. combine realistic-counting-experiment.txt -M HybridNew --LHCmode LHC-limits Show output < < < Combine >>> >>> including systematics >>> using the Profile Likelihood test statistics modified for upper limits (Q_LHC) >>> method used is HybridNew >>> random number generator seed is 123456 Computing results starting from observation (a-posteriori) Search for upper limit to the limit r = 20 +/- 0 CLs = 0 +/- 0 CLs = 0 +/- 0 CLb = 0.264 +/- 0.0394263 CLsplusb = 0 +/- 0 Search for lower limit to the limit Now doing proper bracketing & bisection r = 10 +/- 10 CLs = 0 +/- 0 CLs = 0 +/- 0 CLb = 0.288 +/- 0.0405024 CLsplusb = 0 +/- 0 r = 5 +/- 5 CLs = 0 +/- 0 CLs = 0 +/- 0 CLb = 0.152 +/- 0.0321118 CLsplusb = 0 +/- 0 r = 2.5 +/- 2.5 CLs = 0.0192308 +/- 0.0139799 CLs = 0.02008 +/- 0.0103371 CLs = 0.0271712 +/- 0.00999051 CLs = 0.0239524 +/- 0.00783634 CLs = 0.0239524 +/- 0.00783634 CLb = 0.208748 +/- 0.0181211 CLsplusb = 0.005 +/- 0.00157718 r = 2.00696 +/- 1.25 CLs = 0.0740741 +/- 0.0288829 CLs = 0.0730182 +/- 0.0200897 CLs = 0.0694474 +/- 0.0166468 CLs = 0.0640182 +/- 0.0131693 CLs = 0.0595 +/- 0.010864 CLs = 0.0650862 +/- 0.0105575 CLs = 0.0629286 +/- 0.00966301 CLs = 0.0634945 +/- 0.00914091 CLs = 0.060914 +/- 0.00852667 CLs = 0.06295 +/- 0.00830083 CLs = 0.0612758 +/- 0.00778181 CLs = 0.0608142 +/- 0.00747001 CLs = 0.0587169 +/- 0.00697039 CLs = 0.0591432 +/- 0.00678587 CLs = 0.0599683 +/- 0.00666966 CLs = 0.0574868 +/- 0.00630809 CLs = 0.0571451 +/- 0.00608177 CLs = 0.0553836 +/- 0.00585531 CLs = 0.0531612 +/- 0.0055234 CLs = 0.0516837 +/- 0.0052607 CLs = 0.0496776 +/- 0.00499783 CLs = 0.0496776 +/- 0.00499783 CLb = 0.216635 +/- 0.00801002 CLsplusb = 0.0107619 +/- 0.00100693 Trying to move the interval edges closer r = 1.00348 +/- 0 CLs = 0.191176 +/- 0.0459911 CLs = 0.191176 +/- 0.0459911 CLb = 0.272 +/- 0.0398011 CLsplusb = 0.052 +/- 0.00992935 r = 1.50522 +/- 0 CLs = 0.125 +/- 0.0444346 CLs = 0.09538 +/- 0.0248075 CLs = 0.107714 +/- 0.0226712 CLs = 0.103711 +/- 0.018789 CLs = 0.0845069 +/- 0.0142341 CLs = 0.0828468 +/- 0.0126789 CLs = 0.0879647 +/- 0.0122332 CLs = 0.0879647 +/- 0.0122332 CLb = 0.211124 +/- 0.0137494 CLsplusb = 0.0185714 +/- 0.00228201 r = 1.75609 +/- 0 CLs = 0.0703125 +/- 0.0255807 CLs = 0.0595593 +/- 0.0171995 CLs = 0.0555271 +/- 0.0137075 CLs = 0.0548727 +/- 0.0120557 CLs = 0.0527832 +/- 0.0103348 CLs = 0.0555828 +/- 0.00998248 CLs = 0.0567971 +/- 0.00923449 CLs = 0.0581822 +/- 0.00871417 CLs = 0.0588835 +/- 0.00836245 CLs = 0.0594035 +/- 0.00784761 CLs = 0.0590583 +/- 0.00752672 CLs = 0.0552067 +/- 0.00695542 CLs = 0.0560446 +/- 0.00679746 CLs = 0.0548083 +/- 0.0064351 CLs = 0.0566998 +/- 0.00627124 CLs = 0.0561576 +/- 0.00601888 CLs = 0.0551643 +/- 0.00576338 CLs = 0.0583584 +/- 0.00582854 CLs = 0.0585691 +/- 0.0057078 CLs = 0.0599114 +/- 0.00564585 CLs = 0.061987 +/- 0.00566905 CLs = 0.061836 +/- 0.00549856 CLs = 0.0616849 +/- 0.0053773 CLs = 0.0605352 +/- 0.00516844 CLs = 0.0602028 +/- 0.00502875 CLs = 0.058667 +/- 0.00486263 CLs = 0.058667 +/- 0.00486263 CLb = 0.222901 +/- 0.00727258 CLsplusb = 0.0130769 +/- 0.000996375 r = 2.25348 +/- 0 CLs = 0.0192308 +/- 0.0139799 CLs = 0.0173103 +/- 0.00886481 CLs = 0.0173103 +/- 0.00886481 CLb = 0.231076 +/- 0.0266062 CLsplusb = 0.004 +/- 0.001996 r = 2.13022 +/- 0 CLs = 0.0441176 +/- 0.0190309 CLs = 0.0557778 +/- 0.01736 CLs = 0.0496461 +/- 0.0132776 CLs = 0.0479048 +/- 0.0114407 CLs = 0.0419333 +/- 0.00925719 CLs = 0.0367934 +/- 0.0077345 CLs = 0.0339814 +/- 0.00684844 CLs = 0.03438 +/- 0.0064704 CLs = 0.0337633 +/- 0.00597315 CLs = 0.0321262 +/- 0.00551608 CLs = 0.0321262 +/- 0.00551608 CLb = 0.230342 +/- 0.0118665 CLsplusb = 0.0074 +/- 0.00121204 r = 2.06859 +/- 0 CLs = 0.0357143 +/- 0.0217521 CLs = 0.0381957 +/- 0.0152597 CLs = 0.0368622 +/- 0.0117105 CLs = 0.0415097 +/- 0.0106676 CLs = 0.0442816 +/- 0.0100457 CLs = 0.0376644 +/- 0.00847235 CLs = 0.0395133 +/- 0.0080427 CLs = 0.0377625 +/- 0.00727262 CLs = 0.0364415 +/- 0.00667827 CLs = 0.0368015 +/- 0.00628517 CLs = 0.0357251 +/- 0.00586442 CLs = 0.0341604 +/- 0.00546373 CLs = 0.0361935 +/- 0.00549648 CLs = 0.0403254 +/- 0.00565172 CLs = 0.0408613 +/- 0.00554124 CLs = 0.0416682 +/- 0.00539651 CLs = 0.0432645 +/- 0.00538062 CLs = 0.0435229 +/- 0.00516945 CLs = 0.0427647 +/- 0.00501322 CLs = 0.0414894 +/- 0.00479711 CLs = 0.0414894 +/- 0.00479711 CLb = 0.202461 +/- 0.00800632 CLsplusb = 0.0084 +/- 0.000912658 -- HybridNew, before fit -- Limit: r < 2.00696 +/- 1.25 [1.50522, 2.13022] Warning in : Could not create the Migrad minimizer. Try using the minimizer Minuit Fit to 5 points: 1.91034 +/- 0.0388334 -- Hybrid New -- Limit: r < 1.91034 +/- 0.0388334 @ 95% CL Done in 0.01 min (cpu), 4.09 min (real) Failed to delete temporary file roostats-Sprxsw.root: No such file or directory The result stored in the limit branch of the output tree will be the upper limit (and its error stored in limitErr ). The default behavior will be, as above, to search for the upper limit on r however, the values of p_{\\mu}, p_{b} p_{\\mu}, p_{b} and CL s can be calculated for a particular value r=X by specifying the option --singlePoint=X . In this case, the value stored in the branch limit will be the value of CL s (or p_{\\mu} p_{\\mu} ) (see the FAQ section). Expected Limits For the simple models, we can just run interactively 5 times to compute the median expected and the 68% and 95% interval boundaries. Use the HybridNew method with the same options as per the observed limit but adding a --expectedFromGrid=<quantile> where the quantile is 0.5 for the median, 0.84 for the +ve side of the 68% band, 0.16 for the -ve side of the 68% band, 0.975 for the +ve side of the 95% band, 0.025 for the -ve side of the 95% band. The output file will contain the value of the quantile in the branch quantileExpected which can be used to separate the points. Accuracy The search for the limit is performed using an adaptive algorithm, terminating when the estimate of the limit value is below some limit or when the precision cannot be futher improved with the specified options. The options controlling this behaviour are: rAbsAcc , rRelAcc : define the accuracy on the limit at which the search stops. The default values are 0.1 and 0.05 respectively, meaning that the search is stopped when \u0394r < 0.1 or \u0394r/r < 0.05. clsAcc : this determines the absolute accuracy up to which the CLs values are computed when searching for the limit. The default is 0.5%. Raising the accuracy above this value will increase significantly the time to run the algorithm, as you need N 2 more toys to improve the accuracy by a factor N, you can consider enlarging this value if you're computing limits with a larger CL (e.g. 90% or 68%). Note that if you're using the CLsplusb rule then this parameter will control the uncertainty on p_{\\mu} p_{\\mu} rather than CL s . T or toysH : controls the minimum number of toys that are generated for each point. The default value of 500 should be ok when computing the limit with 90-95% CL. You can decrease this number if you're computing limits at 68% CL, or increase it if you're using 99% CL. Note, to further improve the accuracy when searching for the upper limit, combine will also fit an exponential function to several of the points and interpolate to find the crossing. Complex models For complicated models, it is best to produce a grid of test statistic distributions at various values of the signal strength, and use it to compute the observed and expected limit and bands. This approach is good for complex models since the grid of points can be distributed across any number of jobs. In this approach we will store the distributions of the test-statistic at different values of the signal strength using the option --saveHybridResult . The distribution at a single value of r=X can be determined by combine datacard.txt -M HybridNew --LHCmode LHC-limits --singlePoint X --saveToys --saveHybridResult -T 500 --clsAcc 0 Warning We have specified the accuracy here by including clsAcc=0 which turns off adaptive sampling and specifying the number of toys to be 500 with the -T N option. For complex models, it may be necessary to split the toys internally over a number of instances of HybridNew using the option --iterations I . The total number of toys will be the product I*N . The above can be repeated several times, in parallel, to build the distribution of the test-statistic (giving the random seed option -s -1 ). Once all of the distributions are finished, the resulting output files can be merged into one using hadd and read back to calculate the limit, specifying the merged file with --grid=merged.root . The observed limit can be obtained with combine datacard.txt -M HybridNew --LHCmode LHC-limits --readHybridResults --grid=merged.root and similarly, the median expected and quantiles can be determined using combine datacard.txt -M HybridNew --LHCmode LHC-limits --readHybridResults --grid=merged.root --quantileExpected <quantile> substituting <quantile> with 0.5 for the median, 0.84 for the +ve side of the 68% band, 0.16 for the -ve side of the 68% band, 0.975 for the +ve side of the 95% band, 0.025 for the -ve side of the 95% band. The splitting of the jobs can be left to the user's preference. However, users may wish to use the combineTool for automating this as described in the section on combineTool for job submission Plotting A plot of the CL s (or p_{\\mu} p_{\\mu} ) as a function of r , which is used to find the crossing, can be produced using the option --plot=limit_scan.png . This can be useful for judging if the grid was sufficient in determining the upper limit. If we use our realistic-counting-experiment.txt datacard and generate a grid of points r\\varepsilon[1.4,2.2] r\\varepsilon[1.4,2.2] in steps of 0.1, with 5000 toys for each point, the plot of the observed CL s vs r should look like the following, You should judge in each case if the limit is accurate given the spacing of the points and the precision of CL s at each point. If it is not sufficient, simply generate more points closer to the limit and/or more toys at each point. The distributions of the test-statistic can also be plotted, at each value in the grid, using the simple python tool, python test/plotTestStatCLs.py --input mygrid.root --poi r --val all --mass MASS The resulting output file will contain a canvas showing the distribution of the test statistic background only and signal+background hypothesis at each value of r . Info If you used the TEV or LEP style test statistic (using the commands as described above), then you should include the option --doublesided , which will also take care of defining the correct integrals for p_{\\mu} p_{\\mu} and p_{b} p_{b} . Computing Significances with toys Computation of expected significance with toys is a two step procedure: first you need to run one or more jobs to construct the expected distribution of the test statistic. As with setting limits, there are a number of different configurations for generating toys but we will use the preferred option using, LHC-style : --LHCmode LHC-significance , which is the shortcut for --testStat LHC --generateNuisances=0 --generateExternalMeasurements=1 --fitNuisances=1 --significance The test statistic is defined using the ratio of likelihoods q_{0} = -2\\ln[\\mathcal{L}(\\textrm{data}|r=0,\\hat{\\theta}_{0})/\\mathcal{L}(\\textrm{data}|r=\\hat{r},\\hat{\\theta})] q_{0} = -2\\ln[\\mathcal{L}(\\textrm{data}|r=0,\\hat{\\theta}_{0})/\\mathcal{L}(\\textrm{data}|r=\\hat{r},\\hat{\\theta})] , in which the nuisance parameters are profiled separately for r=\\hat{r} r=\\hat{r} and r=0 r=0 . The value of the test statistic is set to 0 when \\hat{r}<0 \\hat{r}<0 For the purposes of toy generation, the nuisance parameters are fixed to their post-fit values from the data assuming no signal, while the constraint terms are randomized for the evaluation of the likelihood. Observed significance To construct the distribution of the test statistic run as many times as necessary, combine -M HybridNew datacard.txt --LHCmode LHC-significance --saveToys --fullBToys --saveHybridResult -T toys -i iterations -s seed with different seeds, or using -s -1 for random seeds, then merge all those results into a single root file with hadd . The observed significance can be calculated as combine -M HybridNew datacard.txt --LHCmode LHC-significance --readHybridResult --grid=input.root [--pvalue ] where the option --pvalue will replace the result stored in the limit branch output tree to be the p-value instead of the signficance. Expected significance, assuming some signal The expected significance, assuming a signal with r=X can be calculated, by including the option --expectSignal X when generating the distribution of the test statistic and using the option --expectedFromGrid=0.5 when calculating the significance for the median. To get the \u00b11\u03c3 bands, use 0.16 and 0.84 instead of 0.5, and so on... You need a total number of background toys large enough to compute the value of the significance, but you need less signal toys (especially if you only need the median). For large significance, you can then run most of the toys without the --fullBToys option (about a factor 2 faster), and only a smaller part with that option turned on. As with calculating limits with toys, these jobs can be submitted to the grid or batch systems with the help of the combineTool as described in the section on combineTool for job submission Goodness of fit tests The GoodnessOfFit method can be used to evaluate how compatible the observed data are with the model pdf. The module can be run specifying an algorithm, and will compute a goodness of fit indicator for that algorithm and the data. The procedure is therefore to first run on the real data combine -M GoodnessOfFit datacard.txt --algo=<some-algo> and then to run on many toy mc datasets to determine the distribution of the goodness of fit indicator combine -M GoodnessOfFit datacard.txt --algo=<some-algo> -t <number-of-toys> -s <seed> When computing the goodness of fit, by default the signal strength is left floating in the fit, so that the measure is independent from the presence or absence of a signal. It is possible to instead keep it fixed to some value by passing the option --fixedSignalStrength=<value> . The following algorithms are supported: saturated : Compute a goodness-of-fit measure for binned fits based on the saturated model method, as prescribed by the StatisticsCommittee (note) . This quantity is similar to a chi-square, but can be computed for an arbitrary combination of binned channels with arbitrary constraints. KS : Compute a goodness-of-fit measure for binned fits using the Kolmogorov-Smirnov test. It is based on the highest difference between the cumulative distribution function and the empirical distribution function of any bin. AD : Compute a goodness-of-fit measure for binned fits using the Anderson-Darling test. It is based on the integral of the difference between the cumulative distribution function and the empirical distribution function over all bins. It also gives the tail ends of the distribution a higher weighting. The output tree will contain a branch called limit which contains the value of the test-statistic in each toy. You can make a histogram of this test-statistic t t and from this distribution ( f(t) f(t) ) and the single value obtained in the data ( t_{0} t_{0} ) you can calculate the p-value $$p = \\int_{t=t_{0}}^{\\mathrm{+inf}} f(t) dt $$. When generating toys, the default behavior will be used. See the section on toy generation for options on how to generate/fit nuisance parameters in these tests. It is recomended to use the frequentist toys ( --toysFreq ) when running the saturated model, and the default toys for the other two tests. Further goodness of fit methods could be added on request, especially if volunteers are available to code them. The output limit tree will contain the value of the test-statistic in each toy (or the data) Warning The above algorithms are all concerned with one-sample tests. For two-sample tests, you can follow an example CMS HIN analysis described in this Twiki Masking analysis regions in the saturated model For searches that employs a simultaneous fit across signal and control regions, it may be useful to mask one or more analysis regions either when the likelihood is maximized (fit) or when the test-statistic is computed. This can be done by using the options --setParametersForFit and --setParametersForEval , respectively. The former will set parameters before each fit while the latter is used to set parameters after each fit, but before the NLL is evauated. Note of course that if the parameter in the list is floating, it will be still floating in each fit so will not effect the results when using --setParametersForFit . A realistic example for a binned shape analysis performed in one signal region and two control samples can be found in this directory of the Higgs-combine package Datacards-shape-analysis-multiple-regions . First of all, one needs to combine the individual datacards to build a single model and to introduce the channel-masking variables as follow: combineCards.py signal_region.txt dimuon_control_region.txt singlemuon_control_region.txt > combined_card.txt text2workspace.py combined_card.txt --channel-masks More information about the channel-masking can be found in this section Channel Masking . The saturated test-static value for a simultaneous fit across all the analysis regions can be calculated as: combine -M GoodnessOfFit -d combined_card.root --algo=saturated -n _result_sb In this case, signal and control regions are included in both the fit and in the evaluation of the test-static, and the signal strength is freely floating. This measures the compatibility between the signal+background fit and the observed data. Moreover, it can be interesting to assess the level of compatibility between the observed data in all the regions and the background prediction obtained by only fitting the control regions (CR-only fit). This is computed as follow: combine -M GoodnessOfFit -d combined_card.root --algo=saturated -n _result_bonly_CRonly --setParametersForFit mask_ch1=1 --setParametersForEval mask_ch1=0 --freezeParameters r --setParameters r=0 where the signal strength is frozen and the signal region is not considered in the fit ( --setParametersForFit mask_ch1=1 ), but it is included in the test-statistic computation ( --setParametersForEval mask_ch1=0 ). To show the differences between the two models being tested, one can perform a fit to the data using the FitDiagnostics method as: combine -M FitDiagnostics -d combined_card.root -n _fit_result --saveShapes --saveWithUncertainties combine -M FitDiagnostics -d combined_card.root -n _fit_CRonly_result --saveShapes --saveWithUncertainties --setParameters mask_ch1=1 By taking the total background, the total signal, and the data shapes from FitDiagnostics output, we can compare the post-fit predictions from the S+B fit (first case) and the CR-only fit (second case) with the observation as reported below: FitDiagnostics S+B fit FitDiagnostics CR-only fit To compute a p-value for the two results, one needs to compare the observed goodness-of-fit value previously computed with expected distribution of the test-statistic obtained in toys: combine -M GoodnessOfFit combined_card.root --algo=saturated -n result_toy_sb --toysFrequentist -t 500 combine -M GoodnessOfFit -d combined_card.root --algo=saturated -n _result_bonly_CRonly_toy --setParametersForFit mask_ch1=1 --setParametersForEval mask_ch1=0 --freezeParameters r --setParameters r=0,mask_ch1=1 -t 500 --toysFrequentist where the former gives the result for the S+B model, while the latter gives the test-statistic for CR-only fit. The command --setParameters r=0,mask_ch1=1 is needed to ensure that toys are thrown using the nuisance parameters estimated from the CR-only fit to the data. The comparison between the observation and the expected distribition should look like the following two plots: Goodness-of-fit for S+B model Goodness-of-fit for CR-only model Making a plot of the GoF test-statistic distribution If you have also checked out the combineTool , you can use this to run batch jobs or on the grid (see here ) and produce a plot of the results. Once you have the jobs, you can hadd them together and run (e.g for the saturated model), combineTool.py -M CollectGoodnessOfFit --input data_run.root toys_run.root -m 125.0 -o gof.json plotGof.py gof.json --statistic saturated --mass 125.0 -o gof_plot --title-right=\"my label\" Channel Compatibility The ChannelCompatibilityCheck method can be used to evaluate how compatible are the measurements of the signal strength from the separate channels of a combination. The method performs two fits of the data, first with the nominal model in which all channels are assumed to have the same signal strength multiplier r r , and then another allowing separate signal strengths r_{i} r_{i} in each channel. A chisquare-like quantity is computed as -2 \\ln \\mathcal{L}(\\mathrm{data}| r)/L(\\mathrm{data}|\\{r_{i}\\}_{i=1}^{N_{\\mathrm{chan}}}) -2 \\ln \\mathcal{L}(\\mathrm{data}| r)/L(\\mathrm{data}|\\{r_{i}\\}_{i=1}^{N_{\\mathrm{chan}}}) . Just like for the goodness of fit indicators, the expected distribution of this quantity under the nominal model can be computed from toy mc. By default, the signal strength is kept floating in the fit with the nominal model. It can however be fixed to a given value by passing the option --fixedSignalStrength=<value> . In the default models build from the datacards the signal strengths in all channels are constrained to be non-negative. One can allow negative signal strengths in the fits by changing the bound on the variable (option --rMin=<value> ), which should make the quantity more chisquare-like under the hypothesis of zero signal; this however can create issues in channels with small backgrounds, since total expected yields and pdfs in each channel must be positive. When run with the a verbosity of 1, as the default, the program also prints out the best fit signal strengths in all channels; as the fit to all channels is done simultaneously, the correlation between the other systematical uncertainties is taken into account, and so these results can differ from the ones obtained fitting each channel separately. Below is an example output from combine, $ combine -M ChannelCompatibilityCheck comb_hww.txt -m 160 -n HWW <<< Combine >>> >>> including systematics >>> method used to compute upper limit is ChannelCompatibilityCheck >>> random number generator seed is 123456 Sanity checks on the model: OK Computing limit starting from observation --- ChannelCompatibilityCheck --- Nominal fit : r = 0.3431 -0.1408/+0.1636 Alternate fit: r = 0.4010 -0.2173/+0.2724 in channel hww_0jsf_shape Alternate fit: r = 0.2359 -0.1854/+0.2297 in channel hww_0jof_shape Alternate fit: r = 0.7669 -0.4105/+0.5380 in channel hww_1jsf_shape Alternate fit: r = 0.3170 -0.3121/+0.3837 in channel hww_1jof_shape Alternate fit: r = 0.0000 -0.0000/+0.5129 in channel hww_2j_cut Chi2-like compatibility variable: 2.16098 Done in 0.08 min (cpu), 0.08 min (real) The output tree will contain the value of the compatibility (chisquare variable) in the limit branch. If the option --saveFitResult is specified, the output root file contains also two RooFitResult objects fit_nominal and fit_alternate with the results of the two fits. This can be read and used to extract the best fit for each channel and the overall best fit using $ root -l TFile* _file0 = TFile::Open(\"higgsCombineTest.ChannelCompatibilityCheck.mH120.root\"); fit_alternate->floatParsFinal().selectByName(\"*ChannelCompatibilityCheck*\")->Print(\"v\"); fit_nominal->floatParsFinal().selectByName(\"r\")->Print(\"v\"); The macro cccPlot.cxx can be used to produce a comparison plot of the best fit signals from all channels. Likelihood Fits and Scans The MultiDimFit method can do multi-dimensional fits and likelihood based scans/contours using models with several parameters of interest. Taking a toy datacard test/multiDim/toy-hgg-125.txt (counting experiment which vaguely resembles the H\u2192\u03b3\u03b3 analysis at 125 GeV), we need to convert the datacard into a workspace with 2 parameters, ggH and qqH cross sections text2workspace.py toy-hgg-125.txt -m 125 -P HiggsAnalysis.CombinedLimit.PhysicsModel:floatingXSHiggs --PO modes=ggH,qqH A number of different algorithms can be used with the option --algo <algo> , none (default): Perform a maximum likelihood fit combine -M MultiDimFit toy-hgg-125.root ; The output root tree will contain two columns, one for each parameter, with the fitted values. singles : Perform a fit of each parameter separately, treating the others as unconstrained nuisances : combine -M MultiDimFit toy-hgg-125.root --algo singles --cl=0.68 . The output root tree will contain two columns, one for each parameter, with the fitted values; there will be one row with the best fit point (and quantileExpected set to -1) and two rows for each fitted parameter, where the corresponding column will contain the maximum and minimum of that parameter in the 68% CL interval, according to a one-dimensional chisquare (i.e. uncertainties on each fitted parameter do not increase when adding other parameters if they're uncorrelated). Note that if you run, for example, with --cminDefaultMinimizerStrategy=0 , these uncertainties will be derived from the Hessian, while --cminDefaultMinimizerStrategy=1 will invoke Minos to derive them. cross : Perform joint fit of all parameters: combine -M MultiDimFit toy-hgg-125.root --algo=cross --cl=0.68 . The output root tree will have one row with the best fit point, and two rows for each parameter, corresponding to the minimum and maximum of that parameter on the likelihood contour corresponding to the specified CL, according to a N-dimensional chisquare (i.e. uncertainties on each fitted parameter do increase when adding other parameters, even if they're uncorrelated). Note that the output of this way of running are not 1D uncertainties on each parameter, and shouldn't be taken as such. contour2d : Make a 68% CL contour a la minos combine -M MultiDimFit toy-hgg-125.root --algo contour2d --points=20 --cl=0.68 . The output will contain values corresponding to the best fit point (with quantileExpected set to -1) and for a set of points on the contour (with quantileExpected set to 1-CL, or something larger than that if the contour is hitting the boundary of the parameters). Probabilities are computed from the the n-dimensional \\chi^{2} \\chi^{2} distribution. For slow models, you can split it up by running several times with different number of points and merge the outputs (something better can be implemented). You can look at the contourPlot.cxx macro for how to make plots out of this algorithm. random : Scan N random points and compute the probability out of the profile likelihood combine -M MultiDimFit toy-hgg-125.root --algo random --points=20 --cl=0.68 . Again, best fit will have quantileExpected set to -1, while each random point will have quantileExpected set to the probability given by the profile likelihood at that point. fixed : Compare the log-likelihood at a fixed point compared to the best fit. combine -M MultiDimFit toy-hgg-125.root --algo fixed --fixedPointPOIs r=r_fixed,MH=MH_fixed . The output tree will contain the difference in the negative log-likelihood between the points ( \\hat{r},\\hat{m}_{H} \\hat{r},\\hat{m}_{H} ) and ( \\hat{r}_{fixed},\\hat{m}_{H,fixed} \\hat{r}_{fixed},\\hat{m}_{H,fixed} ) in the branch deltaNLL . grid : Scan on a fixed grid of points not with approximately N points in total. combine -M MultiDimFit toy-hgg-125.root --algo grid --points=10000 . You can partition the job in multiple tasks by using options --firstPoint and --lastPoint , for complicated scans, the points can be split as described in the combineTool for job submission section. The output file will contain a column deltaNLL with the difference in negative log likelihood with respect to the best fit point. Ranges/contours can be evaluated by filling TGraphs or TH2 histograms with these points. By default the \"min\" and \"max\" of the POI ranges are not included and the points which are in the scan are centered , eg combine -M MultiDimFit --algo grid --rMin 0 --rMax 5 --points 5 will scan at the points r=0.5, 1.5, 2.5, 3.5, 4.5 r=0.5, 1.5, 2.5, 3.5, 4.5 . You can instead include the option --alignEdges 1 which causes the points to be aligned with the endpoints of the parameter ranges - eg combine -M MultiDimFit --algo grid --rMin 0 --rMax 5 --points 6 --alignEdges 1 will now scan at the points r=0, 1, 2, 3, 4, 5 r=0, 1, 2, 3, 4, 5 . NB - the number of points must be increased by 1 to ensure both end points are included. With the algorithms none and singles you can save the RooFitResult from the initial fit using the option --saveFitResult . The fit result is saved into a new file called muiltidimfit.root . As usual, any floating nuisance parameters will be profiled which can be turned of using the --freezeParameters option. For most of the methods, for lower precision results you can turn off the profiling of the nuisances setting option --fastScan , which for complex models speeds up the process by several orders of magnitude. All nuisance parameters will be kept fixed at the value corresponding to the best fit point. As an example, lets produce the -2\\Delta\\ln{\\mathcal{L}} -2\\Delta\\ln{\\mathcal{L}} scan as a function of r_ggH and r_qqH from the toy H\u2192\u03b3\u03b3 datacard, with the nuisance parameters fixed to their global best fit values. combine toy-hgg-125.root -M MultiDimFit --algo grid --points 2000 --setParameterRanges r_qqH=0,10:r_ggH=0,4 -m 125 --fastScan Show output < < < Combine >>> >>> including systematics >>> method used is MultiDimFit >>> random number generator seed is 123456 ModelConfig 'ModelConfig' defines more than one parameter of interest. This is not supported in some statistical methods. Set Range of Parameter r_qqH To : (0,10) Set Range of Parameter r_ggH To : (0,4) Computing results starting from observation (a-posteriori) POI: r_ggH= 0.88152 -> [0,4] POI: r_qqH= 4.68297 -> [0,10] Point 0/2025, (i,j) = (0,0), r_ggH = 0.044444, r_qqH = 0.111111 Point 11/2025, (i,j) = (0,11), r_ggH = 0.044444, r_qqH = 2.555556 Point 22/2025, (i,j) = (0,22), r_ggH = 0.044444, r_qqH = 5.000000 Point 33/2025, (i,j) = (0,33), r_ggH = 0.044444, r_qqH = 7.444444 Point 55/2025, (i,j) = (1,10), r_ggH = 0.133333, r_qqH = 2.333333 Point 66/2025, (i,j) = (1,21), r_ggH = 0.133333, r_qqH = 4.777778 Point 77/2025, (i,j) = (1,32), r_ggH = 0.133333, r_qqH = 7.222222 Point 88/2025, (i,j) = (1,43), r_ggH = 0.133333, r_qqH = 9.666667 Point 99/2025, (i,j) = (2,9), r_ggH = 0.222222, r_qqH = 2.111111 Point 110/2025, (i,j) = (2,20), r_ggH = 0.222222, r_qqH = 4.555556 Point 121/2025, (i,j) = (2,31), r_ggH = 0.222222, r_qqH = 7.000000 Point 132/2025, (i,j) = (2,42), r_ggH = 0.222222, r_qqH = 9.444444 Point 143/2025, (i,j) = (3,8), r_ggH = 0.311111, r_qqH = 1.888889 Point 154/2025, (i,j) = (3,19), r_ggH = 0.311111, r_qqH = 4.333333 Point 165/2025, (i,j) = (3,30), r_ggH = 0.311111, r_qqH = 6.777778 Point 176/2025, (i,j) = (3,41), r_ggH = 0.311111, r_qqH = 9.222222 Point 187/2025, (i,j) = (4,7), r_ggH = 0.400000, r_qqH = 1.666667 Point 198/2025, (i,j) = (4,18), r_ggH = 0.400000, r_qqH = 4.111111 Point 209/2025, (i,j) = (4,29), r_ggH = 0.400000, r_qqH = 6.555556 Point 220/2025, (i,j) = (4,40), r_ggH = 0.400000, r_qqH = 9.000000 [...] Done in 0.00 min (cpu), 0.02 min (real) The scan, along with the best fit point can be drawn using root, $ root -l higgsCombineTest.MultiDimFit.mH125.root limit->Draw(\"2*deltaNLL:r_ggH:r_qqH>>h(44,0,10,44,0,4)\",\"2*deltaNLL<10\",\"prof colz\") limit->Draw(\"r_ggH:r_qqH\",\"quantileExpected == -1\",\"P same\") TGraph *best_fit = (TGraph*)gROOT->FindObject(\"Graph\") best_fit->SetMarkerSize(3); best_fit->SetMarkerStyle(34); best_fit->Draw(\"p same\") To make the full profiled scan just remove the --fastScan option from the combine command. Similarly, 1D scans can be drawn directly from the tree, however for 1D likelihood scans, there is a python script from the CombineHarvester/CombineTools package plot1DScan.py which can be used to make plots and extract the crossings of the 2*deltaNLL - e.g the 1\u03c3/2\u03c3 boundaries. Useful options for likelihood scans A number of common, useful options (especially for computing likelihood scans with the grid algo) are, --autoBoundsPOIs arg : Adjust bounds for the POIs if they end up close to the boundary. This can be a comma separated list of POIs, or \"*\" to get all of them. --autoMaxPOIs arg : Adjust maxima for the POIs if they end up close to the boundary. Can be a list of POIs, or \"*\" to get all. --autoRange X : Set to any X >= 0 to do the scan in the \\hat{p} \\hat{p} \\pm \\pm X\u03c3 range, where \\hat{p} \\hat{p} and \u03c3 are the best fit parameter value and uncertainty from the initial fit (so it may be fairly approximate). In case you do not trust the estimate of the error from the initial fit, you can just centre the range on the best fit value by using the option --centeredRange X to do the scan in the \\hat{p} \\hat{p} \\pm \\pm X range centered on the best fit value. --squareDistPoiStep : POI step size based on distance from midpoint ( either (max-min)/2 or the best fit if used with --autoRange or --centeredRange ) rather than linear separation. --skipInitialFit : Skip the initial fit (saves time if for example a snapshot is loaded from a previous fit) Below is a comparison in a likelihood scan, with 20 points, as a function of r_qqH with our toy-hgg-125.root workspace with and without some of these options. The options added tell combine to scan more points closer to the minimum (best-fit) than with the default. You may find it useful to use the --robustFit=1 option to turn on robust (brute-force) for likelihood scans (and other algorithms). You can set the strategy and tolerance when using the --robustFit option using the options --setRobustFitAlgo (default is Minuit2,migrad ), setRobustFitStrategy (default is 0) and --setRobustFitTolerance (default is 0.1). If these options are not set, the defaults (set using cminDefaultMinimizerX options) will be used. If running --robustFit=1 with the algo singles , you can tune the accuracy of the routine used to find the crossing points of the likelihood using the option --setCrossingTolerance (default is set to 0.0001) If you suspect your fits/uncertainties are not stable, you may also try to run custom HESSE-style calculation of the covariance matrix. This is enabled by running MultiDimFit with the --robustHesse=1 option. A simple example of how the default behaviour in a simple datacard is given here . For a full list of options use combine -M MultiDimFit --help Fitting only some parameters If your model contains more than one parameter of interest, you can still decide to fit a smaller number of them, using the option --parameters (or -P ), with a syntax like this: combine -M MultiDimFit [...] -P poi1 -P poi2 ... --floatOtherPOIs=(0|1) If --floatOtherPOIs is set to 0, the other parameters of interest (POIs), which are not included as a -P option, are kept fixed to their nominal values. If it's set to 1, they are kept floating , which has different consequences depending on algo : When running with --algo=singles , the other floating POIs are treated as unconstrained nuisance parameters. When running with --algo=cross or --algo=contour2d , the other floating POIs are treated as other POIs, and so they increase the number of dimensions of the chi-square. As a result, when running with floatOtherPOIs set to 1, the uncertainties on each fitted parameters do not depend on what's the selection of POIs passed to MultiDimFit, but only on the number of parameters of the model. Info Note that poi given to the the option -P can also be any nuisance parameter. However, by default, the other nuisance parameters are left floating , so you do not need to specify that. You can save the values of the other parameters of interest in the output tree by adding the option saveInactivePOI=1 . You can additionally save the post-fit values any nuisance parameter, function or discrete index (RooCategory) defined in the workspace using the following options; --saveSpecifiedNuis=arg1,arg2,... will store the fitted value of any specified constrained nuisance parameter. Use all to save every constrained nuisance parameter. Note that if you want to store the values of flatParams (or floating parameters which are not defined in the datacard) or rateParams , which are unconstrained , you should instead use the generic option --trackParameters as described here . --saveSpecifiedFunc=arg1,arg2,... will store the value of any function (eg RooFormulaVar ) in the model. --saveSpecifiedIndex=arg1,arg2,... will store the index of any RooCategory object - eg a discrete nuisance. Using best fit snapshots This can be used to save time when performing scans so that the best-fit needs not be redone and can also be used to perform scans with some nuisances frozen to the best-fit values. Sometimes it is useful to scan freezing certain nuisances to their best-fit values as opposed to the default values. To do this here is an example, Create a workspace workspace for a floating r,m_{H} r,m_{H} fit text2workspace.py hgg_datacard_mva_8TeV_bernsteins.txt -m 125 -P HiggsAnalysis.CombinedLimit.PhysicsModel:floatingHiggsMass --PO higgsMassRange=120,130 -o testmass.root` Perfom the fit, saving the workspace combine -m 123 -M MultiDimFit --saveWorkspace -n teststep1 testmass.root --verbose 9 Now we can load the best-fit \\hat{r},\\hat{m}_{H} \\hat{r},\\hat{m}_{H} and fit for r r freezing m_{H} m_{H} and lumi_8TeV to the best-fit values, combine -m 123 -M MultiDimFit -d higgsCombineteststep1.MultiDimFit.mH123.root -w w --snapshotName \"MultiDimFit\" -n teststep2 --verbose 9 --freezeParameters MH,lumi_8TeV Feldman Cousins The Feldman-Cousins (FC) procedure for computing confidence intervals for a generic model is, use the profile likelihood as the test-statistic q(x) = - 2 \\ln \\mathcal{L}(\\mathrm{data}|x,\\hat{\\theta}_{x})/\\mathcal{L}(\\mathrm{data}|\\hat{x},\\hat{\\theta}) q(x) = - 2 \\ln \\mathcal{L}(\\mathrm{data}|x,\\hat{\\theta}_{x})/\\mathcal{L}(\\mathrm{data}|\\hat{x},\\hat{\\theta}) where x x is a point in the (N-dimensional) parameter space, and \\hat{x} \\hat{x} is the point corresponding to the best fit. In this test-statistic, the nuisance parameters are profiled, separately both in the numerator and denominator. for each point x x : compute the observed test statistic q_{\\mathrm{obs}}(x) q_{\\mathrm{obs}}(x) compute the expected distribution of q(x) q(x) under the hypothesis of x x as the true value. accept the point in the region if p_{x}=P\\left[q(x) > q_{\\mathrm{obs}}(x)| x\\right] > \\alpha p_{x}=P\\left[q(x) > q_{\\mathrm{obs}}(x)| x\\right] > \\alpha With a critical value \\alpha \\alpha . In combine , you can perform this test on each individual point ( param1, param2,... ) = ( value1,value2,... ) by doing, combine workspace.root -M HybridNew --LHCmode LHC-feldman-cousins --clsAcc 0 --singlePoint param1=value1,param2=value2,param3=value3,... --saveHybridResult [Other options for toys, iterations etc as with limits] The point belongs to your confidence region if p_{x} p_{x} is larger than \\alpha \\alpha (e.g. 0.3173 for a 1\u03c3 region, 1-\\alpha=0.6827 1-\\alpha=0.6827 ). Warning You should not use this method without the option --singlePoint . Although combine will not complain, the algorithm to find the crossing will only find a single crossing and therefore not find the correct interval. Instead you should calculate the Feldman-Cousins intervals as described above. Physical boundaries Imposing physical boundaries (such as requiring \\mu>0 \\mu>0 for a signal strength) is achieved by setting the ranges of the physics model parameters using --setParameterRanges param1=param1_min,param1_max:param2=param2_min,param2_max .... The boundary is imposed by restricting the parameter range(s) to those set by the user, in the fits. Note that this is a trick! The actual fitted value, as one of an ensemble of outcomes, can fall outside of the allowed region, while the boundary should be imposed on the physical parameter. The effect of restricting the parameter value in the fit is such that the test-statistic is modified as follows ; q(x) = - 2 \\ln \\mathcal{L}(\\mathrm{data}|x,\\hat{\\theta}_{x})/\\mathcal{L}(\\mathrm{data}|\\hat{x},\\hat{\\theta}) q(x) = - 2 \\ln \\mathcal{L}(\\mathrm{data}|x,\\hat{\\theta}_{x})/\\mathcal{L}(\\mathrm{data}|\\hat{x},\\hat{\\theta}) , if \\hat{x} \\hat{x} in contained in the bounded range and, q(x) = - 2 \\ln \\mathcal{L}(\\mathrm{data}|x,\\hat{\\theta}_{x})/\\mathcal{L}(\\mathrm{data}|x_{B},\\hat{\\theta}_{B}) q(x) = - 2 \\ln \\mathcal{L}(\\mathrm{data}|x,\\hat{\\theta}_{x})/\\mathcal{L}(\\mathrm{data}|x_{B},\\hat{\\theta}_{B}) , if \\hat{x} \\hat{x} is outside of the bounded range. Here x_{B} x_{B} and \\hat{\\theta}_{B} \\hat{\\theta}_{B} are the values of x x and \\theta \\theta which maximise the likelihood excluding values outside of the bounded region for x x - typically, x_{B} x_{B} will be found at one of the boundaries which is imposed. For example, if the boundary x>0 x>0 is imposed, you will typically expect x_{B}=0 x_{B}=0 , when \\hat{x}\\leq 0 \\hat{x}\\leq 0 , and x_{B}=\\hat{x} x_{B}=\\hat{x} otherewise. This can sometimes be an issue as Minuit may not know if has successfully converged when the minimum lies outside of that range. If there is no upper/lower boundary, just set that value to something far from the region of interest. Info One can also imagine imposing the boundaries by first allowing Minuit to find the minimum in the un-restricted region and then setting the test-statistic to that above in the case that minimum lies outside the physical boundary. This would avoid potential issues of convergence - If you are interested in implementing this version in combine, please contact the development team. As in general for HybridNew , you can split the task into multiple tasks (grid and/or batch) and then merge the outputs, as described in the combineTool for job submission section. Extracting contours As in general for HybridNew , you can split the task into multiple tasks (grid and/or batch) and then merge the outputs with hadd . You can also refer to the combineTool for job submission section for submitting the jobs to the grid/batch. 1D intervals For one-dimensional models only, and if the parameter behaves like a cross-section, the code is somewhat able to do interpolation and determine the values of your parameter on the contour (just like it does for the limits). As with limits, read in the grid of points and extract 1D intervals using, combine workspace.root -M HybridNew --LHCmode LHC-feldman-cousins --readHybridResults --grid=mergedfile.root --cl <1-alpha> The output tree will contain the values of the POI which crosses the critical value ( \\alpha \\alpha ) - i.e, the boundaries of the confidence intervals, You can produce a plot of the value of p_{x} p_{x} vs the parameter of interest x x by adding the option --plot <plotname> . 2D contours There is a tool for extracting 2D contours from the output of HybridNew located in test/makeFCcontour.py provided the option --saveHybridResult was included when running HybridNew . It can be run with the usual combine output files (or several of them) as input, ./test/makeFCcontour.py toysfile1.root toysfile2.root .... [options] -out outputfile.root To extract 2D contours, the names of each parameter must be given --xvar poi_x --yvar poi_y . The output will be a root file containing a 2D histogram of value of p_{x,y} p_{x,y} for each point (x,y) (x,y) which can be used to draw 2D contours. There will also be a histogram containing the number of toys found for each point. There are several options for reducing the running time (such as setting limits on the region of interest or the minimum number of toys required for a point to be included) Finally, adding the option --storeToys in this python tool will add histograms for each point to the output file of the test-statistic distribution. This will increase the momory usage however as all of the toys will be stored in memory.","title":"Common statistical methods"},{"location":"part3/commonstatsmethods/#common-statistical-methods","text":"In this section, the most commonly used statistical methods from combine will be covered including specific instructions on how to obtain limits, significances and likelihood scans. For all of these methods, the assumed parameters of interest (POI) is the overall signal strength r (i.e the default PhysicsModel). In general however, the first POI in the list of POIs (as defined by the PhysicsModel) will be taken instead of r which may or may not make sense for a given method ... use your judgment! This section will assume that you are using the default model unless otherwise specified.","title":"Common Statistical Methods"},{"location":"part3/commonstatsmethods/#asymptotic-frequentist-limits","text":"The AsymptoticLimits method allows to compute quickly an estimate of the observed and expected limits, which is fairly accurate when the event yields are not too small and the systematic uncertainties don't play a major role in the result. The limit calculation relies on an asymptotic approximation of the distributions of the LHC test-statistic, which is based on a profile likelihood ratio, under signal and background hypotheses to compute two p-values p_{\\mu}, p_{b} p_{\\mu}, p_{b} and therefore CL_s=p_{\\mu}/(1-p_{b}) CL_s=p_{\\mu}/(1-p_{b}) (see the (see the FAQ section for a description of these) - i.e it is the asymptotic approximation of computing limits with frequentist toys. This method is so commonly used that it is the default method (i.e not specifying -M will run AsymptoticLimits ) A realistic example of datacard for a counting experiment can be found in the HiggsCombination package: data/tutorials/counting/realistic-counting-experiment.txt The method can be run using combine -M AsymptoticLimits realistic-counting-experiment.txt The program will print out the limit on the signal strength r (number of signal events / number of expected signal events) e .g. Observed Limit: r < 1.6297 @ 95% CL , the median expected limit Expected 50.0%: r < 2.3111 and edges of the 68% and 95% ranges for the expected limits. <<< Combine >>> >>> including systematics >>> method used to compute upper limit is AsymptoticLimits [...] -- AsymptoticLimits ( CLs ) -- Observed Limit: r < 1.6281 Expected 2.5%: r < 0.9640 Expected 16.0%: r < 1.4329 Expected 50.0%: r < 2.3281 Expected 84.0%: r < 3.9800 Expected 97.5%: r < 6.6194 Done in 0.01 min (cpu), 0.01 min (real) By default, the limits are calculated using the CL s prescription, as noted in the output, which takes the ratio of p-values under the signal plus background and background only hypothesis. This can be altered to using the strict p-value by using the option --rule CLsplusb (note that CLsplusb is the jargon for calculating the p-value p_{\\mu} p_{\\mu} ). You can also change the confidence level (default is 95%) to 90% using the option --cl 0.9 or any other confidence level. You can find the full list of options for AsymptoticLimits using --help -M AsymptoticLimits . Warning You may find that combine issues a warning that the best fit for the background-only Asimov dataset returns a non-zero value for the signal strength for example; WARNING: Best fit of asimov dataset is at r = 0.220944 (0.011047 times rMax), while it should be at zero If this happens, you should check to make sure that there are no issues with the datacard or the Asimov generation used for your setup. For details on debugging it is recommended that you follow the simple checks used by the HIG PAG here . The program will also create a rootfile higgsCombineTest.AsymptoticLimits.mH120.root containing a root tree limit that contains the limit values and other bookeeping information. The important columns are limit (the limit value) and quantileExpected (-1 for observed limit, 0.5 for median expected limit, 0.16/0.84 for the edges of the 65% interval band of expected limits, 0.025/0.975 for 95%). $ root -l higgsCombineTest.AsymptoticLimits.mH120.root root [0] limit->Scan(\"*\") ************************************************************************************************************************************ * Row * limit * limitErr * mh * syst * iToy * iSeed * iChannel * t_cpu * t_real * quantileE * ************************************************************************************************************************************ * 0 * 0.9639892 * 0 * 120 * 1 * 0 * 123456 * 0 * 0 * 0 * 0.0250000 * * 1 * 1.4329109 * 0 * 120 * 1 * 0 * 123456 * 0 * 0 * 0 * 0.1599999 * * 2 * 2.328125 * 0 * 120 * 1 * 0 * 123456 * 0 * 0 * 0 * 0.5 * * 3 * 3.9799661 * 0 * 120 * 1 * 0 * 123456 * 0 * 0 * 0 * 0.8399999 * * 4 * 6.6194028 * 0 * 120 * 1 * 0 * 123456 * 0 * 0 * 0 * 0.9750000 * * 5 * 1.6281188 * 0.0050568 * 120 * 1 * 0 * 123456 * 0 * 0.0035000 * 0.0055123 * -1 * ************************************************************************************************************************************","title":"Asymptotic Frequentist Limits"},{"location":"part3/commonstatsmethods/#blind-limits","text":"The AsymptoticLimits calculation follows the frequentist paradigm for calculating expected limits. This means that the routine will first fit the observed data, conditionally for a fixed value of r and set the nuisance parameters to the values obtained in the fit for generating the Asimov data, i.e it calculates the post-fit or a-posteriori expected limit. In order to use the pre-fit nuisance parameters (to calculate an a-priori limit), you must add the option --noFitAsimov or --bypassFrequentistFit . For blinding the results completely (i.e not using the data) you can include the option --run blind . Warning You should never use -t -1 to get blind limits!","title":"Blind limits"},{"location":"part3/commonstatsmethods/#splitting-points","text":"In case your model is particularly complex, you can perform the asymptotic calculation by determining the value of CL s for a set grid of points (in r ) and merging the results. This is done by using the option --singlePoint X for multiple values of X, hadding the output files and reading them back in, combine -M AsymptoticLimits realistic-counting-experiment.txt --singlePoint 0.1 -n 0.1 combine -M AsymptoticLimits realistic-counting-experiment.txt --singlePoint 0.2 -n 0.2 combine -M AsymptoticLimits realistic-counting-experiment.txt --singlePoint 0.3 -n 0.3 ... hadd limits.root higgsCombine*.AsymptoticLimits.* combine -M AsymptoticLimits realistic-counting-experiment.txt --getLimitFromGrid limits.root","title":"Splitting points"},{"location":"part3/commonstatsmethods/#asymptotic-significances","text":"The significance of a result is calculated using a ratio of profiled likelihoods, one in which the signal strength is set to 0 and the other in which it is free to float, i.e the quantity is -2\\ln[\\mathcal{L}(\\textrm{data}|r=0,\\hat{\\theta}_{0})/\\mathcal{L}(\\textrm{data}|r=\\hat{r},\\hat{\\theta})] -2\\ln[\\mathcal{L}(\\textrm{data}|r=0,\\hat{\\theta}_{0})/\\mathcal{L}(\\textrm{data}|r=\\hat{r},\\hat{\\theta})] , in which the nuisance parameters are profiled separately for r=\\hat{r} r=\\hat{r} and r=0 r=0 . The distribution of this test-statistic can be determined using Wilke's theorem provided the number of events is large enough (i.e in the Asymptotic limit ). The significance (or p-value) can therefore be calculated very quickly and uses the Significance method. It is also possible to calculate the ratio of likelihoods between the freely floating signal strength to that of a fixed signal strength other than 0 , by specifying it with the option --signalForSignificance=X Info This calculation assumes that the signal strength can only be positive (i.e we are not interested in negative signal strengths). This can be altered by including the option --uncapped","title":"Asymptotic Significances"},{"location":"part3/commonstatsmethods/#compute-the-observed-significance","text":"The observed significance is calculated using the Significance method, as combine -M Significance datacard.txt The printed output will report the significance and the p-value, for example, when using the realistic-counting-experiment.txt datacard, you will see <<< Combine >>> >>> including systematics >>> method used is Significance [...] -- Significance -- Significance: 0 (p-value = 0.5) Done in 0.00 min (cpu), 0.01 min (real) which is not surprising since 0 events were observed in that datacard. The output root file will contain the significance value in the branch limit . To store the p-value instead, include the option --pval . These can be converted between one another using the RooFit functions RooFit::PValueToSignificance and RooFit::SignificanceToPValue . You may find it useful to resort to a brute-force fitting algorithm when calculating the significance which scans the nll (repeating fits until a tolerance is reached), bypassing MINOS, which can be activated with the option bruteForce . This can be tuned using the options setBruteForceAlgo , setBruteForceTypeAndAlgo and setBruteForceTolerance .","title":"Compute the observed significance"},{"location":"part3/commonstatsmethods/#computing-the-expected-significance","text":"The expected significance can be computed from an Asimov dataset of signal+background. There are two options for this a-posteriori expected: will depend on the observed dataset. a-priori expected (the default behavior): does not depend on the observed dataset, and so is a good metric for optimizing an analysis when still blinded. The a-priori expected significance from the Asimov dataset is calculated as combine -M Significance datacard.txt -t -1 --expectSignal=1 In order to produced the a-posteriori expected significance, just generate a post-fit Asimov (i.e add the option --toysFreq in the command above). The output format is the same as for observed signifiances: the variable limit in the tree will be filled with the significance (or with the p-value if you put also the option --pvalue )","title":"Computing the expected significance"},{"location":"part3/commonstatsmethods/#bayesian-limits-and-credible-regions","text":"Bayesian calculation of limits requires the user to assume a particular prior distribution for the parameter of interest (default r ). You can specify the prior using the --prior option, the default is a flat pior in r . Since the Bayesian methods are much less frequently used, the tool will not build the default prior. For running the two methods below, you should include the option --noDefaultPrior=0 .","title":"Bayesian Limits and Credible regions"},{"location":"part3/commonstatsmethods/#computing-the-observed-bayesian-limit-for-simple-models","text":"The BayesianSimple method computes a Bayesian limit performing classical numerical integration; very fast and accurate but only works for simple models (a few channels and nuisance parameters). combine -M BayesianSimple simple-counting-experiment.txt --noDefaultPrior=0 [...] -- BayesianSimple -- Limit: r < 0.672292 @ 95% CL Done in 0.04 min (cpu), 0.05 min (real) The output tree will contain a single entry corresponding to the observed 95% upper limit. The confidence level can be modified to 100*X% using --cl X .","title":"Computing the observed bayesian limit (for simple models)"},{"location":"part3/commonstatsmethods/#computing-the-observed-bayesian-limit-for-arbitrary-models","text":"The MarkovChainMC method computes a Bayesian limit performing a monte-carlo integration. From the statistics point of view it is identical to the BayesianSimple method, only the technical implementation is different. The method is slower, but can also handle complex models. For this method, you can increase the accuracy of the result by increasing the number of markov chains at the expense of a longer running time (option --tries , default is 10). Let's use the realistic counting experiment datacard to test the method To use the MarkovChainMC method, users need to specify this method in the command line, together with the options they want to use. For instance, to set the number of times the algorithm will run with different random seeds, use option --tries : combine -M MarkovChainMC realistic-counting-experiment.txt --tries 100 --noDefaultPrior=0 [...] -- MarkovChainMC -- Limit: r < 2.20438 +/- 0.0144695 @ 95% CL (100 tries) Average chain acceptance: 0.078118 Done in 0.14 min (cpu), 0.15 min (real) Again, the resulting limit tree will contain the result. You can also save the chains using the option --saveChain which will then also be included in the output file. Exclusion regions can be made from the posterior once an ordering principle is defined to decide how to grow the contour (there's infinite possible regions that contain 68% of the posterior pdf...) Below is a simple example script which can be used to plot the posterior distribution from these chains and calculate the smallest such region. Note that in this example we are ignoring the burn-in (but you can add it by just editing for i in range(mychain.numEntries()): to for i in range(200,mychain.numEntries()): eg for a burn-in of 200. import ROOT rmin = 0 rmax = 30 nbins = 100 CL = 0.95 chains = \"higgsCombineTest.MarkovChainMC.blahblahblah.root\" def findSmallestInterval(hist,CL): bins = hist.GetNbinsX() best_i = 1 best_j = 1 bd = bins+1 val = 0; for i in range(1,bins+1): integral = hist.GetBinContent(i) for j in range(i+1,bins+2): integral += hist.GetBinContent(j) if integral > CL : val = integral break if integral > CL and j-i < bd : bd = j-i best_j = j+1 best_i = i val = integral return hist.GetBinLowEdge(best_i), hist.GetBinLowEdge(best_j), val fi_MCMC = ROOT.TFile.Open(chains) # Sum up all of the chains (or we could take the average limit) mychain=0 for k in fi_MCMC.Get(\"toys\").GetListOfKeys(): obj = k.ReadObj if mychain ==0: mychain = k.ReadObj().GetAsDataSet() else : mychain.append(k.ReadObj().GetAsDataSet()) hist = ROOT.TH1F(\"h_post\",\";r;posterior probability\",nbins,rmin,rmax) for i in range(mychain.numEntries()): #for i in range(200,mychain.numEntries()): burn-in of 200 mychain.get(i) hist.Fill(mychain.get(i).getRealValue(\"r\"), mychain.weight()) hist.Scale(1./hist.Integral()) hist.SetLineColor(1) vl,vu,trueCL = findSmallestInterval(hist,CL) histCL = hist.Clone() for b in range(nbins): if histCL.GetBinLowEdge(b+1) < vl or histCL.GetBinLowEdge(b+2)>vu: histCL.SetBinContent(b+1,0) c6a = ROOT.TCanvas() histCL.SetFillColor(ROOT.kAzure-3) histCL.SetFillStyle(1001) hist.Draw() histCL.Draw(\"histFsame\") hist.Draw(\"histsame\") ll = ROOT.TLine(vl,0,vl,2*hist.GetBinContent(hist.FindBin(vl))); ll.SetLineColor(2); ll.SetLineWidth(2) lu = ROOT.TLine(vu,0,vu,2*hist.GetBinContent(hist.FindBin(vu))); lu.SetLineColor(2); lu.SetLineWidth(2) ll.Draw() lu.Draw() print \" %g %% (%g %%) interval (target) = %g < r < %g \"%(trueCL,CL,vl,vu) Running the script on the output file produced for the same datacard (including the --saveChain option) will produce the following output 0.950975 % (0.95 %) interval (target) = 0 < r < 2.2 along with a plot of the posterior shown below. This is the same as the output from combine but the script can also be used to find lower limits (for example) or credible intervals. An example to make contours when ordering by probability density is in bayesContours.cxx , but the implementation is very simplistic, with no clever handling of bin sizes nor any smoothing of statistical fluctuations. The MarkovChainMC algorithm has many configurable parameters, and you're encouraged to experiment with those because the default configuration might not be the best for you (or might not even work for you at all)","title":"Computing the observed bayesian limit (for arbitrary models)"},{"location":"part3/commonstatsmethods/#iterations-burn-in-tries","text":"Three parameters control how the MCMC integration is performed: the number of tries (option --tries ): the algorithm will run multiple times with different ransom seeds and report as result the truncated mean and rms of the different results. The default value is 10, which should be ok for a quick computation, but for something more accurate you might want to increase this number even up to ~200. the number of iterations (option -i ) determines how many points are proposed to fill a single Markov Chain. The default value is 10k, and a plausible range is between 5k (for quick checks) and 20-30k for lengthy calculations. Usually beyond 30k you get a better tradeoff in time vs accuracy by increasing the number of chains (option --tries ) the number of burn-in steps (option -b ) is the number of points that are removed from the beginning of the chain before using it to compute the limit. The default is 200. If your chain is very long, you might want to try increase this a bit (e.g. to some hundreds). Instead going below 50 is probably dangerous.","title":"Iterations, burn-in, tries"},{"location":"part3/commonstatsmethods/#proposals","text":"The option --proposal controls the way new points are proposed to fill in the MC chain. uniform : pick points at random. This works well if you have very few nuisance parameters (or none at all), but normally fails if you have many. gaus : Use a product of independent gaussians one for each nuisance parameter; the sigma of the gaussian for each variable is 1/5 of the range of the variable (this can be controlled using the parameter --propHelperWidthRangeDivisor ). This proposal appears to work well for a reasonable number of nuisances (up to ~15), provided that the range of the nuisance parameters is reasonable, like \u00b15\u03c3. It does not work without systematics. ortho ( default ): This proposalis similar to the multi-gaussian proposal but at every step only a single coordinate of the point is varied, so that the acceptance of the chain is high even for a large number of nuisances (i.e. more than 20). fit : Run a fit and use the uncertainty matrix from HESSE to construct a proposal (or the one from MINOS if the option --runMinos is specified). This sometimes work fine, but sometimes gives biased results, so we don't recommend it in general. If you believe there's something going wrong, e.g. if your chain remains stuck after accepting only a few events, the option --debugProposal can be used to have a printout of the first N proposed points to see what's going on (e.g. if you have some region of the phase space with probability zero, the gaus and fit proposal can get stuck there forever)","title":"Proposals"},{"location":"part3/commonstatsmethods/#computing-the-expected-bayesian-limit","text":"The expected limit is computed by generating many toy mc observations and compute the limit for each of them. This can be done passing the option -t . E.g. to run 100 toys with the BayesianSimple method, just do combine -M BayesianSimple datacard.txt -t 100 --noDefaultPrior=0 The program will print out the mean and median limit, and the 68% and 95% quantiles of the distributions of the limits. This time, the output root tree will contain one entry per toy . For more heavy methods (eg the MarkovChainMC ) you'll probably want to split this in multiple jobs. To do this, just run combine multiple times specifying a smaller number of toys (can be as low as 1 ) each time using a different seed to initialize the random number generator (option -s if you set it to -1, the starting seed will be initialized randomly at the beginning of the job), then merge the resulting trees with hadd and look at the distribution in the merged file.","title":"Computing the expected bayesian limit"},{"location":"part3/commonstatsmethods/#multidimensional-bayesian-credible-regions","text":"The MarkovChainMC method allows the user to produce the posterior pdf as a function of (in principle) any number of parameter of interest. In order to do so, you first need to create a workspace with more than one parameter, as explained in the physics models section. For example, lets use the toy datacard test/multiDim/toy-hgg-125.txt (counting experiment which vaguely resembles the H\u2192\u03b3\u03b3 analysis at 125 GeV) and convert the datacard into a workspace with 2 parameters, ggH and qqH cross sections using text2workspace with the option -P HiggsAnalysis.CombinedLimit.PhysicsModel:floatingXSHiggs --PO modes=ggH,qqH . Now we just run one (or more) MCMC chain(s) and save them in the output tree.By default, the nuisance parameters will be marginalized (integrated) over their pdfs. You can ignore the complaints about not being able to compute an upper limit (since for more than 1D, this isn't well defined), combine -M MarkovChainMC workspace.root --tries 1 --saveChain -i 1000000 -m 125 -s seed --noDefaultPrior=0 The output of the markov chain is again a RooDataSet of weighted events distributed according to the posterior pdf (after you cut out the burn in part), so it can be used to make histograms or other distributions of the posterior pdf. See as an example bayesPosterior2D.cxx . Below is an example of the output of the macro, $ root -l higgsCombineTest.MarkovChainMC.... .L bayesPosterior2D.cxx bayesPosterior2D(\"bayes2D\",\"Posterior PDF\")","title":"Multidimensional bayesian credible regions"},{"location":"part3/commonstatsmethods/#computing-limits-with-toys","text":"The HybridNew method is used to compute either the hybrid bayesian-frequentist limits popularly known as \"CL s of LEP or Tevatron type\" or the fully frequentist limits which are the current recommended method by the LHC Higgs Combination Group. Note that these methods can be resource intensive for complex models. It is possible to define the criterion used for setting limits using --rule CLs (to use the CL s criterion) or --rule CLsplusb (to calculate the limit using p_{\\mu} p_{\\mu} ) and as always the confidence level desired using --cl=X The choice of test-statistic can be made via the option --testStat and different methodologies for treatment of the nuisance parameters are available. While it is possible to mix different test-statistics with different nuisance parameter treatments, this is highly not-reccomended . Instead one should follow one of the following three procedures, LEP-style : --testStat LEP --generateNuisances=1 --fitNuisances=0 The test statistic is defined using the ratio of likelihoods q_{\\mathrm{LEP}}=-2\\ln[\\mathcal{L}(\\mathrm{data}|r=0)/\\mathcal{L}(\\mathrm{data}|r)] q_{\\mathrm{LEP}}=-2\\ln[\\mathcal{L}(\\mathrm{data}|r=0)/\\mathcal{L}(\\mathrm{data}|r)] . The nuisance parameters are fixed to their nominal values for the purpose of evaluating the likelihood, while for generating toys, the nuisance parameters are first randomized within their pdfs before generation of the toy. TEV-style : --testStat TEV --generateNuisances=0 --generateExternalMeasurements=1 --fitNuisances=1 The test statistic is defined using the ratio of likelihoods q_{\\mathrm{TEV}}=-2\\ln[\\mathcal{L}(\\mathrm{data}|r=0,\\hat{\\theta}_{0})/\\mathcal{L}(\\mathrm{data}|r,\\hat{\\theta}_{r})] q_{\\mathrm{TEV}}=-2\\ln[\\mathcal{L}(\\mathrm{data}|r=0,\\hat{\\theta}_{0})/\\mathcal{L}(\\mathrm{data}|r,\\hat{\\theta}_{r})] , in which the nuisance parameters are profiled separately for r=0 r=0 and r r . For the purposes of toy generation, the nuisance parameters are fixed to their post-fit values from the data (conditional on r), while the constraint terms are randomized for the evaluation of the likelihood. LHC-style : --LHCmode LHC-limits , which is the shortcut for --testStat LHC --generateNuisances=0 --generateExternalMeasurements=1 --fitNuisances=1 The test statistic is defined using the ratio of likelihoods q_{r} = -2\\ln[\\mathcal{L}(\\mathrm{data}|r,\\hat{\\theta}_{r})/\\mathcal{L}(\\mathrm{data}|r=\\hat{r},\\hat{\\theta}]) q_{r} = -2\\ln[\\mathcal{L}(\\mathrm{data}|r,\\hat{\\theta}_{r})/\\mathcal{L}(\\mathrm{data}|r=\\hat{r},\\hat{\\theta}]) , in which the nuisance parameters are profiled separately for r=\\hat{r} r=\\hat{r} and r r . The value of q_{r} q_{r} set to 0 when \\hat{r}>r \\hat{r}>r giving a one sided limit. Furthermore, the constraint r>0 r>0 is enforced in the fit. This means that if the unconstrained value of \\hat{r} \\hat{r} would be negative, the test statistic q_{r} q_{r} is evaluated as -2\\ln[\\mathcal{L}(\\mathrm{data}|r,\\hat{\\theta}_{r})/\\mathcal{L}(\\mathrm{data}|r=0,\\hat{\\theta}_{0}]) -2\\ln[\\mathcal{L}(\\mathrm{data}|r,\\hat{\\theta}_{r})/\\mathcal{L}(\\mathrm{data}|r=0,\\hat{\\theta}_{0}]) For the purposes of toy generation, the nuisance parameters are fixed to their post-fit values from the data (conditionally on the value of r ), while the constraint terms are randomized in the evaluation of the likelihood. Warning The recommended style is the LHC-style . Please note that this method is sensitive to the observation in data since the post-fit (after a fit to the data) values of the nuisance parameters (assuming different values of r ) are used when generating the toys. For completely blind limits you can first generate a pre-fit asimov toy dataset (described in the toy data generation section) and use that in place of the data. While the above shortcuts are the common variants, you can also try others. The treatment of the nuisances can be changed to the so-called \"Hybrid-Bayesian\" method which effectively integrates over the nuisance parameters. This can be achieved (with any test-statistic which is not profiled over the nuisances) by setting --generateNuisances=1 --generateExternalMeasurements=0 --fitNuisances=0 . Info Note that (observed and toy) values of the test statistic stored in the instances of RooStats::HypoTestResult when the option --saveHybridResult has been specified, are defined without the factor 2 and therefore are twice as small as the values given by the formulas above. This factor is however included automatically by all plotting script supplied within the Combine package.","title":"Computing Limits with toys"},{"location":"part3/commonstatsmethods/#simple-models","text":"For relatively simple models, the observed and expected limits can be calculated interactively. Since the LHC-style is the reccomended procedure for calculating limits using toys, we will use that in this section but the same applies to the other methods. combine realistic-counting-experiment.txt -M HybridNew --LHCmode LHC-limits Show output < < < Combine >>> >>> including systematics >>> using the Profile Likelihood test statistics modified for upper limits (Q_LHC) >>> method used is HybridNew >>> random number generator seed is 123456 Computing results starting from observation (a-posteriori) Search for upper limit to the limit r = 20 +/- 0 CLs = 0 +/- 0 CLs = 0 +/- 0 CLb = 0.264 +/- 0.0394263 CLsplusb = 0 +/- 0 Search for lower limit to the limit Now doing proper bracketing & bisection r = 10 +/- 10 CLs = 0 +/- 0 CLs = 0 +/- 0 CLb = 0.288 +/- 0.0405024 CLsplusb = 0 +/- 0 r = 5 +/- 5 CLs = 0 +/- 0 CLs = 0 +/- 0 CLb = 0.152 +/- 0.0321118 CLsplusb = 0 +/- 0 r = 2.5 +/- 2.5 CLs = 0.0192308 +/- 0.0139799 CLs = 0.02008 +/- 0.0103371 CLs = 0.0271712 +/- 0.00999051 CLs = 0.0239524 +/- 0.00783634 CLs = 0.0239524 +/- 0.00783634 CLb = 0.208748 +/- 0.0181211 CLsplusb = 0.005 +/- 0.00157718 r = 2.00696 +/- 1.25 CLs = 0.0740741 +/- 0.0288829 CLs = 0.0730182 +/- 0.0200897 CLs = 0.0694474 +/- 0.0166468 CLs = 0.0640182 +/- 0.0131693 CLs = 0.0595 +/- 0.010864 CLs = 0.0650862 +/- 0.0105575 CLs = 0.0629286 +/- 0.00966301 CLs = 0.0634945 +/- 0.00914091 CLs = 0.060914 +/- 0.00852667 CLs = 0.06295 +/- 0.00830083 CLs = 0.0612758 +/- 0.00778181 CLs = 0.0608142 +/- 0.00747001 CLs = 0.0587169 +/- 0.00697039 CLs = 0.0591432 +/- 0.00678587 CLs = 0.0599683 +/- 0.00666966 CLs = 0.0574868 +/- 0.00630809 CLs = 0.0571451 +/- 0.00608177 CLs = 0.0553836 +/- 0.00585531 CLs = 0.0531612 +/- 0.0055234 CLs = 0.0516837 +/- 0.0052607 CLs = 0.0496776 +/- 0.00499783 CLs = 0.0496776 +/- 0.00499783 CLb = 0.216635 +/- 0.00801002 CLsplusb = 0.0107619 +/- 0.00100693 Trying to move the interval edges closer r = 1.00348 +/- 0 CLs = 0.191176 +/- 0.0459911 CLs = 0.191176 +/- 0.0459911 CLb = 0.272 +/- 0.0398011 CLsplusb = 0.052 +/- 0.00992935 r = 1.50522 +/- 0 CLs = 0.125 +/- 0.0444346 CLs = 0.09538 +/- 0.0248075 CLs = 0.107714 +/- 0.0226712 CLs = 0.103711 +/- 0.018789 CLs = 0.0845069 +/- 0.0142341 CLs = 0.0828468 +/- 0.0126789 CLs = 0.0879647 +/- 0.0122332 CLs = 0.0879647 +/- 0.0122332 CLb = 0.211124 +/- 0.0137494 CLsplusb = 0.0185714 +/- 0.00228201 r = 1.75609 +/- 0 CLs = 0.0703125 +/- 0.0255807 CLs = 0.0595593 +/- 0.0171995 CLs = 0.0555271 +/- 0.0137075 CLs = 0.0548727 +/- 0.0120557 CLs = 0.0527832 +/- 0.0103348 CLs = 0.0555828 +/- 0.00998248 CLs = 0.0567971 +/- 0.00923449 CLs = 0.0581822 +/- 0.00871417 CLs = 0.0588835 +/- 0.00836245 CLs = 0.0594035 +/- 0.00784761 CLs = 0.0590583 +/- 0.00752672 CLs = 0.0552067 +/- 0.00695542 CLs = 0.0560446 +/- 0.00679746 CLs = 0.0548083 +/- 0.0064351 CLs = 0.0566998 +/- 0.00627124 CLs = 0.0561576 +/- 0.00601888 CLs = 0.0551643 +/- 0.00576338 CLs = 0.0583584 +/- 0.00582854 CLs = 0.0585691 +/- 0.0057078 CLs = 0.0599114 +/- 0.00564585 CLs = 0.061987 +/- 0.00566905 CLs = 0.061836 +/- 0.00549856 CLs = 0.0616849 +/- 0.0053773 CLs = 0.0605352 +/- 0.00516844 CLs = 0.0602028 +/- 0.00502875 CLs = 0.058667 +/- 0.00486263 CLs = 0.058667 +/- 0.00486263 CLb = 0.222901 +/- 0.00727258 CLsplusb = 0.0130769 +/- 0.000996375 r = 2.25348 +/- 0 CLs = 0.0192308 +/- 0.0139799 CLs = 0.0173103 +/- 0.00886481 CLs = 0.0173103 +/- 0.00886481 CLb = 0.231076 +/- 0.0266062 CLsplusb = 0.004 +/- 0.001996 r = 2.13022 +/- 0 CLs = 0.0441176 +/- 0.0190309 CLs = 0.0557778 +/- 0.01736 CLs = 0.0496461 +/- 0.0132776 CLs = 0.0479048 +/- 0.0114407 CLs = 0.0419333 +/- 0.00925719 CLs = 0.0367934 +/- 0.0077345 CLs = 0.0339814 +/- 0.00684844 CLs = 0.03438 +/- 0.0064704 CLs = 0.0337633 +/- 0.00597315 CLs = 0.0321262 +/- 0.00551608 CLs = 0.0321262 +/- 0.00551608 CLb = 0.230342 +/- 0.0118665 CLsplusb = 0.0074 +/- 0.00121204 r = 2.06859 +/- 0 CLs = 0.0357143 +/- 0.0217521 CLs = 0.0381957 +/- 0.0152597 CLs = 0.0368622 +/- 0.0117105 CLs = 0.0415097 +/- 0.0106676 CLs = 0.0442816 +/- 0.0100457 CLs = 0.0376644 +/- 0.00847235 CLs = 0.0395133 +/- 0.0080427 CLs = 0.0377625 +/- 0.00727262 CLs = 0.0364415 +/- 0.00667827 CLs = 0.0368015 +/- 0.00628517 CLs = 0.0357251 +/- 0.00586442 CLs = 0.0341604 +/- 0.00546373 CLs = 0.0361935 +/- 0.00549648 CLs = 0.0403254 +/- 0.00565172 CLs = 0.0408613 +/- 0.00554124 CLs = 0.0416682 +/- 0.00539651 CLs = 0.0432645 +/- 0.00538062 CLs = 0.0435229 +/- 0.00516945 CLs = 0.0427647 +/- 0.00501322 CLs = 0.0414894 +/- 0.00479711 CLs = 0.0414894 +/- 0.00479711 CLb = 0.202461 +/- 0.00800632 CLsplusb = 0.0084 +/- 0.000912658 -- HybridNew, before fit -- Limit: r < 2.00696 +/- 1.25 [1.50522, 2.13022] Warning in : Could not create the Migrad minimizer. Try using the minimizer Minuit Fit to 5 points: 1.91034 +/- 0.0388334 -- Hybrid New -- Limit: r < 1.91034 +/- 0.0388334 @ 95% CL Done in 0.01 min (cpu), 4.09 min (real) Failed to delete temporary file roostats-Sprxsw.root: No such file or directory The result stored in the limit branch of the output tree will be the upper limit (and its error stored in limitErr ). The default behavior will be, as above, to search for the upper limit on r however, the values of p_{\\mu}, p_{b} p_{\\mu}, p_{b} and CL s can be calculated for a particular value r=X by specifying the option --singlePoint=X . In this case, the value stored in the branch limit will be the value of CL s (or p_{\\mu} p_{\\mu} ) (see the FAQ section).","title":"Simple models"},{"location":"part3/commonstatsmethods/#expected-limits","text":"For the simple models, we can just run interactively 5 times to compute the median expected and the 68% and 95% interval boundaries. Use the HybridNew method with the same options as per the observed limit but adding a --expectedFromGrid=<quantile> where the quantile is 0.5 for the median, 0.84 for the +ve side of the 68% band, 0.16 for the -ve side of the 68% band, 0.975 for the +ve side of the 95% band, 0.025 for the -ve side of the 95% band. The output file will contain the value of the quantile in the branch quantileExpected which can be used to separate the points.","title":"Expected Limits"},{"location":"part3/commonstatsmethods/#accuracy","text":"The search for the limit is performed using an adaptive algorithm, terminating when the estimate of the limit value is below some limit or when the precision cannot be futher improved with the specified options. The options controlling this behaviour are: rAbsAcc , rRelAcc : define the accuracy on the limit at which the search stops. The default values are 0.1 and 0.05 respectively, meaning that the search is stopped when \u0394r < 0.1 or \u0394r/r < 0.05. clsAcc : this determines the absolute accuracy up to which the CLs values are computed when searching for the limit. The default is 0.5%. Raising the accuracy above this value will increase significantly the time to run the algorithm, as you need N 2 more toys to improve the accuracy by a factor N, you can consider enlarging this value if you're computing limits with a larger CL (e.g. 90% or 68%). Note that if you're using the CLsplusb rule then this parameter will control the uncertainty on p_{\\mu} p_{\\mu} rather than CL s . T or toysH : controls the minimum number of toys that are generated for each point. The default value of 500 should be ok when computing the limit with 90-95% CL. You can decrease this number if you're computing limits at 68% CL, or increase it if you're using 99% CL. Note, to further improve the accuracy when searching for the upper limit, combine will also fit an exponential function to several of the points and interpolate to find the crossing.","title":"Accuracy"},{"location":"part3/commonstatsmethods/#complex-models","text":"For complicated models, it is best to produce a grid of test statistic distributions at various values of the signal strength, and use it to compute the observed and expected limit and bands. This approach is good for complex models since the grid of points can be distributed across any number of jobs. In this approach we will store the distributions of the test-statistic at different values of the signal strength using the option --saveHybridResult . The distribution at a single value of r=X can be determined by combine datacard.txt -M HybridNew --LHCmode LHC-limits --singlePoint X --saveToys --saveHybridResult -T 500 --clsAcc 0 Warning We have specified the accuracy here by including clsAcc=0 which turns off adaptive sampling and specifying the number of toys to be 500 with the -T N option. For complex models, it may be necessary to split the toys internally over a number of instances of HybridNew using the option --iterations I . The total number of toys will be the product I*N . The above can be repeated several times, in parallel, to build the distribution of the test-statistic (giving the random seed option -s -1 ). Once all of the distributions are finished, the resulting output files can be merged into one using hadd and read back to calculate the limit, specifying the merged file with --grid=merged.root . The observed limit can be obtained with combine datacard.txt -M HybridNew --LHCmode LHC-limits --readHybridResults --grid=merged.root and similarly, the median expected and quantiles can be determined using combine datacard.txt -M HybridNew --LHCmode LHC-limits --readHybridResults --grid=merged.root --quantileExpected <quantile> substituting <quantile> with 0.5 for the median, 0.84 for the +ve side of the 68% band, 0.16 for the -ve side of the 68% band, 0.975 for the +ve side of the 95% band, 0.025 for the -ve side of the 95% band. The splitting of the jobs can be left to the user's preference. However, users may wish to use the combineTool for automating this as described in the section on combineTool for job submission","title":"Complex models"},{"location":"part3/commonstatsmethods/#plotting","text":"A plot of the CL s (or p_{\\mu} p_{\\mu} ) as a function of r , which is used to find the crossing, can be produced using the option --plot=limit_scan.png . This can be useful for judging if the grid was sufficient in determining the upper limit. If we use our realistic-counting-experiment.txt datacard and generate a grid of points r\\varepsilon[1.4,2.2] r\\varepsilon[1.4,2.2] in steps of 0.1, with 5000 toys for each point, the plot of the observed CL s vs r should look like the following, You should judge in each case if the limit is accurate given the spacing of the points and the precision of CL s at each point. If it is not sufficient, simply generate more points closer to the limit and/or more toys at each point. The distributions of the test-statistic can also be plotted, at each value in the grid, using the simple python tool, python test/plotTestStatCLs.py --input mygrid.root --poi r --val all --mass MASS The resulting output file will contain a canvas showing the distribution of the test statistic background only and signal+background hypothesis at each value of r . Info If you used the TEV or LEP style test statistic (using the commands as described above), then you should include the option --doublesided , which will also take care of defining the correct integrals for p_{\\mu} p_{\\mu} and p_{b} p_{b} .","title":"Plotting"},{"location":"part3/commonstatsmethods/#computing-significances-with-toys","text":"Computation of expected significance with toys is a two step procedure: first you need to run one or more jobs to construct the expected distribution of the test statistic. As with setting limits, there are a number of different configurations for generating toys but we will use the preferred option using, LHC-style : --LHCmode LHC-significance , which is the shortcut for --testStat LHC --generateNuisances=0 --generateExternalMeasurements=1 --fitNuisances=1 --significance The test statistic is defined using the ratio of likelihoods q_{0} = -2\\ln[\\mathcal{L}(\\textrm{data}|r=0,\\hat{\\theta}_{0})/\\mathcal{L}(\\textrm{data}|r=\\hat{r},\\hat{\\theta})] q_{0} = -2\\ln[\\mathcal{L}(\\textrm{data}|r=0,\\hat{\\theta}_{0})/\\mathcal{L}(\\textrm{data}|r=\\hat{r},\\hat{\\theta})] , in which the nuisance parameters are profiled separately for r=\\hat{r} r=\\hat{r} and r=0 r=0 . The value of the test statistic is set to 0 when \\hat{r}<0 \\hat{r}<0 For the purposes of toy generation, the nuisance parameters are fixed to their post-fit values from the data assuming no signal, while the constraint terms are randomized for the evaluation of the likelihood.","title":"Computing Significances with toys"},{"location":"part3/commonstatsmethods/#observed-significance","text":"To construct the distribution of the test statistic run as many times as necessary, combine -M HybridNew datacard.txt --LHCmode LHC-significance --saveToys --fullBToys --saveHybridResult -T toys -i iterations -s seed with different seeds, or using -s -1 for random seeds, then merge all those results into a single root file with hadd . The observed significance can be calculated as combine -M HybridNew datacard.txt --LHCmode LHC-significance --readHybridResult --grid=input.root [--pvalue ] where the option --pvalue will replace the result stored in the limit branch output tree to be the p-value instead of the signficance.","title":"Observed significance"},{"location":"part3/commonstatsmethods/#expected-significance-assuming-some-signal","text":"The expected significance, assuming a signal with r=X can be calculated, by including the option --expectSignal X when generating the distribution of the test statistic and using the option --expectedFromGrid=0.5 when calculating the significance for the median. To get the \u00b11\u03c3 bands, use 0.16 and 0.84 instead of 0.5, and so on... You need a total number of background toys large enough to compute the value of the significance, but you need less signal toys (especially if you only need the median). For large significance, you can then run most of the toys without the --fullBToys option (about a factor 2 faster), and only a smaller part with that option turned on. As with calculating limits with toys, these jobs can be submitted to the grid or batch systems with the help of the combineTool as described in the section on combineTool for job submission","title":"Expected significance, assuming some signal"},{"location":"part3/commonstatsmethods/#goodness-of-fit-tests","text":"The GoodnessOfFit method can be used to evaluate how compatible the observed data are with the model pdf. The module can be run specifying an algorithm, and will compute a goodness of fit indicator for that algorithm and the data. The procedure is therefore to first run on the real data combine -M GoodnessOfFit datacard.txt --algo=<some-algo> and then to run on many toy mc datasets to determine the distribution of the goodness of fit indicator combine -M GoodnessOfFit datacard.txt --algo=<some-algo> -t <number-of-toys> -s <seed> When computing the goodness of fit, by default the signal strength is left floating in the fit, so that the measure is independent from the presence or absence of a signal. It is possible to instead keep it fixed to some value by passing the option --fixedSignalStrength=<value> . The following algorithms are supported: saturated : Compute a goodness-of-fit measure for binned fits based on the saturated model method, as prescribed by the StatisticsCommittee (note) . This quantity is similar to a chi-square, but can be computed for an arbitrary combination of binned channels with arbitrary constraints. KS : Compute a goodness-of-fit measure for binned fits using the Kolmogorov-Smirnov test. It is based on the highest difference between the cumulative distribution function and the empirical distribution function of any bin. AD : Compute a goodness-of-fit measure for binned fits using the Anderson-Darling test. It is based on the integral of the difference between the cumulative distribution function and the empirical distribution function over all bins. It also gives the tail ends of the distribution a higher weighting. The output tree will contain a branch called limit which contains the value of the test-statistic in each toy. You can make a histogram of this test-statistic t t and from this distribution ( f(t) f(t) ) and the single value obtained in the data ( t_{0} t_{0} ) you can calculate the p-value $$p = \\int_{t=t_{0}}^{\\mathrm{+inf}} f(t) dt $$. When generating toys, the default behavior will be used. See the section on toy generation for options on how to generate/fit nuisance parameters in these tests. It is recomended to use the frequentist toys ( --toysFreq ) when running the saturated model, and the default toys for the other two tests. Further goodness of fit methods could be added on request, especially if volunteers are available to code them. The output limit tree will contain the value of the test-statistic in each toy (or the data) Warning The above algorithms are all concerned with one-sample tests. For two-sample tests, you can follow an example CMS HIN analysis described in this Twiki","title":"Goodness of fit tests"},{"location":"part3/commonstatsmethods/#masking-analysis-regions-in-the-saturated-model","text":"For searches that employs a simultaneous fit across signal and control regions, it may be useful to mask one or more analysis regions either when the likelihood is maximized (fit) or when the test-statistic is computed. This can be done by using the options --setParametersForFit and --setParametersForEval , respectively. The former will set parameters before each fit while the latter is used to set parameters after each fit, but before the NLL is evauated. Note of course that if the parameter in the list is floating, it will be still floating in each fit so will not effect the results when using --setParametersForFit . A realistic example for a binned shape analysis performed in one signal region and two control samples can be found in this directory of the Higgs-combine package Datacards-shape-analysis-multiple-regions . First of all, one needs to combine the individual datacards to build a single model and to introduce the channel-masking variables as follow: combineCards.py signal_region.txt dimuon_control_region.txt singlemuon_control_region.txt > combined_card.txt text2workspace.py combined_card.txt --channel-masks More information about the channel-masking can be found in this section Channel Masking . The saturated test-static value for a simultaneous fit across all the analysis regions can be calculated as: combine -M GoodnessOfFit -d combined_card.root --algo=saturated -n _result_sb In this case, signal and control regions are included in both the fit and in the evaluation of the test-static, and the signal strength is freely floating. This measures the compatibility between the signal+background fit and the observed data. Moreover, it can be interesting to assess the level of compatibility between the observed data in all the regions and the background prediction obtained by only fitting the control regions (CR-only fit). This is computed as follow: combine -M GoodnessOfFit -d combined_card.root --algo=saturated -n _result_bonly_CRonly --setParametersForFit mask_ch1=1 --setParametersForEval mask_ch1=0 --freezeParameters r --setParameters r=0 where the signal strength is frozen and the signal region is not considered in the fit ( --setParametersForFit mask_ch1=1 ), but it is included in the test-statistic computation ( --setParametersForEval mask_ch1=0 ). To show the differences between the two models being tested, one can perform a fit to the data using the FitDiagnostics method as: combine -M FitDiagnostics -d combined_card.root -n _fit_result --saveShapes --saveWithUncertainties combine -M FitDiagnostics -d combined_card.root -n _fit_CRonly_result --saveShapes --saveWithUncertainties --setParameters mask_ch1=1 By taking the total background, the total signal, and the data shapes from FitDiagnostics output, we can compare the post-fit predictions from the S+B fit (first case) and the CR-only fit (second case) with the observation as reported below: FitDiagnostics S+B fit FitDiagnostics CR-only fit To compute a p-value for the two results, one needs to compare the observed goodness-of-fit value previously computed with expected distribution of the test-statistic obtained in toys: combine -M GoodnessOfFit combined_card.root --algo=saturated -n result_toy_sb --toysFrequentist -t 500 combine -M GoodnessOfFit -d combined_card.root --algo=saturated -n _result_bonly_CRonly_toy --setParametersForFit mask_ch1=1 --setParametersForEval mask_ch1=0 --freezeParameters r --setParameters r=0,mask_ch1=1 -t 500 --toysFrequentist where the former gives the result for the S+B model, while the latter gives the test-statistic for CR-only fit. The command --setParameters r=0,mask_ch1=1 is needed to ensure that toys are thrown using the nuisance parameters estimated from the CR-only fit to the data. The comparison between the observation and the expected distribition should look like the following two plots: Goodness-of-fit for S+B model Goodness-of-fit for CR-only model","title":"Masking analysis regions in the saturated model"},{"location":"part3/commonstatsmethods/#making-a-plot-of-the-gof-test-statistic-distribution","text":"If you have also checked out the combineTool , you can use this to run batch jobs or on the grid (see here ) and produce a plot of the results. Once you have the jobs, you can hadd them together and run (e.g for the saturated model), combineTool.py -M CollectGoodnessOfFit --input data_run.root toys_run.root -m 125.0 -o gof.json plotGof.py gof.json --statistic saturated --mass 125.0 -o gof_plot --title-right=\"my label\"","title":"Making a plot of the GoF test-statistic distribution"},{"location":"part3/commonstatsmethods/#channel-compatibility","text":"The ChannelCompatibilityCheck method can be used to evaluate how compatible are the measurements of the signal strength from the separate channels of a combination. The method performs two fits of the data, first with the nominal model in which all channels are assumed to have the same signal strength multiplier r r , and then another allowing separate signal strengths r_{i} r_{i} in each channel. A chisquare-like quantity is computed as -2 \\ln \\mathcal{L}(\\mathrm{data}| r)/L(\\mathrm{data}|\\{r_{i}\\}_{i=1}^{N_{\\mathrm{chan}}}) -2 \\ln \\mathcal{L}(\\mathrm{data}| r)/L(\\mathrm{data}|\\{r_{i}\\}_{i=1}^{N_{\\mathrm{chan}}}) . Just like for the goodness of fit indicators, the expected distribution of this quantity under the nominal model can be computed from toy mc. By default, the signal strength is kept floating in the fit with the nominal model. It can however be fixed to a given value by passing the option --fixedSignalStrength=<value> . In the default models build from the datacards the signal strengths in all channels are constrained to be non-negative. One can allow negative signal strengths in the fits by changing the bound on the variable (option --rMin=<value> ), which should make the quantity more chisquare-like under the hypothesis of zero signal; this however can create issues in channels with small backgrounds, since total expected yields and pdfs in each channel must be positive. When run with the a verbosity of 1, as the default, the program also prints out the best fit signal strengths in all channels; as the fit to all channels is done simultaneously, the correlation between the other systematical uncertainties is taken into account, and so these results can differ from the ones obtained fitting each channel separately. Below is an example output from combine, $ combine -M ChannelCompatibilityCheck comb_hww.txt -m 160 -n HWW <<< Combine >>> >>> including systematics >>> method used to compute upper limit is ChannelCompatibilityCheck >>> random number generator seed is 123456 Sanity checks on the model: OK Computing limit starting from observation --- ChannelCompatibilityCheck --- Nominal fit : r = 0.3431 -0.1408/+0.1636 Alternate fit: r = 0.4010 -0.2173/+0.2724 in channel hww_0jsf_shape Alternate fit: r = 0.2359 -0.1854/+0.2297 in channel hww_0jof_shape Alternate fit: r = 0.7669 -0.4105/+0.5380 in channel hww_1jsf_shape Alternate fit: r = 0.3170 -0.3121/+0.3837 in channel hww_1jof_shape Alternate fit: r = 0.0000 -0.0000/+0.5129 in channel hww_2j_cut Chi2-like compatibility variable: 2.16098 Done in 0.08 min (cpu), 0.08 min (real) The output tree will contain the value of the compatibility (chisquare variable) in the limit branch. If the option --saveFitResult is specified, the output root file contains also two RooFitResult objects fit_nominal and fit_alternate with the results of the two fits. This can be read and used to extract the best fit for each channel and the overall best fit using $ root -l TFile* _file0 = TFile::Open(\"higgsCombineTest.ChannelCompatibilityCheck.mH120.root\"); fit_alternate->floatParsFinal().selectByName(\"*ChannelCompatibilityCheck*\")->Print(\"v\"); fit_nominal->floatParsFinal().selectByName(\"r\")->Print(\"v\"); The macro cccPlot.cxx can be used to produce a comparison plot of the best fit signals from all channels.","title":"Channel Compatibility"},{"location":"part3/commonstatsmethods/#likelihood-fits-and-scans","text":"The MultiDimFit method can do multi-dimensional fits and likelihood based scans/contours using models with several parameters of interest. Taking a toy datacard test/multiDim/toy-hgg-125.txt (counting experiment which vaguely resembles the H\u2192\u03b3\u03b3 analysis at 125 GeV), we need to convert the datacard into a workspace with 2 parameters, ggH and qqH cross sections text2workspace.py toy-hgg-125.txt -m 125 -P HiggsAnalysis.CombinedLimit.PhysicsModel:floatingXSHiggs --PO modes=ggH,qqH A number of different algorithms can be used with the option --algo <algo> , none (default): Perform a maximum likelihood fit combine -M MultiDimFit toy-hgg-125.root ; The output root tree will contain two columns, one for each parameter, with the fitted values. singles : Perform a fit of each parameter separately, treating the others as unconstrained nuisances : combine -M MultiDimFit toy-hgg-125.root --algo singles --cl=0.68 . The output root tree will contain two columns, one for each parameter, with the fitted values; there will be one row with the best fit point (and quantileExpected set to -1) and two rows for each fitted parameter, where the corresponding column will contain the maximum and minimum of that parameter in the 68% CL interval, according to a one-dimensional chisquare (i.e. uncertainties on each fitted parameter do not increase when adding other parameters if they're uncorrelated). Note that if you run, for example, with --cminDefaultMinimizerStrategy=0 , these uncertainties will be derived from the Hessian, while --cminDefaultMinimizerStrategy=1 will invoke Minos to derive them. cross : Perform joint fit of all parameters: combine -M MultiDimFit toy-hgg-125.root --algo=cross --cl=0.68 . The output root tree will have one row with the best fit point, and two rows for each parameter, corresponding to the minimum and maximum of that parameter on the likelihood contour corresponding to the specified CL, according to a N-dimensional chisquare (i.e. uncertainties on each fitted parameter do increase when adding other parameters, even if they're uncorrelated). Note that the output of this way of running are not 1D uncertainties on each parameter, and shouldn't be taken as such. contour2d : Make a 68% CL contour a la minos combine -M MultiDimFit toy-hgg-125.root --algo contour2d --points=20 --cl=0.68 . The output will contain values corresponding to the best fit point (with quantileExpected set to -1) and for a set of points on the contour (with quantileExpected set to 1-CL, or something larger than that if the contour is hitting the boundary of the parameters). Probabilities are computed from the the n-dimensional \\chi^{2} \\chi^{2} distribution. For slow models, you can split it up by running several times with different number of points and merge the outputs (something better can be implemented). You can look at the contourPlot.cxx macro for how to make plots out of this algorithm. random : Scan N random points and compute the probability out of the profile likelihood combine -M MultiDimFit toy-hgg-125.root --algo random --points=20 --cl=0.68 . Again, best fit will have quantileExpected set to -1, while each random point will have quantileExpected set to the probability given by the profile likelihood at that point. fixed : Compare the log-likelihood at a fixed point compared to the best fit. combine -M MultiDimFit toy-hgg-125.root --algo fixed --fixedPointPOIs r=r_fixed,MH=MH_fixed . The output tree will contain the difference in the negative log-likelihood between the points ( \\hat{r},\\hat{m}_{H} \\hat{r},\\hat{m}_{H} ) and ( \\hat{r}_{fixed},\\hat{m}_{H,fixed} \\hat{r}_{fixed},\\hat{m}_{H,fixed} ) in the branch deltaNLL . grid : Scan on a fixed grid of points not with approximately N points in total. combine -M MultiDimFit toy-hgg-125.root --algo grid --points=10000 . You can partition the job in multiple tasks by using options --firstPoint and --lastPoint , for complicated scans, the points can be split as described in the combineTool for job submission section. The output file will contain a column deltaNLL with the difference in negative log likelihood with respect to the best fit point. Ranges/contours can be evaluated by filling TGraphs or TH2 histograms with these points. By default the \"min\" and \"max\" of the POI ranges are not included and the points which are in the scan are centered , eg combine -M MultiDimFit --algo grid --rMin 0 --rMax 5 --points 5 will scan at the points r=0.5, 1.5, 2.5, 3.5, 4.5 r=0.5, 1.5, 2.5, 3.5, 4.5 . You can instead include the option --alignEdges 1 which causes the points to be aligned with the endpoints of the parameter ranges - eg combine -M MultiDimFit --algo grid --rMin 0 --rMax 5 --points 6 --alignEdges 1 will now scan at the points r=0, 1, 2, 3, 4, 5 r=0, 1, 2, 3, 4, 5 . NB - the number of points must be increased by 1 to ensure both end points are included. With the algorithms none and singles you can save the RooFitResult from the initial fit using the option --saveFitResult . The fit result is saved into a new file called muiltidimfit.root . As usual, any floating nuisance parameters will be profiled which can be turned of using the --freezeParameters option. For most of the methods, for lower precision results you can turn off the profiling of the nuisances setting option --fastScan , which for complex models speeds up the process by several orders of magnitude. All nuisance parameters will be kept fixed at the value corresponding to the best fit point. As an example, lets produce the -2\\Delta\\ln{\\mathcal{L}} -2\\Delta\\ln{\\mathcal{L}} scan as a function of r_ggH and r_qqH from the toy H\u2192\u03b3\u03b3 datacard, with the nuisance parameters fixed to their global best fit values. combine toy-hgg-125.root -M MultiDimFit --algo grid --points 2000 --setParameterRanges r_qqH=0,10:r_ggH=0,4 -m 125 --fastScan Show output < < < Combine >>> >>> including systematics >>> method used is MultiDimFit >>> random number generator seed is 123456 ModelConfig 'ModelConfig' defines more than one parameter of interest. This is not supported in some statistical methods. Set Range of Parameter r_qqH To : (0,10) Set Range of Parameter r_ggH To : (0,4) Computing results starting from observation (a-posteriori) POI: r_ggH= 0.88152 -> [0,4] POI: r_qqH= 4.68297 -> [0,10] Point 0/2025, (i,j) = (0,0), r_ggH = 0.044444, r_qqH = 0.111111 Point 11/2025, (i,j) = (0,11), r_ggH = 0.044444, r_qqH = 2.555556 Point 22/2025, (i,j) = (0,22), r_ggH = 0.044444, r_qqH = 5.000000 Point 33/2025, (i,j) = (0,33), r_ggH = 0.044444, r_qqH = 7.444444 Point 55/2025, (i,j) = (1,10), r_ggH = 0.133333, r_qqH = 2.333333 Point 66/2025, (i,j) = (1,21), r_ggH = 0.133333, r_qqH = 4.777778 Point 77/2025, (i,j) = (1,32), r_ggH = 0.133333, r_qqH = 7.222222 Point 88/2025, (i,j) = (1,43), r_ggH = 0.133333, r_qqH = 9.666667 Point 99/2025, (i,j) = (2,9), r_ggH = 0.222222, r_qqH = 2.111111 Point 110/2025, (i,j) = (2,20), r_ggH = 0.222222, r_qqH = 4.555556 Point 121/2025, (i,j) = (2,31), r_ggH = 0.222222, r_qqH = 7.000000 Point 132/2025, (i,j) = (2,42), r_ggH = 0.222222, r_qqH = 9.444444 Point 143/2025, (i,j) = (3,8), r_ggH = 0.311111, r_qqH = 1.888889 Point 154/2025, (i,j) = (3,19), r_ggH = 0.311111, r_qqH = 4.333333 Point 165/2025, (i,j) = (3,30), r_ggH = 0.311111, r_qqH = 6.777778 Point 176/2025, (i,j) = (3,41), r_ggH = 0.311111, r_qqH = 9.222222 Point 187/2025, (i,j) = (4,7), r_ggH = 0.400000, r_qqH = 1.666667 Point 198/2025, (i,j) = (4,18), r_ggH = 0.400000, r_qqH = 4.111111 Point 209/2025, (i,j) = (4,29), r_ggH = 0.400000, r_qqH = 6.555556 Point 220/2025, (i,j) = (4,40), r_ggH = 0.400000, r_qqH = 9.000000 [...] Done in 0.00 min (cpu), 0.02 min (real) The scan, along with the best fit point can be drawn using root, $ root -l higgsCombineTest.MultiDimFit.mH125.root limit->Draw(\"2*deltaNLL:r_ggH:r_qqH>>h(44,0,10,44,0,4)\",\"2*deltaNLL<10\",\"prof colz\") limit->Draw(\"r_ggH:r_qqH\",\"quantileExpected == -1\",\"P same\") TGraph *best_fit = (TGraph*)gROOT->FindObject(\"Graph\") best_fit->SetMarkerSize(3); best_fit->SetMarkerStyle(34); best_fit->Draw(\"p same\") To make the full profiled scan just remove the --fastScan option from the combine command. Similarly, 1D scans can be drawn directly from the tree, however for 1D likelihood scans, there is a python script from the CombineHarvester/CombineTools package plot1DScan.py which can be used to make plots and extract the crossings of the 2*deltaNLL - e.g the 1\u03c3/2\u03c3 boundaries.","title":"Likelihood Fits and Scans"},{"location":"part3/commonstatsmethods/#useful-options-for-likelihood-scans","text":"A number of common, useful options (especially for computing likelihood scans with the grid algo) are, --autoBoundsPOIs arg : Adjust bounds for the POIs if they end up close to the boundary. This can be a comma separated list of POIs, or \"*\" to get all of them. --autoMaxPOIs arg : Adjust maxima for the POIs if they end up close to the boundary. Can be a list of POIs, or \"*\" to get all. --autoRange X : Set to any X >= 0 to do the scan in the \\hat{p} \\hat{p} \\pm \\pm X\u03c3 range, where \\hat{p} \\hat{p} and \u03c3 are the best fit parameter value and uncertainty from the initial fit (so it may be fairly approximate). In case you do not trust the estimate of the error from the initial fit, you can just centre the range on the best fit value by using the option --centeredRange X to do the scan in the \\hat{p} \\hat{p} \\pm \\pm X range centered on the best fit value. --squareDistPoiStep : POI step size based on distance from midpoint ( either (max-min)/2 or the best fit if used with --autoRange or --centeredRange ) rather than linear separation. --skipInitialFit : Skip the initial fit (saves time if for example a snapshot is loaded from a previous fit) Below is a comparison in a likelihood scan, with 20 points, as a function of r_qqH with our toy-hgg-125.root workspace with and without some of these options. The options added tell combine to scan more points closer to the minimum (best-fit) than with the default. You may find it useful to use the --robustFit=1 option to turn on robust (brute-force) for likelihood scans (and other algorithms). You can set the strategy and tolerance when using the --robustFit option using the options --setRobustFitAlgo (default is Minuit2,migrad ), setRobustFitStrategy (default is 0) and --setRobustFitTolerance (default is 0.1). If these options are not set, the defaults (set using cminDefaultMinimizerX options) will be used. If running --robustFit=1 with the algo singles , you can tune the accuracy of the routine used to find the crossing points of the likelihood using the option --setCrossingTolerance (default is set to 0.0001) If you suspect your fits/uncertainties are not stable, you may also try to run custom HESSE-style calculation of the covariance matrix. This is enabled by running MultiDimFit with the --robustHesse=1 option. A simple example of how the default behaviour in a simple datacard is given here . For a full list of options use combine -M MultiDimFit --help","title":"Useful options for likelihood scans"},{"location":"part3/commonstatsmethods/#fitting-only-some-parameters","text":"If your model contains more than one parameter of interest, you can still decide to fit a smaller number of them, using the option --parameters (or -P ), with a syntax like this: combine -M MultiDimFit [...] -P poi1 -P poi2 ... --floatOtherPOIs=(0|1) If --floatOtherPOIs is set to 0, the other parameters of interest (POIs), which are not included as a -P option, are kept fixed to their nominal values. If it's set to 1, they are kept floating , which has different consequences depending on algo : When running with --algo=singles , the other floating POIs are treated as unconstrained nuisance parameters. When running with --algo=cross or --algo=contour2d , the other floating POIs are treated as other POIs, and so they increase the number of dimensions of the chi-square. As a result, when running with floatOtherPOIs set to 1, the uncertainties on each fitted parameters do not depend on what's the selection of POIs passed to MultiDimFit, but only on the number of parameters of the model. Info Note that poi given to the the option -P can also be any nuisance parameter. However, by default, the other nuisance parameters are left floating , so you do not need to specify that. You can save the values of the other parameters of interest in the output tree by adding the option saveInactivePOI=1 . You can additionally save the post-fit values any nuisance parameter, function or discrete index (RooCategory) defined in the workspace using the following options; --saveSpecifiedNuis=arg1,arg2,... will store the fitted value of any specified constrained nuisance parameter. Use all to save every constrained nuisance parameter. Note that if you want to store the values of flatParams (or floating parameters which are not defined in the datacard) or rateParams , which are unconstrained , you should instead use the generic option --trackParameters as described here . --saveSpecifiedFunc=arg1,arg2,... will store the value of any function (eg RooFormulaVar ) in the model. --saveSpecifiedIndex=arg1,arg2,... will store the index of any RooCategory object - eg a discrete nuisance.","title":"Fitting only some parameters"},{"location":"part3/commonstatsmethods/#using-best-fit-snapshots","text":"This can be used to save time when performing scans so that the best-fit needs not be redone and can also be used to perform scans with some nuisances frozen to the best-fit values. Sometimes it is useful to scan freezing certain nuisances to their best-fit values as opposed to the default values. To do this here is an example, Create a workspace workspace for a floating r,m_{H} r,m_{H} fit text2workspace.py hgg_datacard_mva_8TeV_bernsteins.txt -m 125 -P HiggsAnalysis.CombinedLimit.PhysicsModel:floatingHiggsMass --PO higgsMassRange=120,130 -o testmass.root` Perfom the fit, saving the workspace combine -m 123 -M MultiDimFit --saveWorkspace -n teststep1 testmass.root --verbose 9 Now we can load the best-fit \\hat{r},\\hat{m}_{H} \\hat{r},\\hat{m}_{H} and fit for r r freezing m_{H} m_{H} and lumi_8TeV to the best-fit values, combine -m 123 -M MultiDimFit -d higgsCombineteststep1.MultiDimFit.mH123.root -w w --snapshotName \"MultiDimFit\" -n teststep2 --verbose 9 --freezeParameters MH,lumi_8TeV","title":"Using best fit snapshots"},{"location":"part3/commonstatsmethods/#feldman-cousins","text":"The Feldman-Cousins (FC) procedure for computing confidence intervals for a generic model is, use the profile likelihood as the test-statistic q(x) = - 2 \\ln \\mathcal{L}(\\mathrm{data}|x,\\hat{\\theta}_{x})/\\mathcal{L}(\\mathrm{data}|\\hat{x},\\hat{\\theta}) q(x) = - 2 \\ln \\mathcal{L}(\\mathrm{data}|x,\\hat{\\theta}_{x})/\\mathcal{L}(\\mathrm{data}|\\hat{x},\\hat{\\theta}) where x x is a point in the (N-dimensional) parameter space, and \\hat{x} \\hat{x} is the point corresponding to the best fit. In this test-statistic, the nuisance parameters are profiled, separately both in the numerator and denominator. for each point x x : compute the observed test statistic q_{\\mathrm{obs}}(x) q_{\\mathrm{obs}}(x) compute the expected distribution of q(x) q(x) under the hypothesis of x x as the true value. accept the point in the region if p_{x}=P\\left[q(x) > q_{\\mathrm{obs}}(x)| x\\right] > \\alpha p_{x}=P\\left[q(x) > q_{\\mathrm{obs}}(x)| x\\right] > \\alpha With a critical value \\alpha \\alpha . In combine , you can perform this test on each individual point ( param1, param2,... ) = ( value1,value2,... ) by doing, combine workspace.root -M HybridNew --LHCmode LHC-feldman-cousins --clsAcc 0 --singlePoint param1=value1,param2=value2,param3=value3,... --saveHybridResult [Other options for toys, iterations etc as with limits] The point belongs to your confidence region if p_{x} p_{x} is larger than \\alpha \\alpha (e.g. 0.3173 for a 1\u03c3 region, 1-\\alpha=0.6827 1-\\alpha=0.6827 ). Warning You should not use this method without the option --singlePoint . Although combine will not complain, the algorithm to find the crossing will only find a single crossing and therefore not find the correct interval. Instead you should calculate the Feldman-Cousins intervals as described above.","title":"Feldman Cousins"},{"location":"part3/commonstatsmethods/#physical-boundaries","text":"Imposing physical boundaries (such as requiring \\mu>0 \\mu>0 for a signal strength) is achieved by setting the ranges of the physics model parameters using --setParameterRanges param1=param1_min,param1_max:param2=param2_min,param2_max .... The boundary is imposed by restricting the parameter range(s) to those set by the user, in the fits. Note that this is a trick! The actual fitted value, as one of an ensemble of outcomes, can fall outside of the allowed region, while the boundary should be imposed on the physical parameter. The effect of restricting the parameter value in the fit is such that the test-statistic is modified as follows ; q(x) = - 2 \\ln \\mathcal{L}(\\mathrm{data}|x,\\hat{\\theta}_{x})/\\mathcal{L}(\\mathrm{data}|\\hat{x},\\hat{\\theta}) q(x) = - 2 \\ln \\mathcal{L}(\\mathrm{data}|x,\\hat{\\theta}_{x})/\\mathcal{L}(\\mathrm{data}|\\hat{x},\\hat{\\theta}) , if \\hat{x} \\hat{x} in contained in the bounded range and, q(x) = - 2 \\ln \\mathcal{L}(\\mathrm{data}|x,\\hat{\\theta}_{x})/\\mathcal{L}(\\mathrm{data}|x_{B},\\hat{\\theta}_{B}) q(x) = - 2 \\ln \\mathcal{L}(\\mathrm{data}|x,\\hat{\\theta}_{x})/\\mathcal{L}(\\mathrm{data}|x_{B},\\hat{\\theta}_{B}) , if \\hat{x} \\hat{x} is outside of the bounded range. Here x_{B} x_{B} and \\hat{\\theta}_{B} \\hat{\\theta}_{B} are the values of x x and \\theta \\theta which maximise the likelihood excluding values outside of the bounded region for x x - typically, x_{B} x_{B} will be found at one of the boundaries which is imposed. For example, if the boundary x>0 x>0 is imposed, you will typically expect x_{B}=0 x_{B}=0 , when \\hat{x}\\leq 0 \\hat{x}\\leq 0 , and x_{B}=\\hat{x} x_{B}=\\hat{x} otherewise. This can sometimes be an issue as Minuit may not know if has successfully converged when the minimum lies outside of that range. If there is no upper/lower boundary, just set that value to something far from the region of interest. Info One can also imagine imposing the boundaries by first allowing Minuit to find the minimum in the un-restricted region and then setting the test-statistic to that above in the case that minimum lies outside the physical boundary. This would avoid potential issues of convergence - If you are interested in implementing this version in combine, please contact the development team. As in general for HybridNew , you can split the task into multiple tasks (grid and/or batch) and then merge the outputs, as described in the combineTool for job submission section.","title":"Physical boundaries"},{"location":"part3/commonstatsmethods/#extracting-contours","text":"As in general for HybridNew , you can split the task into multiple tasks (grid and/or batch) and then merge the outputs with hadd . You can also refer to the combineTool for job submission section for submitting the jobs to the grid/batch.","title":"Extracting contours"},{"location":"part3/commonstatsmethods/#1d-intervals","text":"For one-dimensional models only, and if the parameter behaves like a cross-section, the code is somewhat able to do interpolation and determine the values of your parameter on the contour (just like it does for the limits). As with limits, read in the grid of points and extract 1D intervals using, combine workspace.root -M HybridNew --LHCmode LHC-feldman-cousins --readHybridResults --grid=mergedfile.root --cl <1-alpha> The output tree will contain the values of the POI which crosses the critical value ( \\alpha \\alpha ) - i.e, the boundaries of the confidence intervals, You can produce a plot of the value of p_{x} p_{x} vs the parameter of interest x x by adding the option --plot <plotname> .","title":"1D intervals"},{"location":"part3/commonstatsmethods/#2d-contours","text":"There is a tool for extracting 2D contours from the output of HybridNew located in test/makeFCcontour.py provided the option --saveHybridResult was included when running HybridNew . It can be run with the usual combine output files (or several of them) as input, ./test/makeFCcontour.py toysfile1.root toysfile2.root .... [options] -out outputfile.root To extract 2D contours, the names of each parameter must be given --xvar poi_x --yvar poi_y . The output will be a root file containing a 2D histogram of value of p_{x,y} p_{x,y} for each point (x,y) (x,y) which can be used to draw 2D contours. There will also be a histogram containing the number of toys found for each point. There are several options for reducing the running time (such as setting limits on the region of interest or the minimum number of toys required for a point to be included) Finally, adding the option --storeToys in this python tool will add histograms for each point to the output file of the test-statistic distribution. This will increase the momory usage however as all of the toys will be stored in memory.","title":"2D contours"},{"location":"part3/debugging/","text":"Debugging fits When a fit fails there are several things you can do to investigate. Have a look at these slides from the combine tutorial. This section contains a few pointers for some of the methods mentioned in the slides. Analyzing the NLL shape in each parameter The FastScan mode of combineTool.py can be used to analyze the shape of the NLL as a function of each parameter in the fit model. The NLL is evaluated varying a single parameter at a time, the other parameters stay at the default values they have in the workspace. This produces a file with the NLL, plus its first and second derivatives, as a function of each parameter. Discontinuities in the derivatives, particularly if they are close to the minimum of the parameter, can be the source of issues with the fit. The usage is as follows: combineTool.py -M FastScan -w workspace.root:w Note that this will make use of the data in the workspace for evaluating the NLL. To run this on an asimov data set, with r=1 injected, you can do the following: combine -M GenerateOnly workspace.root -t -1 --saveToys --setParameters r=1 combineTool.py -M FastScan -w workspace.root:w -d higgsCombineTest.GenerateOnly.mH120.123456.root:toys/toy_asimov higgsCombineTest.GenerateOnly.mH120.123456.root is generated by the first command; if you pass a value for -m or change the default output file name with -n the file name will be different and you should change the combineTool call accordingly.","title":"Debugging fit failures"},{"location":"part3/debugging/#debugging-fits","text":"When a fit fails there are several things you can do to investigate. Have a look at these slides from the combine tutorial. This section contains a few pointers for some of the methods mentioned in the slides.","title":"Debugging fits"},{"location":"part3/debugging/#analyzing-the-nll-shape-in-each-parameter","text":"The FastScan mode of combineTool.py can be used to analyze the shape of the NLL as a function of each parameter in the fit model. The NLL is evaluated varying a single parameter at a time, the other parameters stay at the default values they have in the workspace. This produces a file with the NLL, plus its first and second derivatives, as a function of each parameter. Discontinuities in the derivatives, particularly if they are close to the minimum of the parameter, can be the source of issues with the fit. The usage is as follows: combineTool.py -M FastScan -w workspace.root:w Note that this will make use of the data in the workspace for evaluating the NLL. To run this on an asimov data set, with r=1 injected, you can do the following: combine -M GenerateOnly workspace.root -t -1 --saveToys --setParameters r=1 combineTool.py -M FastScan -w workspace.root:w -d higgsCombineTest.GenerateOnly.mH120.123456.root:toys/toy_asimov higgsCombineTest.GenerateOnly.mH120.123456.root is generated by the first command; if you pass a value for -m or change the default output file name with -n the file name will be different and you should change the combineTool call accordingly.","title":"Analyzing the NLL shape in each parameter"},{"location":"part3/nonstandard/","text":"Advanced Use Cases This section will cover some of the more specific use cases for combine which are not necessarily related to the main statistics results. Fitting Diagnostics You may first want to look at the HIG PAG standard checks applied to all datacards if you want to diagnose your limit setting/fitting results which can be found here If you have already found the higgs boson but it's an exotic one, instead of computing a limit or significance you might want to extract it's cross section by performing a maximum-likelihood fit. Or, more seriously, you might want to use this same package to extract the cross section of some other process (e.g. the di-boson production). Or you might want to know how well the data compares to you model, e.g. how strongly it constraints your other nuisance parameters, what's their correlation, etc. These general diagnostic tools are contained in the method FitDiagnostics . combine -M FitDiagnostics datacard.txt The program will print out the result of the two fits performed with signal strength r (or first POI in the list) set to zero and a second with floating r . The output root tree will contain the best fit value for r and it's uncertainty. You will also get a fitDiagnostics.root file containing the following objects: Object Description nuisances_prefit RooArgSet containing the pre-fit values of the nuisance parameters, and their uncertainties from the external constraint terms only fit_b RooFitResult object containing the outcome of the fit of the data with signal strength set to zero fit_s RooFitResult object containing the outcome of the fit of the data with floating signal strength tree_prefit TTree of pre-fit nuisance parameter values and constraint terms (_In) tree_fit_sb TTree of fitted nuisance parameter values and constraint terms (_In) with floating signal strength tree_fit_b TTree of fitted nuisance parameter values and constraint terms (_In) with signal strength set to 0 by including the option --plots , you will additionally find the following contained in the root file . Object Description covariance_fit_s TH2D Covariance matrix of the parameters in the fit with floating signal strength covariance_fit_b TH2D Covariance matrix of the parameters in the fit with signal strength set to zero category_variable_prefit RooPlot plot of the prefit pdfs with the data (or toy if running with -t overlaid) category_variable_fit_b RooPlot plot of the pdfs from the background only fit with the data (or toy if running with -t overlaid) category_variable_fit_s RooPlot plot of the pdfs from the signal+background fit with the data (or toy if running with -t overlaid) where for the RooPlot objects, you will get one per category in the likelihood and one per variable if using a multi-dimensional dataset. You will also get a png file for each of these additional objects. Info If you use the option --name this name will be inserted into the file name for this output file too. As well as values of the constrained nuisance parameters (and their constraint values) in the toys, you will also find branches for the number of \"bad\" nll calls (which you should check is not too large) and the status of the fit fit_status . The fit status is computed as follows fit_status = 100 * hesse_status + 10 * minos_status + minuit_summary_status The minuit_summary_status is the usual status from Minuit, details of which can be found here . For the other status values, check these documentation links for the hesse_status and the minos_status . A fit status of -1 indicates that the fit failed (Minuit summary was not 0 or 1) and hence the fit is not valid. Fit options If you need only the signal+background fit, you can run with --justFit . This can be useful if the background-only fit is not interesting or not converging (e.g. if the significance of your signal is very very large) You can use --rMin and --rMax to set the range of the first POI; a range that is not too large compared to the uncertainties you expect from the fit usually gives more stable and accurate results. By default, the uncertainties are computed using MINOS for the first POI and HESSE for all other parameters (and hence they will be symmetric for the nuisance parameters). You can run MINOS for all parameters using the option --minos all , or for none of the parameters using --minos none . Note that running MINOS is slower so you should only consider using it if you think the HESSE uncertainties are not accurate. If MINOS or HESSE fails to converge, you can try running with --robustFit=1 that will do a slower but more robust likelihood scan; this can be further controlled by the parameter --stepSize (the default is 0.1, and is relative to the range of the parameter) You can set the strategy and tolerance when using the --robustFit option using the options setRobustFitAlgo (default is Minuit2,migrad ), setRobustFitStrategy (default is 0) and --setRobustFitTolerance (default is 0.1). If these options are not set, the defaults (set using cminDefaultMinimizerX options) will be used. You can also tune the accuracy of the routine used to find the crossing points of the likelihood using the option --setCrossingTolerance (default is set to 0.0001) If you find the covariance matrix provided by HESSE is not accurate (i.e. fit_s->Print() reports this was forced positive-definite) then a custom HESSE-style calculation of the covariance matrix can be used instead. This is enabled by running FitDiagnostics with the --robustHesse 1 option. Please note that the status reported by RooFitResult::Print() will contain covariance matrix quality: Unknown, matrix was externally provided when robustHesse is used, this is normal and does not indicate a problem. NB: one feature of the robustHesse algorithm is that if it still cannot calculate a positive-definite covariance matrix it will try to do so by dropping parameters from the hessian matrix before inverting. If this happens it will be reported in the output to the screen. For other fitting options see the generic minimizer options section. Fit parameter uncertainties If you get a warning message when running FitDiagnostics which says Unable to determine uncertainties on all fit parameters . This means the covariance matrix calculated in FitDiagnostics was not correct. The most common problem is that the covariance matrix is forced positive-definite. In this case the constraints on fit parameters as taken from the covariance matrix are incorrect and should not be used. In particular, if you want to make post-fit plots of the distribution used in the signal extraction fit and are extracting the uncertainties on the signal and background expectations from the covariance matrix, the resulting values will not reflect the truth if the covariance matrix was incorrect. By default if this happens and you passed the --saveWithUncertainties flag when calling FitDiagnostics , this option will be ignored as calculating the uncertainties would lead to incorrect results. This behaviour can be overridden by passing --ignoreCovWarning . Such problems with the covariance matrix can be caused by a number of things, for example: Parameters being close to their boundaries after the fit. Strong (anti-) correlations between some parameters. A discontinuity in the NLL function or its derivatives at or near the minimum. If you are aware that your analysis has any of these features you could try resolving these. Setting --cminDefaultMinimizerStrategy 0 can also help with this problem. Pre and post fit nuisance parameters and pulls It is possible to compare pre-fit and post-fit nuisance parameters with the script diffNuisances.py . Taking as input a fitDiagnostics.root file, the script will by default print out the parameters which have changed significantly w.r.t. their initial estimate. For each of those parameters, it will print out the shift in value and the post-fit uncertainty, both normalized to the input values, and the linear correlation between the parameter and the signal strength. python diffNuisances.py fitDiagnostics.root The script has several options to toggle the thresholds used to decide if a parameter has changed significantly, to get the printout of the absolute value of the nuisance parameters, and to get the output in another format for easy cut-n-paste (supported formats are html , latex , twiki ). To print all of the parameters, use the option --all . The output by default will be the changes in the nuisance parameter values and uncertainties, relative to their initial (pre-fit) values (usually relative to initial values of 0 and 1 for most nuisance types). The values in the output will be (\\theta-\\theta_{I})/\\sigma_{I} (\\theta-\\theta_{I})/\\sigma_{I} if the nuisance has a pre-fit uncertainty, otherwise it will be \\theta-\\theta_{I} \\theta-\\theta_{I} if not (eg, a flatParam has no pre-fit uncertainty). The uncertainty reported will be the ratio \\sigma/\\sigma_{I} \\sigma/\\sigma_{I} - i.e the ratio of the post-fit to the pre-fit uncertainty. If there is no pre-fit uncertainty (as for flatParam nuisances) then the post-fit uncertainty is shown. With the option --abs , instead the pre-fit and post-fit values and (asymmetric) uncertainties will be reported in full. Info We recommend you include the options --abs and --all to get the full information on all of the parameters (including unconstrained nuisance parameters) at least once when checking your datacards. If instead of the plain values, you wish to report the pulls , you can do so with the option --pullDef X with X being one of the following options; You should note that since the pulls below are only defined when the pre-fit uncertainty exists, nothing will be reported for parameters which have no prior constraint (except in the case of the unconstPullAsym choice as described below). You may want to run without this option and --all to get information on those parameters. relDiffAsymErrs : This is the same as the default output of the tool except that only constrained parameters (pre-fit uncertainty defined) are reported. The error is also reported and calculated as \\sigma/\\sigma_{I} \\sigma/\\sigma_{I} . unconstPullAsym : Report the pull as \\frac{\\theta-\\theta_{I}}{\\sigma} \\frac{\\theta-\\theta_{I}}{\\sigma} where \\theta_{I} \\theta_{I} and \\sigma \\sigma are the initial value and post-fit uncertainty of that nuisance parameter. The pull defined in this way will have no error bar, but all nuisance parameters will have a result in this case. compatAsym : The pull is defined as \\frac{\\theta-\\theta_{D}}{\\sqrt{\\sigma^{2}+\\sigma_{D}^{2}}} \\frac{\\theta-\\theta_{D}}{\\sqrt{\\sigma^{2}+\\sigma_{D}^{2}}} , where \\theta_{D} \\theta_{D} and \\sigma_{D} \\sigma_{D} are calculated as \\sigma_{D} = (\\frac{1}{\\sigma^{2}} - \\frac{1}{\\sigma_{I}^{2}})^{-1} \\sigma_{D} = (\\frac{1}{\\sigma^{2}} - \\frac{1}{\\sigma_{I}^{2}})^{-1} and \\theta_{D} = \\sigma_{D}(\\theta - \\frac{\\theta_{I}}{\\sigma_{I}^{2}}) \\theta_{D} = \\sigma_{D}(\\theta - \\frac{\\theta_{I}}{\\sigma_{I}^{2}}) , where \\theta_{I} \\theta_{I} and \\sigma_{I} \\sigma_{I} are the initial value and uncertainty of that nuisance parameter. This can be thought of as a compatibility between the initial measurement (prior) an imagined measurement where only the data (with no constraint) is used to measure the nuisance parameter. There is no error bar associated to this value. diffPullAsym : The pull is defined as \\frac{\\theta-\\theta_{I}}{\\sqrt{\\sigma_{I}^{2}-\\sigma^{2}}} \\frac{\\theta-\\theta_{I}}{\\sqrt{\\sigma_{I}^{2}-\\sigma^{2}}} , where \\theta_{I} \\theta_{I} and \\sigma_{I} \\sigma_{I} are the pre-fit value and uncertainty (from L. Demortier and L. Lyons ). If the denominator is close to 0 or the post-fit uncertainty is larger than the pre-fit (usually due to some failure in the calculation), the pull is not defined and the result will be reported as 0 +/- 999 . If using --pullDef , the results for all parameters for which the pull can be calculated will be shown (i.e --all will be set to true ), not just those which have moved by some metric. This script has the option ( -g outputfile.root ) to produce plots of the fitted values of the nuisance parameters and their post-fit, asymmetric uncertainties. Instead, the pulls defined using one of the options above, can be plotted using the option --pullDef X . In addition this will produce a plot showing directly a comparison of the post-fit to pre-fit nuisance (symmetrized) uncertainties. Info In the above options, if an asymmetric uncertainty is associated to the nuisance parameter, then the choice of which uncertainty is used in the definition of the pull will depend on the sign of \\theta-\\theta_{I} \\theta-\\theta_{I} . Normalizations For a certain class of models, like those made from datacards for shape-based analysis, the tool can also compute and save to the output root file the best fit yields of all processes. If this feature is turned on with the option --saveNormalizations , the file will also contain three RooArgSet norm_prefit , norm_fit_s , norm_fit_b objects each containing one RooConstVar for each channel xxx and process yyy with name xxx/yyy and value equal to the best fit yield. You can use RooRealVar::getVal and RooRealVar::getError to estimate both the post-(or pre-)fit values and uncertainties of these normalisations. The sample pyroot macro mlfitNormsToText.py can be used to convert the root file into a text table with four columns: channel, process, yield from the signal+background fit and yield from the background-only fit. To include the uncertainties in the table, add the option --uncertainties Warning Note that when running with multiple toys, the norm_fit_s , norm_fit_b and norm_prefit objects will be stored for the last toy dataset generated and so may not be useful to you. Note that this procedure works only for \"extended likelihoods\" like the ones used in shape-based analysis, not for the cut-and-count datacards. You can however convert a cut-and-count datacard in an equivalent shape-based one by adding a line shapes * * FAKE in the datacard after the imax , jmax , kmax or using combineCards.py countingcard.txt -S > shapecard.txt . Per-bin norms for shape analyses If you have a shape based analysis, you can also (instead) include the option --savePredictionsPerToy . With this option, additional branches will be filled in the three output trees contained in fitDiagnostics.root . The normalisation values for each toy will be stored in the branches inside the TTrees named n_exp[_final]_binxxx_proc_yyy . The _final will only be there if there are systematics affecting this process. Additionally, there will be filled branches which provide the value of the expected bin content for each process, in each channel. These will are named as n_exp[_final]_binxxx_proc_yyy_i (where _final will only be in the name if there are systematics affecting this process) for channel xxx , process yyy bin number i . In the case of the post-fit trees ( tree_fit_s/b ), these will be resulting expectations from the fitted models, while for the pre-fit tree, they will be the expectation from the generated model (i.e if running toys with -t N and using --genNuisances , they will be randomised for each toy). These can be useful, for example, for calculating correlations/covariances between different bins, in different channels or processes, within the model from toys. Info Be aware that for unbinned models, a binning scheme is adopted based on the RooRealVar::getBinning for the observable defining the shape, if it exists, or combine will adopt some appropriate binning for each observable. Plotting FitDiagnostics can also produce pre- and post-fit plots the model in the same directory as fitDiagnostics.root along with the data. To get them, you have to specify the option --plots , and then optionally specify what are the names of the signal and background pdfs, e.g. --signalPdfNames='ggH*,vbfH*' and --backgroundPdfNames='*DY*,*WW*,*Top*' (by default, the definitions of signal and background are taken from the datacard). For models with more than 1 observable, a separate projection onto each observable will be produced. An alternative is to use the options --saveShapes . The result will be additional folders in fitDiagnostics.root for each category, with pre and post-fit distributions of the signals and backgrounds as TH1s and the data as TGraphAsymmErrors (with Poisson intervals as error bars). Info If you want to save post-fit shapes at a specific r value, add the options --customStartingPoint and --skipSBFit , and set the r value. The result will appear in shapes_fit_b , as described below. Three additional folders ( shapes_prefit , shapes_fit_sb and shapes_fit_b ) will contain the following distributions, Object Description data TGraphAsymmErrors containing the observed data (or toy data if using -t ). The vertical error bars correspond to the 68% interval for a Poisson distribution centered on the observed count. $PROCESS (id <= 0) TH1F for each signal process in channel, named as in the datacard $PROCESS (id > 0) TH1F for each background process in channel, named as in the datacard total_signal TH1F Sum over the signal components total_background TH1F Sum over the background components total TH1F Sum over all of the signal and background components The above distributions are provided for each channel included in the datacard , in separate sub-folders, named as in the datacard: There will be one sub-folder per channel. Warning The pre-fit signal is by default for r=1 but this can be modified using the option --preFitValue . The distributions and normalisations are guaranteed to give the correct interpretation: For shape datacards whose inputs are TH1, the histograms/data points will have the bin number as the x-axis and the content of each bin will be a number of events. For datacards whose inputs are RooAbsPdf/RooDataHists, the x-axis will correspond to the observable and the bin content will be the PDF density / events divided by the bin width. This means the absolute number of events in a given bin, i, can be obtained from h.GetBinContent(i)*h.GetBinWidth(i) or similar for the data graphs. Note that for unbinned analyses combine will make a reasonable guess as to an appropriate binning. Uncertainties on the shapes will be added with the option --saveWithUncertainties . These uncertainties are generated by re-sampling of the fit covariance matrix, thereby accounting for the full correlation between the parameters of the fit. Warning It may be tempting to sum up the uncertainties in each bin (in quadrature) to get the total uncertainty on a process however, this is (usually) incorrect as doing so would not account for correlations between the bins . Instead you can refer to the uncertainties which will be added to the post-fit normalizations described above. Additionally, the covariance matrix between bin yields (or yields/bin-widths) in each channel will also be saved as a TH2F named total_covar . If the covariance between all bins across all channels is desired, this can be added using the option --saveOverallShapes . Each folder will now contain additional distributions (and covariance matrices) corresponding to the concatenation of the bins in each channel (and therefore the covaraince between every bin in the analysis). The bin labels should make it clear as to which bin corresponds to which channel. Toy-by-toy diagnostics FitDiagnostics can also be used to diagnose the fitting procedure in toy experiments to identify potentially problematic nuisance parameters when running the full limits/p-values. This can be done by adding the option -t <num toys> . The output file, fitDiagnostics.root the three TTrees will contain the value of the constraint fitted result in each toy, as a separate entry. It is recommended to use the following options when investigating toys to reduce the running time: --toysFrequentist --noErrors --minos none The results can be plotted using the macro test/plotParametersFromToys.C $ root -l .L plotParametersFromToys.C+ plotParametersFomToys(\"fitDiagnosticsToys.root\",\"fitDiagnosticsData.root\",\"workspace.root\",\"r<0\") The first argument is the name of the output file from running with toys, and the second and third (optional) arguments are the name of the file containing the result from a fit to the data and the workspace (created from text2workspace.py ). The fourth argument can be used to specify a cut string applied to one of the branches in the tree which can be used to correlate strange behaviour with specific conditions. The output will be 2 pdf files ( tree_fit_(s)b.pdf ) and 2 root files ( tree_fit_(s)b.root ) containing canvases of the fit results of the tool. For details on the output plots, consult AN-2012/317 . Scaling constraints It possible to scale the constraints on the nuisance parameters when converting the datacard to a workspace (see the section on physics models ) with text2workspace.py . This can be useful for projection studies of the analysis to higher luminosities or with different assumptions about the sizes of certain systematics without changing the datacard by hand. We consider two kinds of scaling; A constant scaling factor to scale the constraints A functional scale factor that depends on some other parameters in the workspace, eg a luminosity scaling parameter (as a rateParam affecting all processes). In both cases these scalings can be introduced by adding some extra options at the text2workspace.py step. To add a constant scaling factor we use the option --X-rescale-nuisance , eg text2workspace.py datacard.txt --X-rescale-nuisance '[some regular expression]' 0.5 will create the workspace in which ever nuisance parameter whose name matches the specified regular expression will have the width of the gaussian constraint scaled by a factor 0.5. Multiple --X-rescale-nuisance options can be specified to set different scalings for different nuisances (note that you actually have to write --X-rescale-nuisance each time as in --X-rescale-nuisance 'theory.*' 0.5 --X-rescale-nuisance 'exp.*' 0.1 ). To add a functional scaling factor we use the option --X-nuisance-function , which works in a similar way. Instead of a constant value you should specify a RooFit factory expression. A typical case would be scaling by 1/\\sqrt{L} 1/\\sqrt{L} , where L L is a luminosity scale factor eg assuming there is some parameter in the datacard/workspace called lumiscale , text2workspace.py datacard.txt --X-nuisance-function '[some regular expression]' 'expr::lumisyst(\"1/sqrt(@0)\",lumiscale[1])' This factory syntax is quite flexible, but for our use case the typical format will be: expr::[function name](\"[formula]\", [arg0], [arg1], ...) . The arg0 , arg1 ... are represented in the formula by @0 , @1 ,... placeholders. Warning We are playing a slight trick here with the lumiscale parameter. At the point at which text2workspace.py is building these scaling terms the lumiscale for the rateParam has not yet been created. By writing lumiscale[1] we are telling RooFit to create this variable with an initial value of 1, and then later this will be re-used by the rateParam creation. A similar option, --X-nuisance-group-function , can be used to scale whole groups of nuisances (see groups of nuisances ). Instead of a regular expression just give the group name instead, text2workspace.py datacard.txt --X-nuisance-group-function [group name] 'expr::lumisyst(\"1/sqrt(@0)\",lumiscale[1])' Nuisance parameter impacts The impact of a nuisance parameter (NP) \u03b8 on a parameter of interest (POI) \u03bc is defined as the shift \u0394\u03bc that is induced as \u03b8 is fixed and brought to its +1\u03c3 or \u22121\u03c3 post-fit values, with all other parameters profiled as normal. This is effectively a measure of the correlation between the NP and the POI, and is useful for determining which NPs have the largest effect on the POI uncertainty. It is possible to use the FitDiagnostics method of combine with the option --algo impact -P parameter to calculate the impact of a particular nuisance parameter on the parameter(s) of interest. We will use the combineTool.py script to automate the fits (see the combineTool section to check out the tool. We will use an example workspace from the H\\rightarrow\\tau\\tau H\\rightarrow\\tau\\tau datacard , $ cp HiggsAnalysis/CombinedLimit/data/tutorials/htt/125/htt_tt.txt . $ text2workspace.py htt_tt.txt -m 125 Calculating the impacts is done in a few stages. First we just fit for each POI, using the --doInitialFit option with combineTool.py , and adding the --robustFit 1 option that will be passed through to combine, combineTool.py -M Impacts -d htt_tt.root -m 125 --doInitialFit --robustFit 1 Have a look at the options as for likelihood scans when using robustFit 1 . Next we perform a similar scan for each nuisance parameter with the --doFits options, combineTool.py -M Impacts -d htt_tt.root -m 125 --robustFit 1 --doFits Note that this will run approximately 60 scans, and to speed things up the option --parallel X can be given to run X combine jobs simultaneously. The batch and grid submission methods described in the combineTool for job submission section can also be used. Once all jobs are completed the output can be collected and written into a json file: combineTool.py -M Impacts -d htt_tt.root -m 125 -o impacts.json A plot summarising the nuisance parameter values and impacts can be made with plotImpacts.py , plotImpacts.py -i impacts.json -o impacts The first page of the output is shown below. The direction of the +1\u03c3 and -1\u03c3 impacts (i.e. when the NP is moved to its +1\u03c3 or -1\u03c3 values) on the POI indicates whether the parameter is correlated or anti-correlated with it. Warning The plot also shows the best fit value of the POI at the top and its uncertainty. You may wish to allow the range to go -ve (i.e using --setParameterRanges or --rMin ) to avoid getting one-sided impacts! This script also accepts an optional json-file argument with - t which can be used to provide a dictionary for renaming parameters. A simple example would be to create a file rename.json , { \"r\" : \"#mu\" } that will rename the POI label on the plot. Info Since combineTool accepts the usual options for combine you can also generate the impacts on an Asimov or toy dataset. The left panel in the summary plot shows the value of (\\theta-\\theta_{0})/\\Delta_{\\theta} (\\theta-\\theta_{0})/\\Delta_{\\theta} where \\theta \\theta and \\theta_{0} \\theta_{0} are the post and pre -fit values of the nuisance parameter and \\Delta_{\\theta} \\Delta_{\\theta} is the pre -fit uncertainty. The asymmetric error bars show the post -fit uncertainty divided by the pre -fit uncertainty meaning that parameters with error bars smaller than \\pm 1 \\pm 1 are constrained in the fit. As with the diffNuisances.py script, use the option --pullDef are defined (eg to show the pull instead). Breakdown of uncertainties Often you will want to report the breakdown of your total (systematic) uncertainty on a measured parameter due to one or more groups of nuisance parameters. For example these groups could be theory uncertainties, trigger uncertainties, ... The prodecude to do this in combine is to sequentially freeze groups of nuisance parameters and subtract (in quadrature) from the total uncertainty. Below are the steps to do so. We will use the data/tutorials/htt/125/htt_tt.txt datacard for this. Add groups to the datacard to group nuisance parameters. Nuisance parameters not in groups will be considered as \"rest\" in the later steps. The lines should look like the following and you should add them to the end of the datacard theory group = QCDscale_VH QCDscale_ggH1in QCDscale_ggH2in QCDscale_qqH UEPS pdf_gg pdf_qqbar calibration group = CMS_scale_j_8TeV CMS_scale_t_tautau_8TeV CMS_htt_scale_met_8TeV efficiency group = CMS_eff_b_8TeV CMS_eff_t_tt_8TeV CMS_fake_b_8TeV Create the workspace with text2workspace.py data/tutorials/htt/125/htt_tt.txt -m 125 . Run a post-fit with all nuisance parameters floating and store the workspace in an output file - combine data/tutorials/htt/125/htt_tt.root -M MultiDimFit --saveWorkspace -n htt.postfit Run a scan from the postfit workspace combine higgsCombinehtt.postfit.MultiDimFit.mH120.root -M MultiDimFit -n htt.total --algo grid --snapshotName MultiDimFit --setParameterRanges r=0,4 Run additional scans using the post-fit workspace sequentially adding another group to the list of groups to freeze combine higgsCombinehtt.postfit.MultiDimFit.mH120.root -M MultiDimFit --algo grid --snapshotName MultiDimFit --setParameterRanges r=0,4 --freezeNuisanceGroups theory -n htt.freeze_theory combine higgsCombinehtt.postfit.MultiDimFit.mH120.root -M MultiDimFit --algo grid --snapshotName MultiDimFit --setParameterRanges r=0,4 --freezeNuisanceGroups theory,calibration -n htt.freeze_theory_calibration combine higgsCombinehtt.postfit.MultiDimFit.mH120.root -M MultiDimFit --algo grid --snapshotName MultiDimFit --setParameterRanges r=0,4 --freezeNuisanceGroups theory,calibration,efficiency -n htt.freeze_theory_calibration_efficiency Run one last scan freezing all of the constrained nuisances (this represents the statistics only uncertainty). combine higgsCombinehtt.postfit.MultiDimFit.mH120.root -M MultiDimFit --algo grid --snapshotName MultiDimFit --setParameterRanges r=0,4 --freezeParameters allConstrainedNuisances -n htt.freeze_all Use the combineTool script plot1D.py to report the breakdown of uncertainties. plot1DScan.py higgsCombinehtt.total.MultiDimFit.mH120.root --main-label \"Total Uncert.\" --others higgsCombinehtt.freeze_theory.MultiDimFit.mH120.root:\"freeze theory\":4 higgsCombinehtt.freeze_theory_calibration.MultiDimFit.mH120.root:\"freeze theory+calibration\":7 higgsCombinehtt.freeze_theory_calibration_efficiency.MultiDimFit.mH120.root:\"freeze theory+calibration+efficiency\":2 higgsCombinehtt.freeze_all.MultiDimFit.mH120.root:\"stat only\":6 --output breakdown --y-max 10 --y-cut 40 --breakdown \"theory,calibration,efficiency,rest,stat\" The final step calculates the contribution of each group of nuisances as the subtraction in quadrature of each scan from the previous one. This procedure guarantees that the sum in quadrature of the individual components is the same as the total uncertainty. The plot below is produced, Warning While the above procedure is guaranteed the have the effect that the sum in quadrature of the breakdown will equal the total uncertainty, the order in which you freeze the groups can make a difference due to correlations induced by the fit. You should check if the answers change significantly if changing the order and we reccomend you start with the largest group (in terms of overall contribution to the uncertainty) first and work down the list in order of contribution. Channel Masking The combine tool has a number of features for diagnostics and plotting results of fits. It can often be useful to turn off particular channels in a combined analysis to see how constraints/pulls can vary. It can also be helpful to plot post-fit shapes + uncertainties of a particular channel (for example a signal region) without including the constraints from the data in that region. This can in some cases be achieved by removing a specific datacard when running combineCards.py however, when doing so the information of particular nuisances and pdfs in that region will be lost. Instead, it is possible to mask that channel from the likelihood! This is acheived at the text2Workspace step using the option --channel-masks . Example: removing constraints from the signal region We will take the control region example from the rate parameters tutorial from data/tutorials/rate_params/ . The first step is to combine the cards combineCards.py signal=signal_region.txt dimuon=dimuon_control_region.txt singlemuon=singlemuon_control_region.txt > datacard.txt Note that we use the directive CHANNELNAME=CHANNEL_DATACARD.txt so that the names of the channels are under our control and easier to interpret. Next, we make a workspace and tell combine to create the parameters used to mask channels text2workspace.py datacard.txt --channel-masks Now lets try a fit ignoring the signal region. We can turn off the signal region by setting the channel mask parameter on: --setParameters mask_signal=1 . Note that text2workspace has created a masking parameter for every channel with the naming scheme mask_CHANNELNAME . By default, every parameter is set to 0 so that the channel is unmasked by default. combine datacard.root -M FitDiagnostics --saveShapes --saveWithUncertainties --setParameters mask_signal=1 Warning There will be a lot of warning from combine. This is safe to ignore as this is due to the s+b fit not converging since the free signal parameter cannot be constrained as the data in the signal region is being ignored. We can compare the background post-fit and uncertainties with and without the signal region by re-running with --setParameters mask_signal=0 (or just removing that command). Below is a comparison of the background in the signal region with and without masking the data in the signal region. We take these from the shapes folder shapes_fit_b/signal/total_background in the fitDiagnostics.root output. Clearly the background shape is different and much less constrained without including the signal region , as expected. Channel masking can be used with any method in combine. RooMultiPdf conventional bias studies Several analyses within the Higgs group use a functional form to describe their background which is fit to the data (eg the Higgs to two photons (Hgg) analysis). Often however, there is some uncertainty associated to the choice of which background function to use and this choice will impact results of a fit. It is therefore often the case that in these analyses, a Bias study is performed which will indicate how much potential bias can be present given a certain choice of functional form. These studies can be conducted using combine. Below is an example script which will produce a workspace based on a simplified Hgg analysis with a single category. It will produce the data and pdfs necessary for this example (use it as a basis to cosntruct your own studies). void makeRooMultiPdfWorkspace(){ // Load the combine Library gSystem->Load(\"libHiggsAnalysisCombinedLimit.so\"); // mass variable RooRealVar mass(\"CMS_hgg_mass\",\"m_{#gamma#gamma}\",120,100,180); // create 3 background pdfs // 1. exponential RooRealVar expo_1(\"expo_1\",\"slope of exponential\",-0.02,-0.1,-0.0001); RooExponential exponential(\"exponential\",\"exponential pdf\",mass,expo_1); // 2. polynomial with 2 parameters RooRealVar poly_1(\"poly_1\",\"T1 of chebychev polynomial\",0,-3,3); RooRealVar poly_2(\"poly_2\",\"T2 of chebychev polynomial\",0,-3,3); RooChebychev polynomial(\"polynomial\",\"polynomial pdf\",mass,RooArgList(poly_1,poly_2)); // 3. A power law function RooRealVar pow_1(\"pow_1\",\"exponent of power law\",-3,-6,-0.0001); RooGenericPdf powerlaw(\"powerlaw\",\"TMath::Power(@0,@1)\",RooArgList(mass,pow_1)); // Generate some data (lets use the power lay function for it) // Here we are using unbinned data, but binning the data is also fine RooDataSet *data = powerlaw.generate(mass,RooFit::NumEvents(1000)); // First we fit the pdfs to the data (gives us a sensible starting value of parameters for, e.g - blind limits) exponential.fitTo(*data); // index 0 polynomial.fitTo(*data); // index 1 powerlaw.fitTo(*data); // index 2 // Make a plot (data is a toy dataset) RooPlot *plot = mass.frame(); data->plotOn(plot); exponential.plotOn(plot,RooFit::LineColor(kGreen)); polynomial.plotOn(plot,RooFit::LineColor(kBlue)); powerlaw.plotOn(plot,RooFit::LineColor(kRed)); plot->SetTitle(\"PDF fits to toy data\"); plot->Draw(); // Make a RooCategory object. This will control which of the pdfs is \"active\" RooCategory cat(\"pdf_index\",\"Index of Pdf which is active\"); // Make a RooMultiPdf object. The order of the pdfs will be the order of their index, ie for below // 0 == exponential // 1 == polynomial // 2 == powerlaw RooArgList mypdfs; mypdfs.add(exponential); mypdfs.add(polynomial); mypdfs.add(powerlaw); RooMultiPdf multipdf(\"roomultipdf\",\"All Pdfs\",cat,mypdfs); // By default the multipdf will tell combine to add 0.5 to the nll for each parameter (this is the penalty for the discrete profiling method) // It can be changed with // multipdf.setCorrectionFactor(penalty) // For bias-studies, this isn;t relevant however, so lets just leave the default // As usual make an extended term for the background with _norm for freely floating yield RooRealVar norm(\"roomultipdf_norm\",\"Number of background events\",1000,0,10000); // We will also produce a signal model for the bias studies RooRealVar sigma(\"sigma\",\"sigma\",1.2); sigma.setConstant(true); RooRealVar MH(\"MH\",\"MH\",125); MH.setConstant(true); RooGaussian signal(\"signal\",\"signal\",mass,MH,sigma); // Save to a new workspace TFile *fout = new TFile(\"workspace.root\",\"RECREATE\"); RooWorkspace wout(\"workspace\",\"workspaace\"); data->SetName(\"data\"); wout.import(*data); wout.import(cat); wout.import(norm); wout.import(multipdf); wout.import(signal); wout.Print(); wout.Write(); } The signal is modelled as a simple Gaussian with a width approximately that of the diphoton resolution and the background is a choice of 3 functions. An exponential, a power-law and a 2nd order polynomial. This choice is accessible inside combine through the use of the RooMultiPdf object which can switch between the functions by setting its associated index (herein called pdf_index ). This (as with all parameters in combine) is accessible via the --setParameters option. To asses the bias, one can throw toys using one function and fit with another. All of this only needs to use one datacard hgg_toy_datacard.txt The bias studies are performed in two stages. The first is to generate toys using one of the functions under some value of the signal strength r (or \\mu \\mu ). This can be repeated for several values of r and also at different masses, but here the Higgs mass is fixed to 125 GeV. combine hgg_toy_datacard.txt -M GenerateOnly --setParameters pdf_index=0 --toysFrequentist -t 100 --expectSignal 1 --saveToys -m 125 --freezeParameters pdf_index Warning It is important to freeze pdf_index otherwise combine will try to iterate over the index in the frequentist fit. Now we have 100 toys which, by setting pdf_index=0 , sets the background pdf to the exponential function i.e assumes the exponential is the true function. Note that the option --toysFrequentist is added. This first performs a fit of the pdf, assuming a signal strength of 1, to the data before generating the toys. This is the most obvious choice as to where to throw the toys from. The next step is to fit the toys under a different background pdf hypothesis. This time we set the pdf_index to be 1, the powerlaw and run fits with the FitDiagnostics method again freezing pdf_index . combine hgg_toy_datacard.txt -M FitDiagnostics --setParameters pdf_index=1 --toysFile higgsCombineTest.GenerateOnly.mH125.123456.root -t 100 --rMin -10 --rMax 10 --freezeParameters pdf_index --cminDefaultMinimizerStrategy=0 Note how we add the option --cminDefaultMinimizerStrategy=0 . This is because we don't need the Hessian, as FitDiagnostics will run minos to get the uncertainty on r . If we don't do this, Minuit will think the fit failed as we have parameters (those not attached to the current pdf) for which the likelihood is flat. Warning You may get warnings about non-accurate errors such as [WARNING]: Unable to determine uncertainties on all fit parameters in b-only fit - These can be ignored since they are related to the free parameters of the background pdfs which are not active. In the output file fitDiagnostics.root there is a tree which contains the best fit results under the signal+background hypothesis. One measure of the bias is the pull defined as the difference between the measured value of \\mu \\mu and the generated value (here we used 1) relative to the uncertainty on \\mu \\mu . The pull distribution can be drawn and the mean provides an estimate of the pull. In this example, we are averaging the +ve and -ve errors, but we could do something smarter if the errors are very asymmetric. root -l fitDiagnostics.root tree_fit_sb->Draw(\"(r-1)/(0.5*(rHiErr+rLoErr))>>h(20,-5,5)\") h->Fit(\"gaus\") From the fitted Gaussian, we see the mean is at -1.29 which would indicate a bias of 129% of the uncertainty on mu from choosing the polynomial when the true function is an exponential! Discrete profiling If the discrete nuisance is left floating, it will be profiled by looping through the possible index values and finding the pdf which gives the best fit. This allows for the discrete profiling method to be applied for any method which involves a profiled likelihood (frequentist methods). Warning You should be careful since MINOS knows nothing about the discretenuisances and hence estimations of uncertainties will be incorrect via MINOS. Instead, uncertainties from scans and limits will correctly account for these nuisances. Currently the Bayesian methods will not properly treat the nuisances so some care should be taken when interpreting Bayesian results. As an example, we can use peform a likelihood scan as a function of the Higgs signal strength in the toy Hgg datacard. By leaving the object pdf_index non-constant, at each point in the likelihood scan, the pdfs will be iterated over and the one which gives the lowest -2 times log-likelihood, including the correction factor c c (as defined in the paper) will be stored in the output tree. We can also check the scan fixing to each pdf individually to check that the envelope is acheived. For this, you will need to include the option --X-rtd REMOVE_CONSTANT_ZERO_POINT=1 . In this way, we can take a look at the absolute value to compare the curves, if we also include --saveNLL . For example for a full scan, you can run combine -M MultiDimFit -d hgg_toy_datacard.txt --algo grid --setParameterRanges r=-1,3 --cminDefaultMinimizerStrategy 0 --saveNLL -n Envelope -m 125 --setParameters myIndex=-1 --X-rtd REMOVE_CONSTANT_ZERO_POINT=1 and for the individual pdf_index set to X , combine -M MultiDimFit -d hgg_toy_datacard.txt --algo grid --setParameterRanges r=-1,3 --cminDefaultMinimizerStrategy 0 --saveNLL --freezeParameters pdf_index --setParameters pdf_index=X -n fixed_pdf_X -m 125 --X-rtd REMOVE_CONSTANT_ZERO_POINT=1 for X=0,1,2 You can then plot the value of 2*(deltaNLL+nll+nll0) to plot the absolute value of (twice) the negative log-likelihood, including the correction term for extra parameters in the different pdfs. The above output will produce the following scans. As expected, the curve obtained by allowing the pdf_index to float (labelled \"Envelope\") picks out the best function (maximum corrected likelihood) for each value of the signal strength. In general, you can improve the performance of combine, when using the disccrete profiling method, by including the following options --X-rtd MINIMIZER_freezeDisassociatedParams , which will stop parameters not associated to the current pdf from floating in the fits. Additionaly, you can also include the following --X-rtd MINIMIZER_multiMin_hideConstants : hide the constant terms in the likelihood when recreating the minimizer --X-rtd MINIMIZER_multiMin_maskConstraints : hide the constraint terms during the discrete minimization process --X-rtd MINIMIZER_multiMin_maskChannels=<choice> mask in the NLL the channels that are not needed: <choice> 1 : keeps unmasked all channels that are participating in the discrete minimization. <choice> 2 : keeps unmasked only the channel whose index is being scanned at the moment. You may want to check with the combine dev team if using these options as they are somewhat for expert use. RooSplineND multidimensional splines RooSplineND can be used to interpolate from tree of points to produce a continuous function in N-dimensions. This function can then be used as input to workspaces allowing for parametric rates/cross-sections/efficiencies etc OR can be used to up-scale the resolution of likelihood scans (i.e like those produced from combine) to produce smooth contours. The spline makes use of a radial basis decomposition to produce a continous N \\to 1 N \\to 1 map (function) from M M provided sample points. The function of the N N variables \\vec{x} \\vec{x} is assumed to be of the form, f(\\vec{x}) = \\sum_{i=1}^{M}w_{i}\\phi(||\\vec{x}-\\vec{x}_{i}||), f(\\vec{x}) = \\sum_{i=1}^{M}w_{i}\\phi(||\\vec{x}-\\vec{x}_{i}||), where \\phi(||\\vec{z}||) = e^{-\\frac{||\\vec{z}||}{\\epsilon^{2}}} \\phi(||\\vec{z}||) = e^{-\\frac{||\\vec{z}||}{\\epsilon^{2}}} . The distance ||.|| ||.|| between two points is given by, ||\\vec{x}-\\vec{y}|| = \\sum_{j=1}^{N}(x_{j}-y_{j})^{2}, ||\\vec{x}-\\vec{y}|| = \\sum_{j=1}^{N}(x_{j}-y_{j})^{2}, if the option rescale=false and, ||\\vec{x}-\\vec{y}|| = \\sum_{j=1}^{N} M^{1/N} \\cdot \\left( \\frac{ x_{j}-y_{j} }{ \\mathrm{max_{i=1,M}}(x_{i,j})-\\mathrm{min_{i=1,M}}(x_{i,j}) }\\right)^{2}, ||\\vec{x}-\\vec{y}|| = \\sum_{j=1}^{N} M^{1/N} \\cdot \\left( \\frac{ x_{j}-y_{j} }{ \\mathrm{max_{i=1,M}}(x_{i,j})-\\mathrm{min_{i=1,M}}(x_{i,j}) }\\right)^{2}, if the option rescale=true . Given the sample points, it is possible to determine the weights w_{i} w_{i} as the solution of the set of equations, \\sum_{i=1}^{M}w_{i}\\phi(||\\vec{x}_{j}-\\vec{x}_{i}||) = f(\\vec{x}_{j}). \\sum_{i=1}^{M}w_{i}\\phi(||\\vec{x}_{j}-\\vec{x}_{i}||) = f(\\vec{x}_{j}). The solution is obtained using the eigen c++ package. The typical constructor of the object is done as follows; RooSplineND(const char *name, const char *title, RooArgList &vars, TTree *tree, const char* fName=\"f\", double eps=3., bool rescale=false, std::string cutstring=\"\" ) ; where the arguments are: vars : A RooArgList of RooRealVars representing the N N dimensions of the spline. The length of this list determines the dimension N N of the spline. tree : a TTree pointer where each entry represents a sample point used to construct the spline. The branch names must correspond to the names of the variables in vars . fName : is a string representing the name of the branch to interpret as the target function f f . eps : is the value of \\epsilon \\epsilon and represents the width of the basis functions \\phi \\phi . rescale : is an option to re-scale the input sample points so that each variable has roughly the same range (see above in the definition of ||.|| ||.|| ). cutstring : a string to remove sample points from the tree. Can be any typical cut string (eg \"var1>10 && var2<3\"). The object can be treaeted as a RooAbsArg and its value for the current values of the parameters is obtained as usual by using the getVal() method. Warning You should not include more variable branches than contained in vars in the tree as the spline will interpret them as additional sample points. You should get a warning if there are two nearby points in the input samples and this will cause a failure in determining the weights. If you cannot create a reduced tree, you can remove entries by using the cutstring . The following script is an example of its use which produces a 2D spline ( N=2 ) from a set of 400 points ( M=400 ) generated from a function. void splinend(){ // library containing the RooSplineND gSystem->Load(\"libHiggsAnalysisCombinedLimit.so\"); TTree *tree = new TTree(\"tree_vals\",\"tree_vals\"); float xb,yb,fb; tree->Branch(\"f\",&fb,\"f/F\"); tree->Branch(\"x\",&xb,\"x/F\"); tree->Branch(\"y\",&yb,\"y/F\"); TRandom3 *r = new TRandom3(); int nentries = 20; // just use a regular grid of 20x20=400 points double xmin = -3.2; double xmax = 3.2; double ymin = -3.2; double ymax = 3.2; for (int n=0;n<nentries;n++){ for (int k=0;k<nentries;k++){ xb=xmin+n*((xmax-xmin)/nentries); yb=ymin+k*((ymax-ymin)/nentries); // Gaussian * cosine function radial in \"F(x^2+y^2)\" double R = (xb*xb)+(yb*yb); fb = 0.1*TMath::Exp(-1*(R)/9)*TMath::Cos(2.5*TMath::Sqrt(R)); tree->Fill(); } } // 2D graph of points in tree TGraph2D *p0 = new TGraph2D(); p0->SetMarkerSize(0.8); p0->SetMarkerStyle(20); int c0=0; for (int p=0;p<tree->GetEntries();p++){ tree->GetEntry(p); p0->SetPoint(c0,xb,yb,fb); c0++; } // ------------------------------ THIS IS WHERE WE BUILD THE SPLINE ------------------------ // // Create 2 Real-vars, one for each of the parameters of the spline // The variables MUST be named the same as the corresponding branches in the tree RooRealVar x(\"x\",\"x\",0.1,xmin,xmax); RooRealVar y(\"y\",\"y\",0.1,ymin,ymax); // And the spline - arguments are // Required -> name, title, arglist of dependants, input tree, // Optional -> function branch name, interpolation width (tunable parameter), rescale Axis bool, cutstring // The tunable parameter gives the radial basis a \"width\", over which the interpolation will be effectively taken // the reascale Axis bool (if true) will first try to rescale the points so that they are of order 1 in range // This can be helpful if for example one dimension is in much larger units than another. // The cutstring is just a ROOT string which can be used to apply cuts to the tree in case only a sub-set of the points should be used RooArgList args(x,y); RooSplineND *spline = new RooSplineND(\"spline\",\"spline\",args,tree,\"f\",1,true); // ----------------------------------------------------------------------------------------- // //TGraph *gr = spline->getGraph(\"x\",0.1); // Return 1D graph. Will be a slice of the spline for fixed y generated at steps of 0.1 // Plot the 2D spline TGraph2D *gr = new TGraph2D(); int pt = 0; for (double xx=xmin;xx<xmax;xx+=0.1){ for (double yy=xmin;yy<ymax;yy+=0.1){ x.setVal(xx); y.setVal(yy); gr->SetPoint(pt,xx,yy,spline->getVal()); pt++; } } gr->SetTitle(\"\"); gr->SetLineColor(1); //p0->SetTitle(\"0.1 exp(-(x{^2}+y{^2})/9) #times Cos(2.5#sqrt{x^{2}+y^{2}})\"); gr->Draw(\"surf\"); gr->GetXaxis()->SetTitle(\"x\"); gr->GetYaxis()->SetTitle(\"y\"); p0->Draw(\"Pcolsame\"); //p0->Draw(\"surfsame\"); TLegend *leg = new TLegend(0.2,0.82,0.82,0.98); leg->SetFillColor(0); leg->AddEntry(p0,\"0.1 exp(-(x{^2}+y{^2})/9) #times Cos(2.5#sqrt{x^{2}+y^{2}})\",\"p\"); leg->AddEntry(gr,\"RooSplineND (N=2) interpolation\",\"L\"); leg->Draw(); } Running the script will produce the following plot. The plot shows the sampled points and the spline produced from them. RooParametricHist gammaN for shapes Currently, there is no straight-forward implementation of using per-bin gmN like uncertainties with shape (histogram) analyses. Instead, it is possible to tie control regions (written as datacards) with the signal region using three methods. For analyses who take the normalisation of some process from a control region, it is possible to use either lnU or rateParam directives to float the normalisation in a correlated way of some process between two regions. Instead if each bin is intended to be determined via a control region, one can use a number of RooFit histogram pdfs/functions to accomplish this. The example below shows a simple implementation of a RooParametricHist to achieve this. copy the script below into a file called examplews.C and create the input workspace using root -l examplews.C ... void examplews(){ // As usual, load the combine library to get access to the RooParametricHist gSystem->Load(\"libHiggsAnalysisCombinedLimit.so\"); // Output file and workspace TFile *fOut = new TFile(\"param_ws.root\",\"RECREATE\"); RooWorkspace wspace(\"wspace\",\"wspace\"); // better to create the bins rather than use the \"nbins,min,max\" to avoid spurious warning about adding bins with different // ranges in combine - see https://root-forum.cern.ch/t/attempt-to-divide-histograms-with-different-bin-limits/17624/3 for why! const int nbins = 4; double xmin=200.; double xmax=1000.; double xbins[5] = {200.,400.,600.,800.,1000.}; // A search in a MET tail, define MET as our variable double xmin=200.; double xmax=1000.; RooRealVar met(\"met\",\"E_{T}^{miss}\",200,xmin,xmax); RooArgList vars(met); // better to create the bins rather than use the \"nbins,min,max\" to avoid spurious warning about adding bins with different // ranges in combine - see https://root-forum.cern.ch/t/attempt-to-divide-histograms-with-different-bin-limits/17624/3 for why! double xbins[5] = {200.,400.,600.,800.,1000.}; // ---------------------------- SIGNAL REGION -------------------------------------------------------------------// // Make a dataset, this will be just four bins in MET. // its easiest to make this from a histogram. Set the contents to \"somehting\" TH1F data_th1(\"data_obs_SR\",\"Data observed in signal region\",nbins,xbins); data_th1.SetBinContent(1,100); data_th1.SetBinContent(2,50); data_th1.SetBinContent(3,25); data_th1.SetBinContent(4,10); RooDataHist data_hist(\"data_obs_SR\",\"Data observed\",vars,&data_th1); wspace.import(data_hist); // In the signal region, our background process will be freely floating, // Create one parameter per bin representing the yield. (note of course we can have multiple processes like this) RooRealVar bin1(\"bkg_SR_bin1\",\"Background yield in signal region, bin 1\",100,0,500); RooRealVar bin2(\"bkg_SR_bin2\",\"Background yield in signal region, bin 2\",50,0,500); RooRealVar bin3(\"bkg_SR_bin3\",\"Background yield in signal region, bin 3\",25,0,500); RooRealVar bin4(\"bkg_SR_bin4\",\"Background yield in signal region, bin 4\",10,0,500); RooArgList bkg_SR_bins; bkg_SR_bins.add(bin1); bkg_SR_bins.add(bin2); bkg_SR_bins.add(bin3); bkg_SR_bins.add(bin4); // Create a RooParametericHist which contains those yields, last argument is just for the binning, // can use the data TH1 for that RooParametricHist p_bkg(\"bkg_SR\", \"Background PDF in signal region\",met,bkg_SR_bins,data_th1); // Always include a _norm term which should be the sum of the yields (thats how combine likes to play with pdfs) RooAddition p_bkg_norm(\"bkg_SR_norm\",\"Total Number of events from background in signal region\",bkg_SR_bins); // Every signal region needs a signal TH1F signal_th1(\"signal_SR\",\"Signal expected in signal region\",nbins,xbins); signal_th1.SetBinContent(1,1); signal_th1.SetBinContent(2,2); signal_th1.SetBinContent(3,3); signal_th1.SetBinContent(4,8); RooDataHist signal_hist(\"signal\",\"Data observed\",vars,&signal_th1); wspace.import(signal_hist); // -------------------------------------------------------------------------------------------------------------// // ---------------------------- CONTROL REGION -----------------------------------------------------------------// TH1F data_CRth1(\"data_obs_CR\",\"Data observed in control region\",nbins,xbins); data_CRth1.SetBinContent(1,200); data_CRth1.SetBinContent(2,100); data_CRth1.SetBinContent(3,50); data_CRth1.SetBinContent(4,20); RooDataHist data_CRhist(\"data_obs_CR\",\"Data observed\",vars,&data_CRth1); wspace.import(data_CRhist); // This time, the background process will be dependent on the yields of the background in the signal region. // The transfer factor TF must account for acceptance/efficiency etc differences in the signal to control // In this example lets assume the control region is populated by the same process decaying to clean daughters with 2xBR // compared to the signal region // NB You could have a different transfer factor for each bin represented by a completely different RooRealVar // We can imagine that the transfer factor could be associated with some uncertainty - lets say a 1% uncertainty due to efficiency and 2% due to acceptance. // We need to make these nuisance parameters ourselves and give them a nominal value of 0 RooRealVar efficiency(\"efficiency\", \"efficiency nuisance parameter\",0); RooRealVar acceptance(\"acceptance\", \"acceptance nuisance parameter\",0); // We would need to make the transfer factor a function of those too. Here we've assumed Log-normal effects (i.e the same as putting lnN in the CR datacard) // but note that we could use any function which could be used to parameterise the effect - eg if the systematic is due to some alternate template, we could // use polynomials for example. RooFormulaVar TF(\"TF\",\"Trasnfer factor\",\"2*TMath::Power(1.01,@0)*TMath::Power(1.02,@1)\",RooArgList(efficiency,acceptance) ); // Finally, we need to make each bin of the background in the control region a function of the background in the signal and the transfer factor // N_CR = N_SR x TF RooFormulaVar CRbin1(\"bkg_CR_bin1\",\"Background yield in control region, bin 1\",\"@0*@1\",RooArgList(TF,bin1)); RooFormulaVar CRbin2(\"bkg_CR_bin2\",\"Background yield in control region, bin 2\",\"@0*@1\",RooArgList(TF,bin2)); RooFormulaVar CRbin3(\"bkg_CR_bin3\",\"Background yield in control region, bin 3\",\"@0*@1\",RooArgList(TF,bin3)); RooFormulaVar CRbin4(\"bkg_CR_bin4\",\"Background yield in control region, bin 4\",\"@0*@1\",RooArgList(TF,bin4)); RooArgList bkg_CR_bins; bkg_CR_bins.add(CRbin1); bkg_CR_bins.add(CRbin2); bkg_CR_bins.add(CRbin3); bkg_CR_bins.add(CRbin4); RooParametricHist p_CRbkg(\"bkg_CR\", \"Background PDF in control region\",met,bkg_CR_bins,data_th1); RooAddition p_CRbkg_norm(\"bkg_CR_norm\",\"Total Number of events from background in control region\",bkg_CR_bins); // -------------------------------------------------------------------------------------------------------------// // we can also use the standard interpolation from combine by providing alternative shapes (as RooDataHists) // here we're adding two of them (JES and ISR) TH1F background_up(\"tbkg_CR_JESUp\",\"\",nbins,xbins); background_up.SetBinContent(1,CRbin1.getVal()*1.01); background_up.SetBinContent(2,CRbin2.getVal()*1.02); background_up.SetBinContent(3,CRbin3.getVal()*1.03); background_up.SetBinContent(4,CRbin4.getVal()*1.04); RooDataHist bkg_CRhist_sysUp(\"bkg_CR_JESUp\",\"Bkg sys up\",vars,&background_up); wspace.import(bkg_CRhist_sysUp); TH1F background_down(\"bkg_CR_JESDown\",\"\",nbins,xbins); background_down.SetBinContent(1,CRbin1.getVal()*0.90); background_down.SetBinContent(2,CRbin2.getVal()*0.98); background_down.SetBinContent(3,CRbin3.getVal()*0.97); background_down.SetBinContent(4,CRbin4.getVal()*0.96); RooDataHist bkg_CRhist_sysDown(\"bkg_CR_JESDown\",\"Bkg sys down\",vars,&background_down); wspace.import(bkg_CRhist_sysDown); TH1F background_2up(\"tbkg_CR_ISRUp\",\"\",nbins,xbins); background_2up.SetBinContent(1,CRbin1.getVal()*0.85); background_2up.SetBinContent(2,CRbin2.getVal()*0.9); background_2up.SetBinContent(3,CRbin3.getVal()*0.95); background_2up.SetBinContent(4,CRbin4.getVal()*0.99); RooDataHist bkg_CRhist_sys2Up(\"bkg_CR_ISRUp\",\"Bkg sys 2up\",vars,&background_2up); wspace.import(bkg_CRhist_sys2Up); TH1F background_2down(\"bkg_CR_ISRDown\",\"\",nbins,xbins); background_2down.SetBinContent(1,CRbin1.getVal()*1.15); background_2down.SetBinContent(2,CRbin2.getVal()*1.1); background_2down.SetBinContent(3,CRbin3.getVal()*1.05); background_2down.SetBinContent(4,CRbin4.getVal()*1.01); RooDataHist bkg_CRhist_sys2Down(\"bkg_CR_ISRDown\",\"Bkg sys 2down\",vars,&background_2down); wspace.import(bkg_CRhist_sys2Down); // import the pdfs wspace.import(p_bkg); wspace.import(p_bkg_norm,RooFit::RecycleConflictNodes()); wspace.import(p_CRbkg); wspace.import(p_CRbkg_norm,RooFit::RecycleConflictNodes()); fOut->cd(); wspace.Write(); // Clean up fOut->Close(); fOut->Delete(); } Lets go through what the script is doing. First, the observable for the search is the missing energy so we create a parameter to represent that. RooRealVar met(\"met\",\"E_{T}^{miss}\",xmin,xmax); First, the following lines create a freely floating parameter for each of our bins (in this example, there are only 4 bins, defined for our observable met . RooRealVar bin1(\"bkg_SR_bin1\",\"Background yield in signal region, bin 1\",100,0,500); RooRealVar bin2(\"bkg_SR_bin2\",\"Background yield in signal region, bin 2\",50,0,500); RooRealVar bin3(\"bkg_SR_bin3\",\"Background yield in signal region, bin 3\",25,0,500); RooRealVar bin4(\"bkg_SR_bin4\",\"Background yield in signal region, bin 4\",10,0,500); RooArgList bkg_SR_bins; bkg_SR_bins.add(bin1); bkg_SR_bins.add(bin2); bkg_SR_bins.add(bin3); bkg_SR_bins.add(bin4); They are put into a list so that we can create a RooParametricHist and its normalisation from that list RooParametricHist p_bkg(\"bkg_SR\", \"Background PDF in signal region\",met,bkg_SR_bins,data_th1); RooAddition p_bkg_norm(\"bkg_SR_norm\",\"Total Number of events from background in signal region\",bkg_SR_bins); For the control region, the background process will be dependent on the yields of the background in the signal region using a transfer factor . The transfer factor TF must account for acceptance/efficiency etc differences in the signal to control regions. In this example lets assume the control region is populated by the same process decaying to a different final state with twice as large branching ratio compared to the one in the signal region. We could imagine that the transfer factor could be associated with some uncertainty - lets say a 1% uncertainty due to efficiency and 2% due to acceptance. We need to make nuisance parameters ourselves to model this and give them a nominal value of 0. RooRealVar efficiency(\"efficiency\", \"efficiency nuisance parameter\",0); RooRealVar acceptance(\"acceptance\", \"acceptance nuisance parameter\",0); We need to make the transfer factor a function of these parameters since variations in these uncertainties will lead to variations of the transfer factor. Here we've assumed Log-normal effects (i.e the same as putting lnN in the CR datacard) but we could use any function which could be used to parameterise the effect - eg if the systematic is due to some alternate template, we could use polynomials for example. RooFormulaVar TF(\"TF\",\"Trasnfer factor\",\"2*TMath::Power(1.01,@0)*TMath::Power(1.02,@1)\",RooArgList(efficiency,acceptance) ); Then need to make each bin of the background in the control region a function of the background in the signal and the transfer factor - i.e $N_{CR} = N_{SR} \\times TF $. RooFormulaVar CRbin1(\"bkg_CR_bin1\",\"Background yield in control region, bin 1\",\"@0*@1\",RooArgList(TF,bin1)); RooFormulaVar CRbin2(\"bkg_CR_bin2\",\"Background yield in control region, bin 2\",\"@0*@1\",RooArgList(TF,bin2)); RooFormulaVar CRbin3(\"bkg_CR_bin3\",\"Background yield in control region, bin 3\",\"@0*@1\",RooArgList(TF,bin3)); RooFormulaVar CRbin4(\"bkg_CR_bin4\",\"Background yield in control region, bin 4\",\"@0*@1\",RooArgList(TF,bin4)); As before, we also need to create the RooParametricHist for this process in the control region but this time the bin yields will be the RooFormulaVars we just created instead of free floating parameters. RooArgList bkg_CR_bins; bkg_CR_bins.add(CRbin1); bkg_CR_bins.add(CRbin2); bkg_CR_bins.add(CRbin3); bkg_CR_bins.add(CRbin4); RooParametricHist p_CRbkg(\"bkg_CR\", \"Background PDF in control region\",met,bkg_CR_bins,data_th1); RooAddition p_CRbkg_norm(\"bkg_CR_norm\",\"Total Number of events from background in control region\",bkg_CR_bins); Finally, we can also create alternative shape variations (Up/Down) that can be fed to combine as we do with TH1 or RooDataHist type workspaces. These need to be of type RooDataHist . The example below is for a Jet Energy Scale type shape uncertainty. TH1F background_up(\"tbkg_CR_JESUp\",\"\",nbins,xbins); background_up.SetBinContent(1,CRbin1.getVal()*1.01); background_up.SetBinContent(2,CRbin2.getVal()*1.02); background_up.SetBinContent(3,CRbin3.getVal()*1.03); background_up.SetBinContent(4,CRbin4.getVal()*1.04); RooDataHist bkg_CRhist_sysUp(\"bkg_CR_JESUp\",\"Bkg sys up\",vars,&background_up); wspace.import(bkg_CRhist_sysUp); TH1F background_down(\"bkg_CR_JESDown\",\"\",nbins,xbins); background_down.SetBinContent(1,CRbin1.getVal()*0.90); background_down.SetBinContent(2,CRbin2.getVal()*0.98); background_down.SetBinContent(3,CRbin3.getVal()*0.97); background_down.SetBinContent(4,CRbin4.getVal()*0.96); RooDataHist bkg_CRhist_sysDown(\"bkg_CR_JESDown\",\"Bkg sys down\",vars,&background_down); wspace.import(bkg_CRhist_sysDown); Below are datacards (for signal and control regions) which can be used in conjunction with the workspace built above. In order to \"use\" the control region, simply combine the two cards as usual using combineCards.py . Signal Region Datacard -- signal category imax * number of bins jmax * number of processes minus 1 kmax * number of nuisance parameters ------------------------------------------------------------------------------------------------------------------------------------------- shapes data_obs signal param_ws.root wspace:data_obs_SR shapes background signal param_ws.root wspace:bkg_SR # the background model pdf which is freely floating, note other backgrounds can be added as usual shapes signal signal param_ws.root wspace:signal ------------------------------------------------------------------------------------------------------------------------------------------- bin signal observation -1 ------------------------------------------------------------------------------------------------------------------------------------------- # background rate must be taken from _norm param x 1 bin signal signal process background signal process 1 0 rate 1 -1 ------------------------------------------------------------------------------------------------------------------------------------------- # Normal uncertainties in the signal region lumi_8TeV lnN - 1.026 ------------------------------------------------------------------------------------------------------------------------------------------- # free floating parameters, we do not need to declare them, but its a good idea to bkg_SR_bin1 flatParam bkg_SR_bin2 flatParam bkg_SR_bin3 flatParam bkg_SR_bin4 flatParam Control Region Datacard -- control category imax * number of bins jmax * number of processes minus 1 kmax * number of nuisance parameters ------------------------------------------------------------------------------------------------------------------------------------------- shapes data_obs control param_ws.root wspace:data_obs_CR shapes background control param_ws.root wspace:bkg_CR wspace:bkg_CR_$SYSTEMATIC # the background model pdf which is dependant on that in the SR, note other backgrounds can be added as usual ------------------------------------------------------------------------------------------------------------------------------------------- bin control observation -1 ------------------------------------------------------------------------------------------------------------------------------------------- # background rate must be taken from _norm param x 1 bin control process background process 1 rate 1 ------------------------------------------------------------------------------------------------------------------------------------------- JES shape 1 ISR shape 1 efficiency param 0 1 acceptance param 0 1 Note that for the control region, our nuisance parameters appear as param types so that combine will correctly constrain them. If we combine the two cards and fit the result with -M MultiDimFit -v 3 we can see that the parameters which give the rate of background in each bin of the signal region, along with the nuisance parameters and signal strength, are determined by the fit - i.e we have properly included the constraint from the control region, just as with the 1-bin gmN . acceptance = 0.00374312 +/- 0.964632 (limited) bkg_SR_bin1 = 99.9922 +/- 5.92062 (limited) bkg_SR_bin2 = 49.9951 +/- 4.13535 (limited) bkg_SR_bin3 = 24.9915 +/- 2.9267 (limited) bkg_SR_bin4 = 9.96478 +/- 2.1348 (limited) efficiency = 0.00109195 +/- 0.979334 (limited) lumi_8TeV = -0.0025911 +/- 0.994458 r = 0.00716347 +/- 12.513 (limited) The example given here is extremely basic and it should be noted that additional complexity in the transfer factors, additional uncertainties/backgrounds etc in the cards are supported as always. Danger If trying to implement parametric uncertainties in this setup (eg on transfer factors) which are correlated with other channels and implemented separately, you MUST normalise the uncertainty effect so that the datacard line can read param name X 1 . That is the uncertainty on this parameter must be 1. Without this, there will be inconsistency with other nuisances of the same name in other channels implemented as shape or lnN .","title":"Advanced use cases"},{"location":"part3/nonstandard/#advanced-use-cases","text":"This section will cover some of the more specific use cases for combine which are not necessarily related to the main statistics results.","title":"Advanced Use Cases"},{"location":"part3/nonstandard/#fitting-diagnostics","text":"You may first want to look at the HIG PAG standard checks applied to all datacards if you want to diagnose your limit setting/fitting results which can be found here If you have already found the higgs boson but it's an exotic one, instead of computing a limit or significance you might want to extract it's cross section by performing a maximum-likelihood fit. Or, more seriously, you might want to use this same package to extract the cross section of some other process (e.g. the di-boson production). Or you might want to know how well the data compares to you model, e.g. how strongly it constraints your other nuisance parameters, what's their correlation, etc. These general diagnostic tools are contained in the method FitDiagnostics . combine -M FitDiagnostics datacard.txt The program will print out the result of the two fits performed with signal strength r (or first POI in the list) set to zero and a second with floating r . The output root tree will contain the best fit value for r and it's uncertainty. You will also get a fitDiagnostics.root file containing the following objects: Object Description nuisances_prefit RooArgSet containing the pre-fit values of the nuisance parameters, and their uncertainties from the external constraint terms only fit_b RooFitResult object containing the outcome of the fit of the data with signal strength set to zero fit_s RooFitResult object containing the outcome of the fit of the data with floating signal strength tree_prefit TTree of pre-fit nuisance parameter values and constraint terms (_In) tree_fit_sb TTree of fitted nuisance parameter values and constraint terms (_In) with floating signal strength tree_fit_b TTree of fitted nuisance parameter values and constraint terms (_In) with signal strength set to 0 by including the option --plots , you will additionally find the following contained in the root file . Object Description covariance_fit_s TH2D Covariance matrix of the parameters in the fit with floating signal strength covariance_fit_b TH2D Covariance matrix of the parameters in the fit with signal strength set to zero category_variable_prefit RooPlot plot of the prefit pdfs with the data (or toy if running with -t overlaid) category_variable_fit_b RooPlot plot of the pdfs from the background only fit with the data (or toy if running with -t overlaid) category_variable_fit_s RooPlot plot of the pdfs from the signal+background fit with the data (or toy if running with -t overlaid) where for the RooPlot objects, you will get one per category in the likelihood and one per variable if using a multi-dimensional dataset. You will also get a png file for each of these additional objects. Info If you use the option --name this name will be inserted into the file name for this output file too. As well as values of the constrained nuisance parameters (and their constraint values) in the toys, you will also find branches for the number of \"bad\" nll calls (which you should check is not too large) and the status of the fit fit_status . The fit status is computed as follows fit_status = 100 * hesse_status + 10 * minos_status + minuit_summary_status The minuit_summary_status is the usual status from Minuit, details of which can be found here . For the other status values, check these documentation links for the hesse_status and the minos_status . A fit status of -1 indicates that the fit failed (Minuit summary was not 0 or 1) and hence the fit is not valid.","title":"Fitting Diagnostics"},{"location":"part3/nonstandard/#fit-options","text":"If you need only the signal+background fit, you can run with --justFit . This can be useful if the background-only fit is not interesting or not converging (e.g. if the significance of your signal is very very large) You can use --rMin and --rMax to set the range of the first POI; a range that is not too large compared to the uncertainties you expect from the fit usually gives more stable and accurate results. By default, the uncertainties are computed using MINOS for the first POI and HESSE for all other parameters (and hence they will be symmetric for the nuisance parameters). You can run MINOS for all parameters using the option --minos all , or for none of the parameters using --minos none . Note that running MINOS is slower so you should only consider using it if you think the HESSE uncertainties are not accurate. If MINOS or HESSE fails to converge, you can try running with --robustFit=1 that will do a slower but more robust likelihood scan; this can be further controlled by the parameter --stepSize (the default is 0.1, and is relative to the range of the parameter) You can set the strategy and tolerance when using the --robustFit option using the options setRobustFitAlgo (default is Minuit2,migrad ), setRobustFitStrategy (default is 0) and --setRobustFitTolerance (default is 0.1). If these options are not set, the defaults (set using cminDefaultMinimizerX options) will be used. You can also tune the accuracy of the routine used to find the crossing points of the likelihood using the option --setCrossingTolerance (default is set to 0.0001) If you find the covariance matrix provided by HESSE is not accurate (i.e. fit_s->Print() reports this was forced positive-definite) then a custom HESSE-style calculation of the covariance matrix can be used instead. This is enabled by running FitDiagnostics with the --robustHesse 1 option. Please note that the status reported by RooFitResult::Print() will contain covariance matrix quality: Unknown, matrix was externally provided when robustHesse is used, this is normal and does not indicate a problem. NB: one feature of the robustHesse algorithm is that if it still cannot calculate a positive-definite covariance matrix it will try to do so by dropping parameters from the hessian matrix before inverting. If this happens it will be reported in the output to the screen. For other fitting options see the generic minimizer options section.","title":"Fit options"},{"location":"part3/nonstandard/#fit-parameter-uncertainties","text":"If you get a warning message when running FitDiagnostics which says Unable to determine uncertainties on all fit parameters . This means the covariance matrix calculated in FitDiagnostics was not correct. The most common problem is that the covariance matrix is forced positive-definite. In this case the constraints on fit parameters as taken from the covariance matrix are incorrect and should not be used. In particular, if you want to make post-fit plots of the distribution used in the signal extraction fit and are extracting the uncertainties on the signal and background expectations from the covariance matrix, the resulting values will not reflect the truth if the covariance matrix was incorrect. By default if this happens and you passed the --saveWithUncertainties flag when calling FitDiagnostics , this option will be ignored as calculating the uncertainties would lead to incorrect results. This behaviour can be overridden by passing --ignoreCovWarning . Such problems with the covariance matrix can be caused by a number of things, for example: Parameters being close to their boundaries after the fit. Strong (anti-) correlations between some parameters. A discontinuity in the NLL function or its derivatives at or near the minimum. If you are aware that your analysis has any of these features you could try resolving these. Setting --cminDefaultMinimizerStrategy 0 can also help with this problem.","title":"Fit parameter uncertainties"},{"location":"part3/nonstandard/#pre-and-post-fit-nuisance-parameters-and-pulls","text":"It is possible to compare pre-fit and post-fit nuisance parameters with the script diffNuisances.py . Taking as input a fitDiagnostics.root file, the script will by default print out the parameters which have changed significantly w.r.t. their initial estimate. For each of those parameters, it will print out the shift in value and the post-fit uncertainty, both normalized to the input values, and the linear correlation between the parameter and the signal strength. python diffNuisances.py fitDiagnostics.root The script has several options to toggle the thresholds used to decide if a parameter has changed significantly, to get the printout of the absolute value of the nuisance parameters, and to get the output in another format for easy cut-n-paste (supported formats are html , latex , twiki ). To print all of the parameters, use the option --all . The output by default will be the changes in the nuisance parameter values and uncertainties, relative to their initial (pre-fit) values (usually relative to initial values of 0 and 1 for most nuisance types). The values in the output will be (\\theta-\\theta_{I})/\\sigma_{I} (\\theta-\\theta_{I})/\\sigma_{I} if the nuisance has a pre-fit uncertainty, otherwise it will be \\theta-\\theta_{I} \\theta-\\theta_{I} if not (eg, a flatParam has no pre-fit uncertainty). The uncertainty reported will be the ratio \\sigma/\\sigma_{I} \\sigma/\\sigma_{I} - i.e the ratio of the post-fit to the pre-fit uncertainty. If there is no pre-fit uncertainty (as for flatParam nuisances) then the post-fit uncertainty is shown. With the option --abs , instead the pre-fit and post-fit values and (asymmetric) uncertainties will be reported in full. Info We recommend you include the options --abs and --all to get the full information on all of the parameters (including unconstrained nuisance parameters) at least once when checking your datacards. If instead of the plain values, you wish to report the pulls , you can do so with the option --pullDef X with X being one of the following options; You should note that since the pulls below are only defined when the pre-fit uncertainty exists, nothing will be reported for parameters which have no prior constraint (except in the case of the unconstPullAsym choice as described below). You may want to run without this option and --all to get information on those parameters. relDiffAsymErrs : This is the same as the default output of the tool except that only constrained parameters (pre-fit uncertainty defined) are reported. The error is also reported and calculated as \\sigma/\\sigma_{I} \\sigma/\\sigma_{I} . unconstPullAsym : Report the pull as \\frac{\\theta-\\theta_{I}}{\\sigma} \\frac{\\theta-\\theta_{I}}{\\sigma} where \\theta_{I} \\theta_{I} and \\sigma \\sigma are the initial value and post-fit uncertainty of that nuisance parameter. The pull defined in this way will have no error bar, but all nuisance parameters will have a result in this case. compatAsym : The pull is defined as \\frac{\\theta-\\theta_{D}}{\\sqrt{\\sigma^{2}+\\sigma_{D}^{2}}} \\frac{\\theta-\\theta_{D}}{\\sqrt{\\sigma^{2}+\\sigma_{D}^{2}}} , where \\theta_{D} \\theta_{D} and \\sigma_{D} \\sigma_{D} are calculated as \\sigma_{D} = (\\frac{1}{\\sigma^{2}} - \\frac{1}{\\sigma_{I}^{2}})^{-1} \\sigma_{D} = (\\frac{1}{\\sigma^{2}} - \\frac{1}{\\sigma_{I}^{2}})^{-1} and \\theta_{D} = \\sigma_{D}(\\theta - \\frac{\\theta_{I}}{\\sigma_{I}^{2}}) \\theta_{D} = \\sigma_{D}(\\theta - \\frac{\\theta_{I}}{\\sigma_{I}^{2}}) , where \\theta_{I} \\theta_{I} and \\sigma_{I} \\sigma_{I} are the initial value and uncertainty of that nuisance parameter. This can be thought of as a compatibility between the initial measurement (prior) an imagined measurement where only the data (with no constraint) is used to measure the nuisance parameter. There is no error bar associated to this value. diffPullAsym : The pull is defined as \\frac{\\theta-\\theta_{I}}{\\sqrt{\\sigma_{I}^{2}-\\sigma^{2}}} \\frac{\\theta-\\theta_{I}}{\\sqrt{\\sigma_{I}^{2}-\\sigma^{2}}} , where \\theta_{I} \\theta_{I} and \\sigma_{I} \\sigma_{I} are the pre-fit value and uncertainty (from L. Demortier and L. Lyons ). If the denominator is close to 0 or the post-fit uncertainty is larger than the pre-fit (usually due to some failure in the calculation), the pull is not defined and the result will be reported as 0 +/- 999 . If using --pullDef , the results for all parameters for which the pull can be calculated will be shown (i.e --all will be set to true ), not just those which have moved by some metric. This script has the option ( -g outputfile.root ) to produce plots of the fitted values of the nuisance parameters and their post-fit, asymmetric uncertainties. Instead, the pulls defined using one of the options above, can be plotted using the option --pullDef X . In addition this will produce a plot showing directly a comparison of the post-fit to pre-fit nuisance (symmetrized) uncertainties. Info In the above options, if an asymmetric uncertainty is associated to the nuisance parameter, then the choice of which uncertainty is used in the definition of the pull will depend on the sign of \\theta-\\theta_{I} \\theta-\\theta_{I} .","title":"Pre and post fit nuisance parameters and pulls"},{"location":"part3/nonstandard/#normalizations","text":"For a certain class of models, like those made from datacards for shape-based analysis, the tool can also compute and save to the output root file the best fit yields of all processes. If this feature is turned on with the option --saveNormalizations , the file will also contain three RooArgSet norm_prefit , norm_fit_s , norm_fit_b objects each containing one RooConstVar for each channel xxx and process yyy with name xxx/yyy and value equal to the best fit yield. You can use RooRealVar::getVal and RooRealVar::getError to estimate both the post-(or pre-)fit values and uncertainties of these normalisations. The sample pyroot macro mlfitNormsToText.py can be used to convert the root file into a text table with four columns: channel, process, yield from the signal+background fit and yield from the background-only fit. To include the uncertainties in the table, add the option --uncertainties Warning Note that when running with multiple toys, the norm_fit_s , norm_fit_b and norm_prefit objects will be stored for the last toy dataset generated and so may not be useful to you. Note that this procedure works only for \"extended likelihoods\" like the ones used in shape-based analysis, not for the cut-and-count datacards. You can however convert a cut-and-count datacard in an equivalent shape-based one by adding a line shapes * * FAKE in the datacard after the imax , jmax , kmax or using combineCards.py countingcard.txt -S > shapecard.txt .","title":"Normalizations"},{"location":"part3/nonstandard/#per-bin-norms-for-shape-analyses","text":"If you have a shape based analysis, you can also (instead) include the option --savePredictionsPerToy . With this option, additional branches will be filled in the three output trees contained in fitDiagnostics.root . The normalisation values for each toy will be stored in the branches inside the TTrees named n_exp[_final]_binxxx_proc_yyy . The _final will only be there if there are systematics affecting this process. Additionally, there will be filled branches which provide the value of the expected bin content for each process, in each channel. These will are named as n_exp[_final]_binxxx_proc_yyy_i (where _final will only be in the name if there are systematics affecting this process) for channel xxx , process yyy bin number i . In the case of the post-fit trees ( tree_fit_s/b ), these will be resulting expectations from the fitted models, while for the pre-fit tree, they will be the expectation from the generated model (i.e if running toys with -t N and using --genNuisances , they will be randomised for each toy). These can be useful, for example, for calculating correlations/covariances between different bins, in different channels or processes, within the model from toys. Info Be aware that for unbinned models, a binning scheme is adopted based on the RooRealVar::getBinning for the observable defining the shape, if it exists, or combine will adopt some appropriate binning for each observable.","title":"Per-bin norms for shape analyses"},{"location":"part3/nonstandard/#plotting","text":"FitDiagnostics can also produce pre- and post-fit plots the model in the same directory as fitDiagnostics.root along with the data. To get them, you have to specify the option --plots , and then optionally specify what are the names of the signal and background pdfs, e.g. --signalPdfNames='ggH*,vbfH*' and --backgroundPdfNames='*DY*,*WW*,*Top*' (by default, the definitions of signal and background are taken from the datacard). For models with more than 1 observable, a separate projection onto each observable will be produced. An alternative is to use the options --saveShapes . The result will be additional folders in fitDiagnostics.root for each category, with pre and post-fit distributions of the signals and backgrounds as TH1s and the data as TGraphAsymmErrors (with Poisson intervals as error bars). Info If you want to save post-fit shapes at a specific r value, add the options --customStartingPoint and --skipSBFit , and set the r value. The result will appear in shapes_fit_b , as described below. Three additional folders ( shapes_prefit , shapes_fit_sb and shapes_fit_b ) will contain the following distributions, Object Description data TGraphAsymmErrors containing the observed data (or toy data if using -t ). The vertical error bars correspond to the 68% interval for a Poisson distribution centered on the observed count. $PROCESS (id <= 0) TH1F for each signal process in channel, named as in the datacard $PROCESS (id > 0) TH1F for each background process in channel, named as in the datacard total_signal TH1F Sum over the signal components total_background TH1F Sum over the background components total TH1F Sum over all of the signal and background components The above distributions are provided for each channel included in the datacard , in separate sub-folders, named as in the datacard: There will be one sub-folder per channel. Warning The pre-fit signal is by default for r=1 but this can be modified using the option --preFitValue . The distributions and normalisations are guaranteed to give the correct interpretation: For shape datacards whose inputs are TH1, the histograms/data points will have the bin number as the x-axis and the content of each bin will be a number of events. For datacards whose inputs are RooAbsPdf/RooDataHists, the x-axis will correspond to the observable and the bin content will be the PDF density / events divided by the bin width. This means the absolute number of events in a given bin, i, can be obtained from h.GetBinContent(i)*h.GetBinWidth(i) or similar for the data graphs. Note that for unbinned analyses combine will make a reasonable guess as to an appropriate binning. Uncertainties on the shapes will be added with the option --saveWithUncertainties . These uncertainties are generated by re-sampling of the fit covariance matrix, thereby accounting for the full correlation between the parameters of the fit. Warning It may be tempting to sum up the uncertainties in each bin (in quadrature) to get the total uncertainty on a process however, this is (usually) incorrect as doing so would not account for correlations between the bins . Instead you can refer to the uncertainties which will be added to the post-fit normalizations described above. Additionally, the covariance matrix between bin yields (or yields/bin-widths) in each channel will also be saved as a TH2F named total_covar . If the covariance between all bins across all channels is desired, this can be added using the option --saveOverallShapes . Each folder will now contain additional distributions (and covariance matrices) corresponding to the concatenation of the bins in each channel (and therefore the covaraince between every bin in the analysis). The bin labels should make it clear as to which bin corresponds to which channel.","title":"Plotting"},{"location":"part3/nonstandard/#toy-by-toy-diagnostics","text":"FitDiagnostics can also be used to diagnose the fitting procedure in toy experiments to identify potentially problematic nuisance parameters when running the full limits/p-values. This can be done by adding the option -t <num toys> . The output file, fitDiagnostics.root the three TTrees will contain the value of the constraint fitted result in each toy, as a separate entry. It is recommended to use the following options when investigating toys to reduce the running time: --toysFrequentist --noErrors --minos none The results can be plotted using the macro test/plotParametersFromToys.C $ root -l .L plotParametersFromToys.C+ plotParametersFomToys(\"fitDiagnosticsToys.root\",\"fitDiagnosticsData.root\",\"workspace.root\",\"r<0\") The first argument is the name of the output file from running with toys, and the second and third (optional) arguments are the name of the file containing the result from a fit to the data and the workspace (created from text2workspace.py ). The fourth argument can be used to specify a cut string applied to one of the branches in the tree which can be used to correlate strange behaviour with specific conditions. The output will be 2 pdf files ( tree_fit_(s)b.pdf ) and 2 root files ( tree_fit_(s)b.root ) containing canvases of the fit results of the tool. For details on the output plots, consult AN-2012/317 .","title":"Toy-by-toy diagnostics"},{"location":"part3/nonstandard/#scaling-constraints","text":"It possible to scale the constraints on the nuisance parameters when converting the datacard to a workspace (see the section on physics models ) with text2workspace.py . This can be useful for projection studies of the analysis to higher luminosities or with different assumptions about the sizes of certain systematics without changing the datacard by hand. We consider two kinds of scaling; A constant scaling factor to scale the constraints A functional scale factor that depends on some other parameters in the workspace, eg a luminosity scaling parameter (as a rateParam affecting all processes). In both cases these scalings can be introduced by adding some extra options at the text2workspace.py step. To add a constant scaling factor we use the option --X-rescale-nuisance , eg text2workspace.py datacard.txt --X-rescale-nuisance '[some regular expression]' 0.5 will create the workspace in which ever nuisance parameter whose name matches the specified regular expression will have the width of the gaussian constraint scaled by a factor 0.5. Multiple --X-rescale-nuisance options can be specified to set different scalings for different nuisances (note that you actually have to write --X-rescale-nuisance each time as in --X-rescale-nuisance 'theory.*' 0.5 --X-rescale-nuisance 'exp.*' 0.1 ). To add a functional scaling factor we use the option --X-nuisance-function , which works in a similar way. Instead of a constant value you should specify a RooFit factory expression. A typical case would be scaling by 1/\\sqrt{L} 1/\\sqrt{L} , where L L is a luminosity scale factor eg assuming there is some parameter in the datacard/workspace called lumiscale , text2workspace.py datacard.txt --X-nuisance-function '[some regular expression]' 'expr::lumisyst(\"1/sqrt(@0)\",lumiscale[1])' This factory syntax is quite flexible, but for our use case the typical format will be: expr::[function name](\"[formula]\", [arg0], [arg1], ...) . The arg0 , arg1 ... are represented in the formula by @0 , @1 ,... placeholders. Warning We are playing a slight trick here with the lumiscale parameter. At the point at which text2workspace.py is building these scaling terms the lumiscale for the rateParam has not yet been created. By writing lumiscale[1] we are telling RooFit to create this variable with an initial value of 1, and then later this will be re-used by the rateParam creation. A similar option, --X-nuisance-group-function , can be used to scale whole groups of nuisances (see groups of nuisances ). Instead of a regular expression just give the group name instead, text2workspace.py datacard.txt --X-nuisance-group-function [group name] 'expr::lumisyst(\"1/sqrt(@0)\",lumiscale[1])'","title":"Scaling constraints"},{"location":"part3/nonstandard/#nuisance-parameter-impacts","text":"The impact of a nuisance parameter (NP) \u03b8 on a parameter of interest (POI) \u03bc is defined as the shift \u0394\u03bc that is induced as \u03b8 is fixed and brought to its +1\u03c3 or \u22121\u03c3 post-fit values, with all other parameters profiled as normal. This is effectively a measure of the correlation between the NP and the POI, and is useful for determining which NPs have the largest effect on the POI uncertainty. It is possible to use the FitDiagnostics method of combine with the option --algo impact -P parameter to calculate the impact of a particular nuisance parameter on the parameter(s) of interest. We will use the combineTool.py script to automate the fits (see the combineTool section to check out the tool. We will use an example workspace from the H\\rightarrow\\tau\\tau H\\rightarrow\\tau\\tau datacard , $ cp HiggsAnalysis/CombinedLimit/data/tutorials/htt/125/htt_tt.txt . $ text2workspace.py htt_tt.txt -m 125 Calculating the impacts is done in a few stages. First we just fit for each POI, using the --doInitialFit option with combineTool.py , and adding the --robustFit 1 option that will be passed through to combine, combineTool.py -M Impacts -d htt_tt.root -m 125 --doInitialFit --robustFit 1 Have a look at the options as for likelihood scans when using robustFit 1 . Next we perform a similar scan for each nuisance parameter with the --doFits options, combineTool.py -M Impacts -d htt_tt.root -m 125 --robustFit 1 --doFits Note that this will run approximately 60 scans, and to speed things up the option --parallel X can be given to run X combine jobs simultaneously. The batch and grid submission methods described in the combineTool for job submission section can also be used. Once all jobs are completed the output can be collected and written into a json file: combineTool.py -M Impacts -d htt_tt.root -m 125 -o impacts.json A plot summarising the nuisance parameter values and impacts can be made with plotImpacts.py , plotImpacts.py -i impacts.json -o impacts The first page of the output is shown below. The direction of the +1\u03c3 and -1\u03c3 impacts (i.e. when the NP is moved to its +1\u03c3 or -1\u03c3 values) on the POI indicates whether the parameter is correlated or anti-correlated with it. Warning The plot also shows the best fit value of the POI at the top and its uncertainty. You may wish to allow the range to go -ve (i.e using --setParameterRanges or --rMin ) to avoid getting one-sided impacts! This script also accepts an optional json-file argument with - t which can be used to provide a dictionary for renaming parameters. A simple example would be to create a file rename.json , { \"r\" : \"#mu\" } that will rename the POI label on the plot. Info Since combineTool accepts the usual options for combine you can also generate the impacts on an Asimov or toy dataset. The left panel in the summary plot shows the value of (\\theta-\\theta_{0})/\\Delta_{\\theta} (\\theta-\\theta_{0})/\\Delta_{\\theta} where \\theta \\theta and \\theta_{0} \\theta_{0} are the post and pre -fit values of the nuisance parameter and \\Delta_{\\theta} \\Delta_{\\theta} is the pre -fit uncertainty. The asymmetric error bars show the post -fit uncertainty divided by the pre -fit uncertainty meaning that parameters with error bars smaller than \\pm 1 \\pm 1 are constrained in the fit. As with the diffNuisances.py script, use the option --pullDef are defined (eg to show the pull instead).","title":"Nuisance parameter impacts"},{"location":"part3/nonstandard/#breakdown-of-uncertainties","text":"Often you will want to report the breakdown of your total (systematic) uncertainty on a measured parameter due to one or more groups of nuisance parameters. For example these groups could be theory uncertainties, trigger uncertainties, ... The prodecude to do this in combine is to sequentially freeze groups of nuisance parameters and subtract (in quadrature) from the total uncertainty. Below are the steps to do so. We will use the data/tutorials/htt/125/htt_tt.txt datacard for this. Add groups to the datacard to group nuisance parameters. Nuisance parameters not in groups will be considered as \"rest\" in the later steps. The lines should look like the following and you should add them to the end of the datacard theory group = QCDscale_VH QCDscale_ggH1in QCDscale_ggH2in QCDscale_qqH UEPS pdf_gg pdf_qqbar calibration group = CMS_scale_j_8TeV CMS_scale_t_tautau_8TeV CMS_htt_scale_met_8TeV efficiency group = CMS_eff_b_8TeV CMS_eff_t_tt_8TeV CMS_fake_b_8TeV Create the workspace with text2workspace.py data/tutorials/htt/125/htt_tt.txt -m 125 . Run a post-fit with all nuisance parameters floating and store the workspace in an output file - combine data/tutorials/htt/125/htt_tt.root -M MultiDimFit --saveWorkspace -n htt.postfit Run a scan from the postfit workspace combine higgsCombinehtt.postfit.MultiDimFit.mH120.root -M MultiDimFit -n htt.total --algo grid --snapshotName MultiDimFit --setParameterRanges r=0,4 Run additional scans using the post-fit workspace sequentially adding another group to the list of groups to freeze combine higgsCombinehtt.postfit.MultiDimFit.mH120.root -M MultiDimFit --algo grid --snapshotName MultiDimFit --setParameterRanges r=0,4 --freezeNuisanceGroups theory -n htt.freeze_theory combine higgsCombinehtt.postfit.MultiDimFit.mH120.root -M MultiDimFit --algo grid --snapshotName MultiDimFit --setParameterRanges r=0,4 --freezeNuisanceGroups theory,calibration -n htt.freeze_theory_calibration combine higgsCombinehtt.postfit.MultiDimFit.mH120.root -M MultiDimFit --algo grid --snapshotName MultiDimFit --setParameterRanges r=0,4 --freezeNuisanceGroups theory,calibration,efficiency -n htt.freeze_theory_calibration_efficiency Run one last scan freezing all of the constrained nuisances (this represents the statistics only uncertainty). combine higgsCombinehtt.postfit.MultiDimFit.mH120.root -M MultiDimFit --algo grid --snapshotName MultiDimFit --setParameterRanges r=0,4 --freezeParameters allConstrainedNuisances -n htt.freeze_all Use the combineTool script plot1D.py to report the breakdown of uncertainties. plot1DScan.py higgsCombinehtt.total.MultiDimFit.mH120.root --main-label \"Total Uncert.\" --others higgsCombinehtt.freeze_theory.MultiDimFit.mH120.root:\"freeze theory\":4 higgsCombinehtt.freeze_theory_calibration.MultiDimFit.mH120.root:\"freeze theory+calibration\":7 higgsCombinehtt.freeze_theory_calibration_efficiency.MultiDimFit.mH120.root:\"freeze theory+calibration+efficiency\":2 higgsCombinehtt.freeze_all.MultiDimFit.mH120.root:\"stat only\":6 --output breakdown --y-max 10 --y-cut 40 --breakdown \"theory,calibration,efficiency,rest,stat\" The final step calculates the contribution of each group of nuisances as the subtraction in quadrature of each scan from the previous one. This procedure guarantees that the sum in quadrature of the individual components is the same as the total uncertainty. The plot below is produced, Warning While the above procedure is guaranteed the have the effect that the sum in quadrature of the breakdown will equal the total uncertainty, the order in which you freeze the groups can make a difference due to correlations induced by the fit. You should check if the answers change significantly if changing the order and we reccomend you start with the largest group (in terms of overall contribution to the uncertainty) first and work down the list in order of contribution.","title":"Breakdown of uncertainties"},{"location":"part3/nonstandard/#channel-masking","text":"The combine tool has a number of features for diagnostics and plotting results of fits. It can often be useful to turn off particular channels in a combined analysis to see how constraints/pulls can vary. It can also be helpful to plot post-fit shapes + uncertainties of a particular channel (for example a signal region) without including the constraints from the data in that region. This can in some cases be achieved by removing a specific datacard when running combineCards.py however, when doing so the information of particular nuisances and pdfs in that region will be lost. Instead, it is possible to mask that channel from the likelihood! This is acheived at the text2Workspace step using the option --channel-masks .","title":"Channel Masking"},{"location":"part3/nonstandard/#example-removing-constraints-from-the-signal-region","text":"We will take the control region example from the rate parameters tutorial from data/tutorials/rate_params/ . The first step is to combine the cards combineCards.py signal=signal_region.txt dimuon=dimuon_control_region.txt singlemuon=singlemuon_control_region.txt > datacard.txt Note that we use the directive CHANNELNAME=CHANNEL_DATACARD.txt so that the names of the channels are under our control and easier to interpret. Next, we make a workspace and tell combine to create the parameters used to mask channels text2workspace.py datacard.txt --channel-masks Now lets try a fit ignoring the signal region. We can turn off the signal region by setting the channel mask parameter on: --setParameters mask_signal=1 . Note that text2workspace has created a masking parameter for every channel with the naming scheme mask_CHANNELNAME . By default, every parameter is set to 0 so that the channel is unmasked by default. combine datacard.root -M FitDiagnostics --saveShapes --saveWithUncertainties --setParameters mask_signal=1 Warning There will be a lot of warning from combine. This is safe to ignore as this is due to the s+b fit not converging since the free signal parameter cannot be constrained as the data in the signal region is being ignored. We can compare the background post-fit and uncertainties with and without the signal region by re-running with --setParameters mask_signal=0 (or just removing that command). Below is a comparison of the background in the signal region with and without masking the data in the signal region. We take these from the shapes folder shapes_fit_b/signal/total_background in the fitDiagnostics.root output. Clearly the background shape is different and much less constrained without including the signal region , as expected. Channel masking can be used with any method in combine.","title":"Example: removing constraints from the signal region"},{"location":"part3/nonstandard/#roomultipdf-conventional-bias-studies","text":"Several analyses within the Higgs group use a functional form to describe their background which is fit to the data (eg the Higgs to two photons (Hgg) analysis). Often however, there is some uncertainty associated to the choice of which background function to use and this choice will impact results of a fit. It is therefore often the case that in these analyses, a Bias study is performed which will indicate how much potential bias can be present given a certain choice of functional form. These studies can be conducted using combine. Below is an example script which will produce a workspace based on a simplified Hgg analysis with a single category. It will produce the data and pdfs necessary for this example (use it as a basis to cosntruct your own studies). void makeRooMultiPdfWorkspace(){ // Load the combine Library gSystem->Load(\"libHiggsAnalysisCombinedLimit.so\"); // mass variable RooRealVar mass(\"CMS_hgg_mass\",\"m_{#gamma#gamma}\",120,100,180); // create 3 background pdfs // 1. exponential RooRealVar expo_1(\"expo_1\",\"slope of exponential\",-0.02,-0.1,-0.0001); RooExponential exponential(\"exponential\",\"exponential pdf\",mass,expo_1); // 2. polynomial with 2 parameters RooRealVar poly_1(\"poly_1\",\"T1 of chebychev polynomial\",0,-3,3); RooRealVar poly_2(\"poly_2\",\"T2 of chebychev polynomial\",0,-3,3); RooChebychev polynomial(\"polynomial\",\"polynomial pdf\",mass,RooArgList(poly_1,poly_2)); // 3. A power law function RooRealVar pow_1(\"pow_1\",\"exponent of power law\",-3,-6,-0.0001); RooGenericPdf powerlaw(\"powerlaw\",\"TMath::Power(@0,@1)\",RooArgList(mass,pow_1)); // Generate some data (lets use the power lay function for it) // Here we are using unbinned data, but binning the data is also fine RooDataSet *data = powerlaw.generate(mass,RooFit::NumEvents(1000)); // First we fit the pdfs to the data (gives us a sensible starting value of parameters for, e.g - blind limits) exponential.fitTo(*data); // index 0 polynomial.fitTo(*data); // index 1 powerlaw.fitTo(*data); // index 2 // Make a plot (data is a toy dataset) RooPlot *plot = mass.frame(); data->plotOn(plot); exponential.plotOn(plot,RooFit::LineColor(kGreen)); polynomial.plotOn(plot,RooFit::LineColor(kBlue)); powerlaw.plotOn(plot,RooFit::LineColor(kRed)); plot->SetTitle(\"PDF fits to toy data\"); plot->Draw(); // Make a RooCategory object. This will control which of the pdfs is \"active\" RooCategory cat(\"pdf_index\",\"Index of Pdf which is active\"); // Make a RooMultiPdf object. The order of the pdfs will be the order of their index, ie for below // 0 == exponential // 1 == polynomial // 2 == powerlaw RooArgList mypdfs; mypdfs.add(exponential); mypdfs.add(polynomial); mypdfs.add(powerlaw); RooMultiPdf multipdf(\"roomultipdf\",\"All Pdfs\",cat,mypdfs); // By default the multipdf will tell combine to add 0.5 to the nll for each parameter (this is the penalty for the discrete profiling method) // It can be changed with // multipdf.setCorrectionFactor(penalty) // For bias-studies, this isn;t relevant however, so lets just leave the default // As usual make an extended term for the background with _norm for freely floating yield RooRealVar norm(\"roomultipdf_norm\",\"Number of background events\",1000,0,10000); // We will also produce a signal model for the bias studies RooRealVar sigma(\"sigma\",\"sigma\",1.2); sigma.setConstant(true); RooRealVar MH(\"MH\",\"MH\",125); MH.setConstant(true); RooGaussian signal(\"signal\",\"signal\",mass,MH,sigma); // Save to a new workspace TFile *fout = new TFile(\"workspace.root\",\"RECREATE\"); RooWorkspace wout(\"workspace\",\"workspaace\"); data->SetName(\"data\"); wout.import(*data); wout.import(cat); wout.import(norm); wout.import(multipdf); wout.import(signal); wout.Print(); wout.Write(); } The signal is modelled as a simple Gaussian with a width approximately that of the diphoton resolution and the background is a choice of 3 functions. An exponential, a power-law and a 2nd order polynomial. This choice is accessible inside combine through the use of the RooMultiPdf object which can switch between the functions by setting its associated index (herein called pdf_index ). This (as with all parameters in combine) is accessible via the --setParameters option. To asses the bias, one can throw toys using one function and fit with another. All of this only needs to use one datacard hgg_toy_datacard.txt The bias studies are performed in two stages. The first is to generate toys using one of the functions under some value of the signal strength r (or \\mu \\mu ). This can be repeated for several values of r and also at different masses, but here the Higgs mass is fixed to 125 GeV. combine hgg_toy_datacard.txt -M GenerateOnly --setParameters pdf_index=0 --toysFrequentist -t 100 --expectSignal 1 --saveToys -m 125 --freezeParameters pdf_index Warning It is important to freeze pdf_index otherwise combine will try to iterate over the index in the frequentist fit. Now we have 100 toys which, by setting pdf_index=0 , sets the background pdf to the exponential function i.e assumes the exponential is the true function. Note that the option --toysFrequentist is added. This first performs a fit of the pdf, assuming a signal strength of 1, to the data before generating the toys. This is the most obvious choice as to where to throw the toys from. The next step is to fit the toys under a different background pdf hypothesis. This time we set the pdf_index to be 1, the powerlaw and run fits with the FitDiagnostics method again freezing pdf_index . combine hgg_toy_datacard.txt -M FitDiagnostics --setParameters pdf_index=1 --toysFile higgsCombineTest.GenerateOnly.mH125.123456.root -t 100 --rMin -10 --rMax 10 --freezeParameters pdf_index --cminDefaultMinimizerStrategy=0 Note how we add the option --cminDefaultMinimizerStrategy=0 . This is because we don't need the Hessian, as FitDiagnostics will run minos to get the uncertainty on r . If we don't do this, Minuit will think the fit failed as we have parameters (those not attached to the current pdf) for which the likelihood is flat. Warning You may get warnings about non-accurate errors such as [WARNING]: Unable to determine uncertainties on all fit parameters in b-only fit - These can be ignored since they are related to the free parameters of the background pdfs which are not active. In the output file fitDiagnostics.root there is a tree which contains the best fit results under the signal+background hypothesis. One measure of the bias is the pull defined as the difference between the measured value of \\mu \\mu and the generated value (here we used 1) relative to the uncertainty on \\mu \\mu . The pull distribution can be drawn and the mean provides an estimate of the pull. In this example, we are averaging the +ve and -ve errors, but we could do something smarter if the errors are very asymmetric. root -l fitDiagnostics.root tree_fit_sb->Draw(\"(r-1)/(0.5*(rHiErr+rLoErr))>>h(20,-5,5)\") h->Fit(\"gaus\") From the fitted Gaussian, we see the mean is at -1.29 which would indicate a bias of 129% of the uncertainty on mu from choosing the polynomial when the true function is an exponential!","title":"RooMultiPdf conventional bias studies"},{"location":"part3/nonstandard/#discrete-profiling","text":"If the discrete nuisance is left floating, it will be profiled by looping through the possible index values and finding the pdf which gives the best fit. This allows for the discrete profiling method to be applied for any method which involves a profiled likelihood (frequentist methods). Warning You should be careful since MINOS knows nothing about the discretenuisances and hence estimations of uncertainties will be incorrect via MINOS. Instead, uncertainties from scans and limits will correctly account for these nuisances. Currently the Bayesian methods will not properly treat the nuisances so some care should be taken when interpreting Bayesian results. As an example, we can use peform a likelihood scan as a function of the Higgs signal strength in the toy Hgg datacard. By leaving the object pdf_index non-constant, at each point in the likelihood scan, the pdfs will be iterated over and the one which gives the lowest -2 times log-likelihood, including the correction factor c c (as defined in the paper) will be stored in the output tree. We can also check the scan fixing to each pdf individually to check that the envelope is acheived. For this, you will need to include the option --X-rtd REMOVE_CONSTANT_ZERO_POINT=1 . In this way, we can take a look at the absolute value to compare the curves, if we also include --saveNLL . For example for a full scan, you can run combine -M MultiDimFit -d hgg_toy_datacard.txt --algo grid --setParameterRanges r=-1,3 --cminDefaultMinimizerStrategy 0 --saveNLL -n Envelope -m 125 --setParameters myIndex=-1 --X-rtd REMOVE_CONSTANT_ZERO_POINT=1 and for the individual pdf_index set to X , combine -M MultiDimFit -d hgg_toy_datacard.txt --algo grid --setParameterRanges r=-1,3 --cminDefaultMinimizerStrategy 0 --saveNLL --freezeParameters pdf_index --setParameters pdf_index=X -n fixed_pdf_X -m 125 --X-rtd REMOVE_CONSTANT_ZERO_POINT=1 for X=0,1,2 You can then plot the value of 2*(deltaNLL+nll+nll0) to plot the absolute value of (twice) the negative log-likelihood, including the correction term for extra parameters in the different pdfs. The above output will produce the following scans. As expected, the curve obtained by allowing the pdf_index to float (labelled \"Envelope\") picks out the best function (maximum corrected likelihood) for each value of the signal strength. In general, you can improve the performance of combine, when using the disccrete profiling method, by including the following options --X-rtd MINIMIZER_freezeDisassociatedParams , which will stop parameters not associated to the current pdf from floating in the fits. Additionaly, you can also include the following --X-rtd MINIMIZER_multiMin_hideConstants : hide the constant terms in the likelihood when recreating the minimizer --X-rtd MINIMIZER_multiMin_maskConstraints : hide the constraint terms during the discrete minimization process --X-rtd MINIMIZER_multiMin_maskChannels=<choice> mask in the NLL the channels that are not needed: <choice> 1 : keeps unmasked all channels that are participating in the discrete minimization. <choice> 2 : keeps unmasked only the channel whose index is being scanned at the moment. You may want to check with the combine dev team if using these options as they are somewhat for expert use.","title":"Discrete profiling"},{"location":"part3/nonstandard/#roosplinend-multidimensional-splines","text":"RooSplineND can be used to interpolate from tree of points to produce a continuous function in N-dimensions. This function can then be used as input to workspaces allowing for parametric rates/cross-sections/efficiencies etc OR can be used to up-scale the resolution of likelihood scans (i.e like those produced from combine) to produce smooth contours. The spline makes use of a radial basis decomposition to produce a continous N \\to 1 N \\to 1 map (function) from M M provided sample points. The function of the N N variables \\vec{x} \\vec{x} is assumed to be of the form, f(\\vec{x}) = \\sum_{i=1}^{M}w_{i}\\phi(||\\vec{x}-\\vec{x}_{i}||), f(\\vec{x}) = \\sum_{i=1}^{M}w_{i}\\phi(||\\vec{x}-\\vec{x}_{i}||), where \\phi(||\\vec{z}||) = e^{-\\frac{||\\vec{z}||}{\\epsilon^{2}}} \\phi(||\\vec{z}||) = e^{-\\frac{||\\vec{z}||}{\\epsilon^{2}}} . The distance ||.|| ||.|| between two points is given by, ||\\vec{x}-\\vec{y}|| = \\sum_{j=1}^{N}(x_{j}-y_{j})^{2}, ||\\vec{x}-\\vec{y}|| = \\sum_{j=1}^{N}(x_{j}-y_{j})^{2}, if the option rescale=false and, ||\\vec{x}-\\vec{y}|| = \\sum_{j=1}^{N} M^{1/N} \\cdot \\left( \\frac{ x_{j}-y_{j} }{ \\mathrm{max_{i=1,M}}(x_{i,j})-\\mathrm{min_{i=1,M}}(x_{i,j}) }\\right)^{2}, ||\\vec{x}-\\vec{y}|| = \\sum_{j=1}^{N} M^{1/N} \\cdot \\left( \\frac{ x_{j}-y_{j} }{ \\mathrm{max_{i=1,M}}(x_{i,j})-\\mathrm{min_{i=1,M}}(x_{i,j}) }\\right)^{2}, if the option rescale=true . Given the sample points, it is possible to determine the weights w_{i} w_{i} as the solution of the set of equations, \\sum_{i=1}^{M}w_{i}\\phi(||\\vec{x}_{j}-\\vec{x}_{i}||) = f(\\vec{x}_{j}). \\sum_{i=1}^{M}w_{i}\\phi(||\\vec{x}_{j}-\\vec{x}_{i}||) = f(\\vec{x}_{j}). The solution is obtained using the eigen c++ package. The typical constructor of the object is done as follows; RooSplineND(const char *name, const char *title, RooArgList &vars, TTree *tree, const char* fName=\"f\", double eps=3., bool rescale=false, std::string cutstring=\"\" ) ; where the arguments are: vars : A RooArgList of RooRealVars representing the N N dimensions of the spline. The length of this list determines the dimension N N of the spline. tree : a TTree pointer where each entry represents a sample point used to construct the spline. The branch names must correspond to the names of the variables in vars . fName : is a string representing the name of the branch to interpret as the target function f f . eps : is the value of \\epsilon \\epsilon and represents the width of the basis functions \\phi \\phi . rescale : is an option to re-scale the input sample points so that each variable has roughly the same range (see above in the definition of ||.|| ||.|| ). cutstring : a string to remove sample points from the tree. Can be any typical cut string (eg \"var1>10 && var2<3\"). The object can be treaeted as a RooAbsArg and its value for the current values of the parameters is obtained as usual by using the getVal() method. Warning You should not include more variable branches than contained in vars in the tree as the spline will interpret them as additional sample points. You should get a warning if there are two nearby points in the input samples and this will cause a failure in determining the weights. If you cannot create a reduced tree, you can remove entries by using the cutstring . The following script is an example of its use which produces a 2D spline ( N=2 ) from a set of 400 points ( M=400 ) generated from a function. void splinend(){ // library containing the RooSplineND gSystem->Load(\"libHiggsAnalysisCombinedLimit.so\"); TTree *tree = new TTree(\"tree_vals\",\"tree_vals\"); float xb,yb,fb; tree->Branch(\"f\",&fb,\"f/F\"); tree->Branch(\"x\",&xb,\"x/F\"); tree->Branch(\"y\",&yb,\"y/F\"); TRandom3 *r = new TRandom3(); int nentries = 20; // just use a regular grid of 20x20=400 points double xmin = -3.2; double xmax = 3.2; double ymin = -3.2; double ymax = 3.2; for (int n=0;n<nentries;n++){ for (int k=0;k<nentries;k++){ xb=xmin+n*((xmax-xmin)/nentries); yb=ymin+k*((ymax-ymin)/nentries); // Gaussian * cosine function radial in \"F(x^2+y^2)\" double R = (xb*xb)+(yb*yb); fb = 0.1*TMath::Exp(-1*(R)/9)*TMath::Cos(2.5*TMath::Sqrt(R)); tree->Fill(); } } // 2D graph of points in tree TGraph2D *p0 = new TGraph2D(); p0->SetMarkerSize(0.8); p0->SetMarkerStyle(20); int c0=0; for (int p=0;p<tree->GetEntries();p++){ tree->GetEntry(p); p0->SetPoint(c0,xb,yb,fb); c0++; } // ------------------------------ THIS IS WHERE WE BUILD THE SPLINE ------------------------ // // Create 2 Real-vars, one for each of the parameters of the spline // The variables MUST be named the same as the corresponding branches in the tree RooRealVar x(\"x\",\"x\",0.1,xmin,xmax); RooRealVar y(\"y\",\"y\",0.1,ymin,ymax); // And the spline - arguments are // Required -> name, title, arglist of dependants, input tree, // Optional -> function branch name, interpolation width (tunable parameter), rescale Axis bool, cutstring // The tunable parameter gives the radial basis a \"width\", over which the interpolation will be effectively taken // the reascale Axis bool (if true) will first try to rescale the points so that they are of order 1 in range // This can be helpful if for example one dimension is in much larger units than another. // The cutstring is just a ROOT string which can be used to apply cuts to the tree in case only a sub-set of the points should be used RooArgList args(x,y); RooSplineND *spline = new RooSplineND(\"spline\",\"spline\",args,tree,\"f\",1,true); // ----------------------------------------------------------------------------------------- // //TGraph *gr = spline->getGraph(\"x\",0.1); // Return 1D graph. Will be a slice of the spline for fixed y generated at steps of 0.1 // Plot the 2D spline TGraph2D *gr = new TGraph2D(); int pt = 0; for (double xx=xmin;xx<xmax;xx+=0.1){ for (double yy=xmin;yy<ymax;yy+=0.1){ x.setVal(xx); y.setVal(yy); gr->SetPoint(pt,xx,yy,spline->getVal()); pt++; } } gr->SetTitle(\"\"); gr->SetLineColor(1); //p0->SetTitle(\"0.1 exp(-(x{^2}+y{^2})/9) #times Cos(2.5#sqrt{x^{2}+y^{2}})\"); gr->Draw(\"surf\"); gr->GetXaxis()->SetTitle(\"x\"); gr->GetYaxis()->SetTitle(\"y\"); p0->Draw(\"Pcolsame\"); //p0->Draw(\"surfsame\"); TLegend *leg = new TLegend(0.2,0.82,0.82,0.98); leg->SetFillColor(0); leg->AddEntry(p0,\"0.1 exp(-(x{^2}+y{^2})/9) #times Cos(2.5#sqrt{x^{2}+y^{2}})\",\"p\"); leg->AddEntry(gr,\"RooSplineND (N=2) interpolation\",\"L\"); leg->Draw(); } Running the script will produce the following plot. The plot shows the sampled points and the spline produced from them.","title":"RooSplineND multidimensional splines"},{"location":"part3/nonstandard/#rooparametrichist-gamman-for-shapes","text":"Currently, there is no straight-forward implementation of using per-bin gmN like uncertainties with shape (histogram) analyses. Instead, it is possible to tie control regions (written as datacards) with the signal region using three methods. For analyses who take the normalisation of some process from a control region, it is possible to use either lnU or rateParam directives to float the normalisation in a correlated way of some process between two regions. Instead if each bin is intended to be determined via a control region, one can use a number of RooFit histogram pdfs/functions to accomplish this. The example below shows a simple implementation of a RooParametricHist to achieve this. copy the script below into a file called examplews.C and create the input workspace using root -l examplews.C ... void examplews(){ // As usual, load the combine library to get access to the RooParametricHist gSystem->Load(\"libHiggsAnalysisCombinedLimit.so\"); // Output file and workspace TFile *fOut = new TFile(\"param_ws.root\",\"RECREATE\"); RooWorkspace wspace(\"wspace\",\"wspace\"); // better to create the bins rather than use the \"nbins,min,max\" to avoid spurious warning about adding bins with different // ranges in combine - see https://root-forum.cern.ch/t/attempt-to-divide-histograms-with-different-bin-limits/17624/3 for why! const int nbins = 4; double xmin=200.; double xmax=1000.; double xbins[5] = {200.,400.,600.,800.,1000.}; // A search in a MET tail, define MET as our variable double xmin=200.; double xmax=1000.; RooRealVar met(\"met\",\"E_{T}^{miss}\",200,xmin,xmax); RooArgList vars(met); // better to create the bins rather than use the \"nbins,min,max\" to avoid spurious warning about adding bins with different // ranges in combine - see https://root-forum.cern.ch/t/attempt-to-divide-histograms-with-different-bin-limits/17624/3 for why! double xbins[5] = {200.,400.,600.,800.,1000.}; // ---------------------------- SIGNAL REGION -------------------------------------------------------------------// // Make a dataset, this will be just four bins in MET. // its easiest to make this from a histogram. Set the contents to \"somehting\" TH1F data_th1(\"data_obs_SR\",\"Data observed in signal region\",nbins,xbins); data_th1.SetBinContent(1,100); data_th1.SetBinContent(2,50); data_th1.SetBinContent(3,25); data_th1.SetBinContent(4,10); RooDataHist data_hist(\"data_obs_SR\",\"Data observed\",vars,&data_th1); wspace.import(data_hist); // In the signal region, our background process will be freely floating, // Create one parameter per bin representing the yield. (note of course we can have multiple processes like this) RooRealVar bin1(\"bkg_SR_bin1\",\"Background yield in signal region, bin 1\",100,0,500); RooRealVar bin2(\"bkg_SR_bin2\",\"Background yield in signal region, bin 2\",50,0,500); RooRealVar bin3(\"bkg_SR_bin3\",\"Background yield in signal region, bin 3\",25,0,500); RooRealVar bin4(\"bkg_SR_bin4\",\"Background yield in signal region, bin 4\",10,0,500); RooArgList bkg_SR_bins; bkg_SR_bins.add(bin1); bkg_SR_bins.add(bin2); bkg_SR_bins.add(bin3); bkg_SR_bins.add(bin4); // Create a RooParametericHist which contains those yields, last argument is just for the binning, // can use the data TH1 for that RooParametricHist p_bkg(\"bkg_SR\", \"Background PDF in signal region\",met,bkg_SR_bins,data_th1); // Always include a _norm term which should be the sum of the yields (thats how combine likes to play with pdfs) RooAddition p_bkg_norm(\"bkg_SR_norm\",\"Total Number of events from background in signal region\",bkg_SR_bins); // Every signal region needs a signal TH1F signal_th1(\"signal_SR\",\"Signal expected in signal region\",nbins,xbins); signal_th1.SetBinContent(1,1); signal_th1.SetBinContent(2,2); signal_th1.SetBinContent(3,3); signal_th1.SetBinContent(4,8); RooDataHist signal_hist(\"signal\",\"Data observed\",vars,&signal_th1); wspace.import(signal_hist); // -------------------------------------------------------------------------------------------------------------// // ---------------------------- CONTROL REGION -----------------------------------------------------------------// TH1F data_CRth1(\"data_obs_CR\",\"Data observed in control region\",nbins,xbins); data_CRth1.SetBinContent(1,200); data_CRth1.SetBinContent(2,100); data_CRth1.SetBinContent(3,50); data_CRth1.SetBinContent(4,20); RooDataHist data_CRhist(\"data_obs_CR\",\"Data observed\",vars,&data_CRth1); wspace.import(data_CRhist); // This time, the background process will be dependent on the yields of the background in the signal region. // The transfer factor TF must account for acceptance/efficiency etc differences in the signal to control // In this example lets assume the control region is populated by the same process decaying to clean daughters with 2xBR // compared to the signal region // NB You could have a different transfer factor for each bin represented by a completely different RooRealVar // We can imagine that the transfer factor could be associated with some uncertainty - lets say a 1% uncertainty due to efficiency and 2% due to acceptance. // We need to make these nuisance parameters ourselves and give them a nominal value of 0 RooRealVar efficiency(\"efficiency\", \"efficiency nuisance parameter\",0); RooRealVar acceptance(\"acceptance\", \"acceptance nuisance parameter\",0); // We would need to make the transfer factor a function of those too. Here we've assumed Log-normal effects (i.e the same as putting lnN in the CR datacard) // but note that we could use any function which could be used to parameterise the effect - eg if the systematic is due to some alternate template, we could // use polynomials for example. RooFormulaVar TF(\"TF\",\"Trasnfer factor\",\"2*TMath::Power(1.01,@0)*TMath::Power(1.02,@1)\",RooArgList(efficiency,acceptance) ); // Finally, we need to make each bin of the background in the control region a function of the background in the signal and the transfer factor // N_CR = N_SR x TF RooFormulaVar CRbin1(\"bkg_CR_bin1\",\"Background yield in control region, bin 1\",\"@0*@1\",RooArgList(TF,bin1)); RooFormulaVar CRbin2(\"bkg_CR_bin2\",\"Background yield in control region, bin 2\",\"@0*@1\",RooArgList(TF,bin2)); RooFormulaVar CRbin3(\"bkg_CR_bin3\",\"Background yield in control region, bin 3\",\"@0*@1\",RooArgList(TF,bin3)); RooFormulaVar CRbin4(\"bkg_CR_bin4\",\"Background yield in control region, bin 4\",\"@0*@1\",RooArgList(TF,bin4)); RooArgList bkg_CR_bins; bkg_CR_bins.add(CRbin1); bkg_CR_bins.add(CRbin2); bkg_CR_bins.add(CRbin3); bkg_CR_bins.add(CRbin4); RooParametricHist p_CRbkg(\"bkg_CR\", \"Background PDF in control region\",met,bkg_CR_bins,data_th1); RooAddition p_CRbkg_norm(\"bkg_CR_norm\",\"Total Number of events from background in control region\",bkg_CR_bins); // -------------------------------------------------------------------------------------------------------------// // we can also use the standard interpolation from combine by providing alternative shapes (as RooDataHists) // here we're adding two of them (JES and ISR) TH1F background_up(\"tbkg_CR_JESUp\",\"\",nbins,xbins); background_up.SetBinContent(1,CRbin1.getVal()*1.01); background_up.SetBinContent(2,CRbin2.getVal()*1.02); background_up.SetBinContent(3,CRbin3.getVal()*1.03); background_up.SetBinContent(4,CRbin4.getVal()*1.04); RooDataHist bkg_CRhist_sysUp(\"bkg_CR_JESUp\",\"Bkg sys up\",vars,&background_up); wspace.import(bkg_CRhist_sysUp); TH1F background_down(\"bkg_CR_JESDown\",\"\",nbins,xbins); background_down.SetBinContent(1,CRbin1.getVal()*0.90); background_down.SetBinContent(2,CRbin2.getVal()*0.98); background_down.SetBinContent(3,CRbin3.getVal()*0.97); background_down.SetBinContent(4,CRbin4.getVal()*0.96); RooDataHist bkg_CRhist_sysDown(\"bkg_CR_JESDown\",\"Bkg sys down\",vars,&background_down); wspace.import(bkg_CRhist_sysDown); TH1F background_2up(\"tbkg_CR_ISRUp\",\"\",nbins,xbins); background_2up.SetBinContent(1,CRbin1.getVal()*0.85); background_2up.SetBinContent(2,CRbin2.getVal()*0.9); background_2up.SetBinContent(3,CRbin3.getVal()*0.95); background_2up.SetBinContent(4,CRbin4.getVal()*0.99); RooDataHist bkg_CRhist_sys2Up(\"bkg_CR_ISRUp\",\"Bkg sys 2up\",vars,&background_2up); wspace.import(bkg_CRhist_sys2Up); TH1F background_2down(\"bkg_CR_ISRDown\",\"\",nbins,xbins); background_2down.SetBinContent(1,CRbin1.getVal()*1.15); background_2down.SetBinContent(2,CRbin2.getVal()*1.1); background_2down.SetBinContent(3,CRbin3.getVal()*1.05); background_2down.SetBinContent(4,CRbin4.getVal()*1.01); RooDataHist bkg_CRhist_sys2Down(\"bkg_CR_ISRDown\",\"Bkg sys 2down\",vars,&background_2down); wspace.import(bkg_CRhist_sys2Down); // import the pdfs wspace.import(p_bkg); wspace.import(p_bkg_norm,RooFit::RecycleConflictNodes()); wspace.import(p_CRbkg); wspace.import(p_CRbkg_norm,RooFit::RecycleConflictNodes()); fOut->cd(); wspace.Write(); // Clean up fOut->Close(); fOut->Delete(); } Lets go through what the script is doing. First, the observable for the search is the missing energy so we create a parameter to represent that. RooRealVar met(\"met\",\"E_{T}^{miss}\",xmin,xmax); First, the following lines create a freely floating parameter for each of our bins (in this example, there are only 4 bins, defined for our observable met . RooRealVar bin1(\"bkg_SR_bin1\",\"Background yield in signal region, bin 1\",100,0,500); RooRealVar bin2(\"bkg_SR_bin2\",\"Background yield in signal region, bin 2\",50,0,500); RooRealVar bin3(\"bkg_SR_bin3\",\"Background yield in signal region, bin 3\",25,0,500); RooRealVar bin4(\"bkg_SR_bin4\",\"Background yield in signal region, bin 4\",10,0,500); RooArgList bkg_SR_bins; bkg_SR_bins.add(bin1); bkg_SR_bins.add(bin2); bkg_SR_bins.add(bin3); bkg_SR_bins.add(bin4); They are put into a list so that we can create a RooParametricHist and its normalisation from that list RooParametricHist p_bkg(\"bkg_SR\", \"Background PDF in signal region\",met,bkg_SR_bins,data_th1); RooAddition p_bkg_norm(\"bkg_SR_norm\",\"Total Number of events from background in signal region\",bkg_SR_bins); For the control region, the background process will be dependent on the yields of the background in the signal region using a transfer factor . The transfer factor TF must account for acceptance/efficiency etc differences in the signal to control regions. In this example lets assume the control region is populated by the same process decaying to a different final state with twice as large branching ratio compared to the one in the signal region. We could imagine that the transfer factor could be associated with some uncertainty - lets say a 1% uncertainty due to efficiency and 2% due to acceptance. We need to make nuisance parameters ourselves to model this and give them a nominal value of 0. RooRealVar efficiency(\"efficiency\", \"efficiency nuisance parameter\",0); RooRealVar acceptance(\"acceptance\", \"acceptance nuisance parameter\",0); We need to make the transfer factor a function of these parameters since variations in these uncertainties will lead to variations of the transfer factor. Here we've assumed Log-normal effects (i.e the same as putting lnN in the CR datacard) but we could use any function which could be used to parameterise the effect - eg if the systematic is due to some alternate template, we could use polynomials for example. RooFormulaVar TF(\"TF\",\"Trasnfer factor\",\"2*TMath::Power(1.01,@0)*TMath::Power(1.02,@1)\",RooArgList(efficiency,acceptance) ); Then need to make each bin of the background in the control region a function of the background in the signal and the transfer factor - i.e $N_{CR} = N_{SR} \\times TF $. RooFormulaVar CRbin1(\"bkg_CR_bin1\",\"Background yield in control region, bin 1\",\"@0*@1\",RooArgList(TF,bin1)); RooFormulaVar CRbin2(\"bkg_CR_bin2\",\"Background yield in control region, bin 2\",\"@0*@1\",RooArgList(TF,bin2)); RooFormulaVar CRbin3(\"bkg_CR_bin3\",\"Background yield in control region, bin 3\",\"@0*@1\",RooArgList(TF,bin3)); RooFormulaVar CRbin4(\"bkg_CR_bin4\",\"Background yield in control region, bin 4\",\"@0*@1\",RooArgList(TF,bin4)); As before, we also need to create the RooParametricHist for this process in the control region but this time the bin yields will be the RooFormulaVars we just created instead of free floating parameters. RooArgList bkg_CR_bins; bkg_CR_bins.add(CRbin1); bkg_CR_bins.add(CRbin2); bkg_CR_bins.add(CRbin3); bkg_CR_bins.add(CRbin4); RooParametricHist p_CRbkg(\"bkg_CR\", \"Background PDF in control region\",met,bkg_CR_bins,data_th1); RooAddition p_CRbkg_norm(\"bkg_CR_norm\",\"Total Number of events from background in control region\",bkg_CR_bins); Finally, we can also create alternative shape variations (Up/Down) that can be fed to combine as we do with TH1 or RooDataHist type workspaces. These need to be of type RooDataHist . The example below is for a Jet Energy Scale type shape uncertainty. TH1F background_up(\"tbkg_CR_JESUp\",\"\",nbins,xbins); background_up.SetBinContent(1,CRbin1.getVal()*1.01); background_up.SetBinContent(2,CRbin2.getVal()*1.02); background_up.SetBinContent(3,CRbin3.getVal()*1.03); background_up.SetBinContent(4,CRbin4.getVal()*1.04); RooDataHist bkg_CRhist_sysUp(\"bkg_CR_JESUp\",\"Bkg sys up\",vars,&background_up); wspace.import(bkg_CRhist_sysUp); TH1F background_down(\"bkg_CR_JESDown\",\"\",nbins,xbins); background_down.SetBinContent(1,CRbin1.getVal()*0.90); background_down.SetBinContent(2,CRbin2.getVal()*0.98); background_down.SetBinContent(3,CRbin3.getVal()*0.97); background_down.SetBinContent(4,CRbin4.getVal()*0.96); RooDataHist bkg_CRhist_sysDown(\"bkg_CR_JESDown\",\"Bkg sys down\",vars,&background_down); wspace.import(bkg_CRhist_sysDown); Below are datacards (for signal and control regions) which can be used in conjunction with the workspace built above. In order to \"use\" the control region, simply combine the two cards as usual using combineCards.py . Signal Region Datacard -- signal category imax * number of bins jmax * number of processes minus 1 kmax * number of nuisance parameters ------------------------------------------------------------------------------------------------------------------------------------------- shapes data_obs signal param_ws.root wspace:data_obs_SR shapes background signal param_ws.root wspace:bkg_SR # the background model pdf which is freely floating, note other backgrounds can be added as usual shapes signal signal param_ws.root wspace:signal ------------------------------------------------------------------------------------------------------------------------------------------- bin signal observation -1 ------------------------------------------------------------------------------------------------------------------------------------------- # background rate must be taken from _norm param x 1 bin signal signal process background signal process 1 0 rate 1 -1 ------------------------------------------------------------------------------------------------------------------------------------------- # Normal uncertainties in the signal region lumi_8TeV lnN - 1.026 ------------------------------------------------------------------------------------------------------------------------------------------- # free floating parameters, we do not need to declare them, but its a good idea to bkg_SR_bin1 flatParam bkg_SR_bin2 flatParam bkg_SR_bin3 flatParam bkg_SR_bin4 flatParam Control Region Datacard -- control category imax * number of bins jmax * number of processes minus 1 kmax * number of nuisance parameters ------------------------------------------------------------------------------------------------------------------------------------------- shapes data_obs control param_ws.root wspace:data_obs_CR shapes background control param_ws.root wspace:bkg_CR wspace:bkg_CR_$SYSTEMATIC # the background model pdf which is dependant on that in the SR, note other backgrounds can be added as usual ------------------------------------------------------------------------------------------------------------------------------------------- bin control observation -1 ------------------------------------------------------------------------------------------------------------------------------------------- # background rate must be taken from _norm param x 1 bin control process background process 1 rate 1 ------------------------------------------------------------------------------------------------------------------------------------------- JES shape 1 ISR shape 1 efficiency param 0 1 acceptance param 0 1 Note that for the control region, our nuisance parameters appear as param types so that combine will correctly constrain them. If we combine the two cards and fit the result with -M MultiDimFit -v 3 we can see that the parameters which give the rate of background in each bin of the signal region, along with the nuisance parameters and signal strength, are determined by the fit - i.e we have properly included the constraint from the control region, just as with the 1-bin gmN . acceptance = 0.00374312 +/- 0.964632 (limited) bkg_SR_bin1 = 99.9922 +/- 5.92062 (limited) bkg_SR_bin2 = 49.9951 +/- 4.13535 (limited) bkg_SR_bin3 = 24.9915 +/- 2.9267 (limited) bkg_SR_bin4 = 9.96478 +/- 2.1348 (limited) efficiency = 0.00109195 +/- 0.979334 (limited) lumi_8TeV = -0.0025911 +/- 0.994458 r = 0.00716347 +/- 12.513 (limited) The example given here is extremely basic and it should be noted that additional complexity in the transfer factors, additional uncertainties/backgrounds etc in the cards are supported as always. Danger If trying to implement parametric uncertainties in this setup (eg on transfer factors) which are correlated with other channels and implemented separately, you MUST normalise the uncertainty effect so that the datacard line can read param name X 1 . That is the uncertainty on this parameter must be 1. Without this, there will be inconsistency with other nuisances of the same name in other channels implemented as shape or lnN .","title":"RooParametricHist gammaN for shapes"},{"location":"part3/regularisation/","text":"Unfolding & regularization This section details how to perform an unfolded cross-section measurement, including regularization , inside Combine. There are many resources available that describe unfolding, including when to use it (or not), and what are the usual issues around it. A useful summary is available at the CMS Statistics Committee pages on unfolding. You can also find a nice overview of unfolding and its usage in combine in these slides . The basic idea behind the unfolding technique to describe smearing introduced through the reconstruction (eg of the particle energy) in a given truth level bin x_{i} x_{i} through a linear relationship to the effects in the nearby truth-bins. We can make statements about the probability p_{j} p_{j} that the event falling in the truth bin x_{i} x_{i} is reconstructed in the bin y_{i} y_{i} via the linear relationship, y_{obs} = \\tilde{\\boldsymbol{R}}\\cdot x_{true} + b y_{obs} = \\tilde{\\boldsymbol{R}}\\cdot x_{true} + b or, if the truth bins are expressed relative to some particular model, we use the usual signal strength terminology, y_{obs} = \\boldsymbol{R}\\cdot \\mu + b y_{obs} = \\boldsymbol{R}\\cdot \\mu + b Unfolding aims to find the distribution at truth level x x , given the observations y y at reco-level. Likelihood-based unfolding Since Combine has access to the full likelihood for any analysis written in the usual datacard format, we will use likelihood-based unfolding throughout - for other approaches, there are many other tools available (eg RooUnfold or TUnfold ), which can be used instead. The benefits of the likelihood-based approach are that, Background subtraction is accounted for directly in the likelihood Systematic uncertainties are accounted for directly during the unfolding as nuisance parameters We can profile the nuisance parameters during the unfolding to make the most of the data available In practice, one must construct the response matrix and unroll it in the reconstructed bins: First, one derives the truth distribution, eg after the generator-cut only, x_{i} x_{i} . Each reconstructed bin (eg each datacard) should describe the contribution from each truth bin - this is how combine knows about \\boldsymbol{R} \\boldsymbol{R} and folds in the acceptance/efficiency effects as usual. The out-of-acceptance contributions can also be included in the above. The model we use for this is then just the usual PhysicsModel:multiSignalModel , where each signal refers to a particular truth level bin. The results can be extracted through a simple maximum likelihood fit with, text2workspace.py -m 125 --X-allow-no-background -o datacard.root datacard.txt -P HiggsAnalysis.CombinedLimit.PhysicsModel:multiSignalModel --PO map='.*GenBin0.*:r_Bin0[1,-1,20]' --PO map='.*GenBin1.*:r_Bin1[1,-1,20]' --PO map='.*GenBin2.*:r_Bin2[1,-1,20]' --PO map='.*GenBin3.*:r_Bin3[1,-1,20]' --PO map='.*GenBin4.*:r_Bin4[1,-1,20]' combine -M MultiDimFit --setParameters=r_Bin0=1,r_Bin1=1,r_Bin2=1,r_Bin3=1,r_Bin4=1 -t -1 -m 125 datacard.root combine -M MultiDimFit --setParameters=r_Bin0=1,r_Bin1=1,r_Bin2=1,r_Bin3=1,r_Bin4=1 -t -1 -m 125 --algo=grid --points=100 -P r_Bin1 --setParameterRanges r_Bin1=0.5,1.5 --floatOtherPOIs=1 datacard.root Notice that one can also perform the so called bin-by-bin unfolding (though it is strongly discouraged except for testing) with, text2workspace.py -m 125 --X-allow-no-background -o datacard.root datacard.txt -P HiggsAnalysis.CombinedLimit.PhysicsModel:multiSignalModel --PO map='.*RecoBin0.*:r_Bin0[1,-1,20]' --PO map='.*RecoBin1.*:r_Bin1[1,-1,20]' --PO map='.*RecoBin2.*:r_Bin2[1,-1,20]' --PO map='.*RecoBin3.*:r_Bin3[1,-1,20]' --PO map='.*RecoBin4.*:r_Bin4[1,-1,20]' Nuisance parameters can be added to the likelihood function and profiled in the usual way via the datacards. Theory uncertainties on the inclusive cross section are typically not included in unfolded measurements. Regularization The main difference with respect to other models with multiple signal contributions is the introduction of Regularization , which is used to stabilize the unfolding process. An example of unfolding in combine with and without regularization, can be found under data/tutorials/regularization . Running python createWs.py [-r] will create a simple datacard and perform a fit both with and without including regularization. The simplest way to introduce regularization in the likelihood based approach, is to apply a penalty term in the likelihood function which depends on the values of the truth bins (so called Tickonov regularization ): -2\\ln L = -2\\ln L + P(\\vec{x}) -2\\ln L = -2\\ln L + P(\\vec{x}) where P P is a linear operator. There are two different approches which are supported to construct P P . If instead you run python makeModel.py , you will create a more complex datacard with each the two regularization scheme implemented. You will need to uncomment the relevant sections of code to activate SVD or TUnfold type regularization. Warning Any unfolding method which makes use of regularization must perform studies of the potential bias/coverage properties introduced through the inclusion of regularization, and how strong the associated regularization is. Advice on this can be found in the CMS Statistics Committee pages. Singular Value Decomposition (SVD) In the SVD approach - as described in the SVD paper - the penalty term is constructed directly based on the strengths ( \\vec{\\mu}=\\{\\mu_{i}\\}_{i=1}^{N} \\vec{\\mu}=\\{\\mu_{i}\\}_{i=1}^{N} ), P = \\tau\\left| A\\cdot \\vec{\\mu} \\right|^{2}, P = \\tau\\left| A\\cdot \\vec{\\mu} \\right|^{2}, where A A is typically the discrete curvature matrix, with A = \\begin{bmatrix} 1 & -1 & ... \\\\ 1 & -2 & 1 & ... \\\\ ... \\end{bmatrix} A = \\begin{bmatrix} 1 & -1 & ... \\\\ 1 & -2 & 1 & ... \\\\ ... \\end{bmatrix} Penalty terms on the derivatives can also be included. Such a penalty term is included by modifying the likelihood to include one constraint for each row of the product A\\cdot\\vec{\\mu} A\\cdot\\vec{\\mu} , by including them as lines in the datacard of the form, name constr formula dependents delta where the regularization strength \\delta=\\frac{1}{\\sqrt{\\tau}} \\delta=\\frac{1}{\\sqrt{\\tau}} and can either be a fixed value (eg by putting directly 0.01 ) or as a modifiable parameter with eg delta[0.01] . For example, for 3 bins and a regularization strength of 0.03, the first line would be name constr @0-2*@2+@1 r_Bin0,r_Bin1,r_Bin2 0.03 Alternative, valid syntaxes are constr1 constr r_bin0-r_bin1 0.01 constr1 constr r_bin0-r_bin1 delta[0.01] constr1 constr r_bin0+r_bin1 r_bin0,r_bin1 0.01 constr1 constr r_bin0+r_bin1 {r_bin0,r_bin1} delta[0.01] TUnfold method The Tikhonov regularization as implemented in TUnfold uses the MC information, or rather the densities prediction, as a bias vector. In order to give this information to Combine, a single datacard for each reco-level bin needs to be produced, so that we have access to the proper normalization terms during the minimization. In this case the bias vector is \\vec{x}_{obs}-\\vec{x}_{true} \\vec{x}_{obs}-\\vec{x}_{true} Then one can write a constraint term in the datacard via (eg.) constr1 constr (r_Bin0-1.)*(shapeSig_GenBin0_RecoBin0__norm+shapeSig_GenBin0_RecoBin1__norm+shapeSig_GenBin0_RecoBin2__norm+shapeSig_GenBin0_RecoBin3__norm+shapeSig_GenBin0_RecoBin4__norm)+(r_Bin2-1.)*(shapeSig_GenBin2_RecoBin0__norm+shapeSig_GenBin2_RecoBin1__norm+shapeSig_GenBin2_RecoBin2__norm+shapeSig_GenBin2_RecoBin3__norm+shapeSig_GenBin2_RecoBin4__norm)-2*(r_Bin1-1.)*(shapeSig_GenBin1_RecoBin0__norm+shapeSig_GenBin1_RecoBin1__norm+shapeSig_GenBin1_RecoBin2__norm+shapeSig_GenBin1_RecoBin3__norm+shapeSig_GenBin1_RecoBin4__norm) {r_Bin0,r_Bin1,r_Bin2,shapeSig_GenBin1_RecoBin0__norm,shapeSig_GenBin0_RecoBin0__norm,shapeSig_GenBin2_RecoBin0__norm,shapeSig_GenBin1_RecoBin1__norm,shapeSig_GenBin0_RecoBin1__norm,shapeSig_GenBin2_RecoBin1__norm,shapeSig_GenBin1_RecoBin2__norm,shapeSig_GenBin0_RecoBin2__norm,shapeSig_GenBin2_RecoBin2__norm,shapeSig_GenBin1_RecoBin3__norm,shapeSig_GenBin0_RecoBin3__norm,shapeSig_GenBin2_RecoBin3__norm,shapeSig_GenBin1_RecoBin4__norm,shapeSig_GenBin0_RecoBin4__norm,shapeSig_GenBin2_RecoBin4__norm} delta[0.03]","title":"Unfolding & regularization"},{"location":"part3/regularisation/#unfolding-regularization","text":"This section details how to perform an unfolded cross-section measurement, including regularization , inside Combine. There are many resources available that describe unfolding, including when to use it (or not), and what are the usual issues around it. A useful summary is available at the CMS Statistics Committee pages on unfolding. You can also find a nice overview of unfolding and its usage in combine in these slides . The basic idea behind the unfolding technique to describe smearing introduced through the reconstruction (eg of the particle energy) in a given truth level bin x_{i} x_{i} through a linear relationship to the effects in the nearby truth-bins. We can make statements about the probability p_{j} p_{j} that the event falling in the truth bin x_{i} x_{i} is reconstructed in the bin y_{i} y_{i} via the linear relationship, y_{obs} = \\tilde{\\boldsymbol{R}}\\cdot x_{true} + b y_{obs} = \\tilde{\\boldsymbol{R}}\\cdot x_{true} + b or, if the truth bins are expressed relative to some particular model, we use the usual signal strength terminology, y_{obs} = \\boldsymbol{R}\\cdot \\mu + b y_{obs} = \\boldsymbol{R}\\cdot \\mu + b Unfolding aims to find the distribution at truth level x x , given the observations y y at reco-level.","title":"Unfolding &amp; regularization"},{"location":"part3/regularisation/#likelihood-based-unfolding","text":"Since Combine has access to the full likelihood for any analysis written in the usual datacard format, we will use likelihood-based unfolding throughout - for other approaches, there are many other tools available (eg RooUnfold or TUnfold ), which can be used instead. The benefits of the likelihood-based approach are that, Background subtraction is accounted for directly in the likelihood Systematic uncertainties are accounted for directly during the unfolding as nuisance parameters We can profile the nuisance parameters during the unfolding to make the most of the data available In practice, one must construct the response matrix and unroll it in the reconstructed bins: First, one derives the truth distribution, eg after the generator-cut only, x_{i} x_{i} . Each reconstructed bin (eg each datacard) should describe the contribution from each truth bin - this is how combine knows about \\boldsymbol{R} \\boldsymbol{R} and folds in the acceptance/efficiency effects as usual. The out-of-acceptance contributions can also be included in the above. The model we use for this is then just the usual PhysicsModel:multiSignalModel , where each signal refers to a particular truth level bin. The results can be extracted through a simple maximum likelihood fit with, text2workspace.py -m 125 --X-allow-no-background -o datacard.root datacard.txt -P HiggsAnalysis.CombinedLimit.PhysicsModel:multiSignalModel --PO map='.*GenBin0.*:r_Bin0[1,-1,20]' --PO map='.*GenBin1.*:r_Bin1[1,-1,20]' --PO map='.*GenBin2.*:r_Bin2[1,-1,20]' --PO map='.*GenBin3.*:r_Bin3[1,-1,20]' --PO map='.*GenBin4.*:r_Bin4[1,-1,20]' combine -M MultiDimFit --setParameters=r_Bin0=1,r_Bin1=1,r_Bin2=1,r_Bin3=1,r_Bin4=1 -t -1 -m 125 datacard.root combine -M MultiDimFit --setParameters=r_Bin0=1,r_Bin1=1,r_Bin2=1,r_Bin3=1,r_Bin4=1 -t -1 -m 125 --algo=grid --points=100 -P r_Bin1 --setParameterRanges r_Bin1=0.5,1.5 --floatOtherPOIs=1 datacard.root Notice that one can also perform the so called bin-by-bin unfolding (though it is strongly discouraged except for testing) with, text2workspace.py -m 125 --X-allow-no-background -o datacard.root datacard.txt -P HiggsAnalysis.CombinedLimit.PhysicsModel:multiSignalModel --PO map='.*RecoBin0.*:r_Bin0[1,-1,20]' --PO map='.*RecoBin1.*:r_Bin1[1,-1,20]' --PO map='.*RecoBin2.*:r_Bin2[1,-1,20]' --PO map='.*RecoBin3.*:r_Bin3[1,-1,20]' --PO map='.*RecoBin4.*:r_Bin4[1,-1,20]' Nuisance parameters can be added to the likelihood function and profiled in the usual way via the datacards. Theory uncertainties on the inclusive cross section are typically not included in unfolded measurements.","title":"Likelihood-based unfolding"},{"location":"part3/regularisation/#regularization","text":"The main difference with respect to other models with multiple signal contributions is the introduction of Regularization , which is used to stabilize the unfolding process. An example of unfolding in combine with and without regularization, can be found under data/tutorials/regularization . Running python createWs.py [-r] will create a simple datacard and perform a fit both with and without including regularization. The simplest way to introduce regularization in the likelihood based approach, is to apply a penalty term in the likelihood function which depends on the values of the truth bins (so called Tickonov regularization ): -2\\ln L = -2\\ln L + P(\\vec{x}) -2\\ln L = -2\\ln L + P(\\vec{x}) where P P is a linear operator. There are two different approches which are supported to construct P P . If instead you run python makeModel.py , you will create a more complex datacard with each the two regularization scheme implemented. You will need to uncomment the relevant sections of code to activate SVD or TUnfold type regularization. Warning Any unfolding method which makes use of regularization must perform studies of the potential bias/coverage properties introduced through the inclusion of regularization, and how strong the associated regularization is. Advice on this can be found in the CMS Statistics Committee pages.","title":"Regularization"},{"location":"part3/regularisation/#singular-value-decomposition-svd","text":"In the SVD approach - as described in the SVD paper - the penalty term is constructed directly based on the strengths ( \\vec{\\mu}=\\{\\mu_{i}\\}_{i=1}^{N} \\vec{\\mu}=\\{\\mu_{i}\\}_{i=1}^{N} ), P = \\tau\\left| A\\cdot \\vec{\\mu} \\right|^{2}, P = \\tau\\left| A\\cdot \\vec{\\mu} \\right|^{2}, where A A is typically the discrete curvature matrix, with A = \\begin{bmatrix} 1 & -1 & ... \\\\ 1 & -2 & 1 & ... \\\\ ... \\end{bmatrix} A = \\begin{bmatrix} 1 & -1 & ... \\\\ 1 & -2 & 1 & ... \\\\ ... \\end{bmatrix} Penalty terms on the derivatives can also be included. Such a penalty term is included by modifying the likelihood to include one constraint for each row of the product A\\cdot\\vec{\\mu} A\\cdot\\vec{\\mu} , by including them as lines in the datacard of the form, name constr formula dependents delta where the regularization strength \\delta=\\frac{1}{\\sqrt{\\tau}} \\delta=\\frac{1}{\\sqrt{\\tau}} and can either be a fixed value (eg by putting directly 0.01 ) or as a modifiable parameter with eg delta[0.01] . For example, for 3 bins and a regularization strength of 0.03, the first line would be name constr @0-2*@2+@1 r_Bin0,r_Bin1,r_Bin2 0.03 Alternative, valid syntaxes are constr1 constr r_bin0-r_bin1 0.01 constr1 constr r_bin0-r_bin1 delta[0.01] constr1 constr r_bin0+r_bin1 r_bin0,r_bin1 0.01 constr1 constr r_bin0+r_bin1 {r_bin0,r_bin1} delta[0.01]","title":"Singular Value Decomposition (SVD)"},{"location":"part3/regularisation/#tunfold-method","text":"The Tikhonov regularization as implemented in TUnfold uses the MC information, or rather the densities prediction, as a bias vector. In order to give this information to Combine, a single datacard for each reco-level bin needs to be produced, so that we have access to the proper normalization terms during the minimization. In this case the bias vector is \\vec{x}_{obs}-\\vec{x}_{true} \\vec{x}_{obs}-\\vec{x}_{true} Then one can write a constraint term in the datacard via (eg.) constr1 constr (r_Bin0-1.)*(shapeSig_GenBin0_RecoBin0__norm+shapeSig_GenBin0_RecoBin1__norm+shapeSig_GenBin0_RecoBin2__norm+shapeSig_GenBin0_RecoBin3__norm+shapeSig_GenBin0_RecoBin4__norm)+(r_Bin2-1.)*(shapeSig_GenBin2_RecoBin0__norm+shapeSig_GenBin2_RecoBin1__norm+shapeSig_GenBin2_RecoBin2__norm+shapeSig_GenBin2_RecoBin3__norm+shapeSig_GenBin2_RecoBin4__norm)-2*(r_Bin1-1.)*(shapeSig_GenBin1_RecoBin0__norm+shapeSig_GenBin1_RecoBin1__norm+shapeSig_GenBin1_RecoBin2__norm+shapeSig_GenBin1_RecoBin3__norm+shapeSig_GenBin1_RecoBin4__norm) {r_Bin0,r_Bin1,r_Bin2,shapeSig_GenBin1_RecoBin0__norm,shapeSig_GenBin0_RecoBin0__norm,shapeSig_GenBin2_RecoBin0__norm,shapeSig_GenBin1_RecoBin1__norm,shapeSig_GenBin0_RecoBin1__norm,shapeSig_GenBin2_RecoBin1__norm,shapeSig_GenBin1_RecoBin2__norm,shapeSig_GenBin0_RecoBin2__norm,shapeSig_GenBin2_RecoBin2__norm,shapeSig_GenBin1_RecoBin3__norm,shapeSig_GenBin0_RecoBin3__norm,shapeSig_GenBin2_RecoBin3__norm,shapeSig_GenBin1_RecoBin4__norm,shapeSig_GenBin0_RecoBin4__norm,shapeSig_GenBin2_RecoBin4__norm} delta[0.03]","title":"TUnfold method"},{"location":"part3/runningthetool/","text":"How to run the tool The executable combine provided by the package allows to use the Higgs Combination Tool indicating by command line which is the method to use for limit combination and which are user's preferences to run it. To see the entire list of all available options ask for the help: combine --help The option -M allows to chose the method used. There are several groups of statistical methods: Asymptotic likelihood methods: AsymptoticLimits : limits calculated according to the asymptotic formulas in arxiv:1007.1727 Significance : simple profile likelihood approximation, for calculating significances. Bayesian methods: BayesianSimple : performing a classical numerical integration (for simple models only) MarkovChainMC : performing Markov Chain integration, for arbitrarily complex models. Frequentist or hybrid bayesian-frequentist methods: HybridNew : compute modified frequentist limits according to several possible prescriptions Fitting FitDiagnostics : performs maximum likelihood fits to extract the signal yield and provide diagnostic tools such as pre and post-fit models and correlations MultiDimFit : perform maximum likelihood fits in multiple parameters and likelihood scans Miscellaneous other modules that don't compute limits but use the same framework: GoodnessOfFit : perform a goodness of fit test for models including shape information using several GOF estimators ChannelConsistencyCheck : check how consistent are the individual channels of a combination are GenerateOnly : generate random or asimov toy datasets for use as input to other methods The command help is organized into five parts: Main options section indicates how to pass the datacard as input to the tool ( -d datacardName ) and how to choose the statistical method ( -M MethodName ) to compute a limit and level of verbosity for output -v Common statistics options include options common to different statistical methods such as --cl to specify the CL (default is 0.95) or -t to give the number of toy MC extractions required. Common input-output options . Is it possible to specify hypothesis point under analysis using -m or include specific string in output filename --name . Common miscellaneous options . Method specific options sections are dedicated to each method. By providing the Method name with the -M option, only the options for that specific method are shown in addition to the common options Those options reported above are just a sample of all available.The command --help provides documentation of all of them. Common command line options There are a number of useful command line options which can be used to alter the model (or parameters of the model) at run These are the most commonly used, generic options, -H : run first another faster algorithm (e.g. the ProfileLikelihood described below) to get a hint of the limit, allowing the real algorithm to converge more quickly. We strongly recommend to use this option when using MarkovChainMC, HybridNew or FeldmanCousins calculators, unless you know in which range your limit lies and you set it manually (the default is [0, 20] ) --rMax , --rMin : manually restrict the range of signal strengths to consider. For Bayesian limits with MCMC, rMax a rule of thumb is that rMax should be 3-5 times the limit (a too small value of rMax will bias your limit towards low values, since you are restricting the integration range, while a too large value will bias you to higher limits) --setParameters name=value[,name2=value2,...] sets the starting values of the parameters, useful e.g. when generating toy MC or when also setting the parameters as fixed. This option supports the use of regexp via by replacing name with rgx{some regular expression} . --setParameterRanges name=min,max[:name2=min2,max2:...] sets the ranges of the parameters (useful e.g. for scanning in MultiDimFit, or for Bayesian integration). This option supports the use of regexp via by replacing name with rgx{some regular expression} . --redefineSignalPOIs name[,name2,...] redefines the set of parameters of interest. if the parameters where constant in the input workspace, they are re-defined to be floating. nuisances promoted to parameters of interest are removed from the list of nuisances, and thus they are not randomized in methods that randomize nuisances (e.g. HybridNew in non-frequentist mode, or BayesianToyMC, or in toy generation with -t but without --toysFreq ). This doesn't have any impact on algorithms that don't randomize nuisances (e.g. fits, AsymptoticLimits, or HybridNew in fequentist mode) or on algorithms that treat all parameters in the same way (e.g. MarkovChainMC). Note that constraint terms for the nuisances are dropped after promotion to a POI using --redefineSignalPOI . To produce a likelihood scan for a nuisance parameter, using MultiDimFit with --algo grid , you should instead use the --parameters (-P) option which will not cause the loss of the constraint term when scanning. parameters of interest of the input workspace that are not selected by this command become unconstrained nuisance parameters, but they are not added to the list of nuisances so they will not be randomized (see above). --freezeParameters name1[,name2,...] Will freeze the parameters with the given names to their set values. This option supports the use of regexps via by replacing name with rgx{some regular expression} for matching to constrained nuisance parameters or var{some regular expression} for matching to any parameter. For example --freezeParameters rgx{CMS_scale_j.*} will freeze all constrained nuisance parameters with the prefix CMS_scale_j , while --freezeParameters var{.*rate_scale} will freeze any parameter (constrained nuisance or otherwise) with the suffix rate_scale . use the option --freezeParameters allConstrainedNuisances to freeze all nuisance parameters that have a constraint term (i.e not flatParams or rateParams or other freely floating parameters). similarly the option --floatParameters sets the parameter floating. groups of nuisances (constrained or otherwise), as defined in the datacard, can be frozen using --freezeNuisanceGroups . You can also specify to freeze nuisances which are not contained in a particular group using a ^ before the group name ( --freezeNuisanceGroups=^group_name will freeze everything except nuisance parameters in the group \"group_name\".) all constrained nuisance parameters (not flatParam or rateParam ) can be set floating using --floatAllNuisances . Warning Note that the floating/freezing options have a priority ordering from lowest to highest as floatParameters < freezeParameters < freezeNuisanceGroups < floatAllNuisances . Options with higher priority will override those with lower priority. --trackParameters name1[,name2,...] will add a branch to the output tree for each of the named parameters. This option supports the use of regexp via by replacing name with rgx{some regular expression} the name of the branch will be trackedParam_ name . the exact behaviour depends on the method. For example, when using MultiDimFit with the --algo scan , the value of the parameter at each point in the scan will be saved while for FitDiagnostics , only the value at the end of the method will be saved. --trackErrors name1[,name2,...] will add a branch to the output tree for the error of each of the named parameters. This option supports the use of regexp via by replacing name with rgx{some regular expression} the name of the branch will be trackedError_ name . the behaviour is the same as --trackParameters above. Generic Minimizer Options Combine uses its own minimizer class which is used to steer Minuit (via RooMinimizer) named the CascadeMinimizer . This allows for sequential minimization which can help in case a particular setting/algo fails. Also, the CascadeMinimizer knows about extra features of Combine such as discrete nuisance parameters. All of the fits which are performed in several of the methods available use this minimizer. This means that the fits can be tuned using these common options, --cminPoiOnlyFit : First, perform a fit floating only the parameters of interest. This can be useful to find, roughly, where the global minimum is. --cminPreScan : Do a scan before first minimization --cminPreFit arg: If set to a value N > 0, the minimizer will perform a pre-fit with strategy (N-1) with frozen nuisance parameters. --cminApproxPreFitTolerance arg : If non-zero, do first a pre-fit with this tolerance (or 10 times the final tolerance, whichever is largest) --cminApproxPreFitStrategy arg : Strategy to use in the pre-fit. The default is strategy 0. --cminDefaultMinimizerType arg : Set the default minimizer Type. Default is Minuit2. --cminDefaultMinimizerAlgo arg : Set the default minimizer Algo. The default is Migrad --cminDefaultMinimizerTolerance arg : Set the default minimizer Tolerance, the default is 0.1 --cminDefaultMinimizerStrategy arg : Set the default minimizer Strategy between 0 (speed), 1 (balance - default ), 2 (robustness). The Minuit documentation for this is pretty sparse but in general, 0 means evaluate the function less often, while 2 will waste function calls to get precise answers. An important note is that Hesse (error/correlation estimation) will be run only if the strategy is 1 or 2. --cminFallbackAlgo arg : Provides a list of fallback algorithms if the default minimizer fails. You can provide multiple ones using the syntax is Type[,algo],strategy[:tolerance] : eg --cminFallbackAlgo Minuit2,Simplex,0:0.1 will fall back to the simplex algo of Minuit2 with strategy 0 and a tolerance 0.1, while --cminFallbackAlgo Minuit2,1 will use the default algo (migrad) of Minuit2 with strategy 1. --cminSetZeroPoint (0/1) : Set the reference of the NLL to 0 when minimizing, this can help faster convergence to the minimum if the NLL itself is large. The default is true (1), set to 0 to turn off. The allowed combinations of minimizer types and minimizer algos are as follows Minimizer Type Minimizer Algo Minuit Migrad , Simplex , Combined , Scan Minuit2 Migrad , Simplex , Combined , Scan GSLMultiMin ConjugateFR , ConjugatePR , BFGS , BFGS2 , SteepestDescent More of these options can be found in the Cascade Minimizer options section when running --help . Output from combine Most methods will print the results of the computation to the screen, however, in addition, combine will also produce a root file containing a tree called limit with these results. The name of this file will be of the format, higgsCombineTest.MethodName.mH$MASS.[word$WORD].root where $WORD is any user defined keyword from the datacard which has been set to a particular value. A few command line options of combine can be used to control this output: The option -n allows you to specify part of the name of the rootfile. e.g. if you do -n HWW the roofile will be called higgsCombineHWW.... instead of higgsCombineTest The option -m allows you to specify the higgs boson mass, which gets written in the filename and also in the tree (this simplifies the bookeeping because you can merge together multiple trees corresponding to different higgs masses using hadd and then use the tree to plot the value of the limit vs mass) (default is m=120) The option -s allows to specify the seed (eg -s 12345 ) used in toy generation. If this option is given, the name of the file will be extended by this seed, eg higgsCombineTest.AsymptoticLimits.mH120.12345.root The option --keyword-value allows you to specify the value of a keyword in the datacard such that $WORD (in the datacard) will be given the value of VALUE in the command --keyword-value WORD=VALUE , eg higgsCombineTest.AsymptoticLimits.mH120.WORDVALUE.12345.root The output file will contain a TDirectory named toys , which will be empty if no toys are generated (see below for details) and a TTree called limit with the following branches; Branch name Type Description limit Double_t Main result of combine run with method dependent meaning limitErr Double_t Estimated uncertainty on the result mh Double_t Value of MH , specified with -m option iToy Int_t Toy number identifier if running with -t iSeed Int_t Seed specified with -s t_cpu Float_t Estimated CPU time for algorithm t_real Float_t Estimated real time for algorithm quantileExpected Float_t Quantile identifier for methods which calculated expected (quantiles) and observed results (eg conversions from \\Delta\\ln L \\Delta\\ln L values) with method dependent meaning. Negative values are reserved for entries which do not related to quantiles of a calculation with the default being set to -1 (usually meaning the observed result). The value of any user defined keyword $WORD which is set using keyword-value described above will also be included as a branch with type string named WORD . The option can be repeated multiple times for multiple keywords. In some cases, the precise meanings of the branches will depend on the Method being used, which is included in this documentation. Toy data generation By default, each of these methods will be run using the observed data as the input. In several cases (as detailed below), it might be useful to run the tool using toy datasets, including Asimov data. The option -t is used to specify to combine to first generate a toy dataset(s) which will be used in replacement of the real data. There are two versions of this, -t N with N > 0. Combine will generate N toy datasets from the model and re-run the method once per toy. The seed for the toy generation can be modified with the option -s (use -s -1 for a random seed). The output file will contain one entry in the tree for each of these toys. -t -1 will produce an Asimov dataset in which statistical fluctuations are suppressed. The procedure to generate this Asimov dataset depends on which type of analysis you are using, see below for details. The output file will contain the toys (as RooDataSets for the observables, including global observables) in the toys directory if the option --saveToys is provided. If you include this option, the limit TTree in the output will have an entry corresponding to the state of the POI used for the generation of the toy, with the value of quantileExpected set to -2 . You can add additional branches using the --trackParameters option as described in the common command line options section above. Warning The default values of the nuisance parameters (or any parameter) are used to generate the toy. This means that if, for example, you are using parametric shapes and the parameters inside the workspace are set to arbitrary values, those arbitrary values will be used to generate the toy. This behaviour can be modified through the use of the option --setParameters x=value_x,y=value_y... which will set the values of the parameters ( x and y ) before toy generation. You can also load a snap-shot from a previous fit to set the nuisances to their post-fit values (see below). Asimov datasets If you are using wither -t -1 or using AsymptoticLimits , combine will calculate results based on an Asimov dataset. For counting experiments, the Asimov data will just be set to the total number of expected events (given the values of the nuisance parameters and POIs of the model) For shape analyses with templates, the Asimov dataset will be constructed as a histogram using the same binning which is defined for your analysis. If your model uses parametric shapes (for example when you are using binned data, there are some options as to what Asimov dataset to produce. By default , combine will produce the Asimov data as a histogram using the binning which is associated to each observable (ie as set using RooRealVar::setBins ). If this binning doesn't exist, combine will guess a suitable binning - it is therefore best to use RooRealVar::setBins to associate a binning to each observable, even if your data is unbinned, if you intend to use Asimov datasets. You can also ask combine to use a Pseudo-Asimov dataset, which is created from many weighted unbinned events. Setting --X-rtd TMCSO_AdaptivePseudoAsimov= \\beta \\beta with \\beta>0 \\beta>0 will trigger the internal logic of whether to produce a Pseudo-Asimov dataset. This logic is as follows; For each observable in your dataset, the number of bins, n_{b} n_{b} is determined either from the value of RooRealVar::getBins if it exists or assumed to be 100. If N_{b}=\\prod_{b}n_{b}>5000 N_{b}=\\prod_{b}n_{b}>5000 , the number of expected events N_{ev} N_{ev} is determined. Note if you are combining multiple channels, N_{ev} N_{ev} refers to the number of expected events in a single channel, the logic is separate for each channel. If N_{ev}/N_{b}<0.01 N_{ev}/N_{b}<0.01 then a Pseudo-Asimov dataset is created with the number of events equal to \\beta \\cdot \\mathrm{max}\\{100*N_{ev},1000\\} \\beta \\cdot \\mathrm{max}\\{100*N_{ev},1000\\} . If N_{ev}/N_{b}\\geq 0.01 N_{ev}/N_{b}\\geq 0.01 , then a normal Asimov dataset is produced. If N_{b}\\leq 5000 N_{b}\\leq 5000 then a normal Asimov dataset will be produced The production of a Pseudo-Asimov dataset can be forced by using the option --X-rtd TMCSO_PseudoAsimov=X where X>0 will determine the number of weighted events for the Pseudo-Asimov dataset. You should try different values of X since larger values leads to more events in the Pseudo-Asimov dataset resulting in higher precision but in general the fit will be slower. You can turn off the internal logic by setting --X-rtd TMCSO_AdaptivePseudoAsimov=0 --X-rtd TMCSO_PseudoAsimov=0 thereby forcing histograms to be generated. Info If you set --X-rtd TMCSO_PseudoAsimov=X with X>0 and also turn on --X-rtd TMCSO_AdaptivePseudoAsimov= \\beta \\beta , with \\beta>0 \\beta>0 , the internal logic will be used but this time the default will be to generate Pseudo-Asimov datasets, rather than the normal Asimov ones. Nuisance parameter generation The default method of dealing with systematics is to generate random values (around their nominal values, see above) for the nuisance parameters, according to their prior pdfs centred around their default values, before generating the data. The unconstrained nuisance parameters (eg flatParam or rateParam ) or those with flat priors are not randomised before the data generation. The following are options which define how the toys will be generated, --toysNoSystematics the nuisance parameters in each toy are not randomised when generating the toy datasets - i.e their nominal values are used to generate the data. Note that for methods which profile (fit) the nuisances, the parameters are still floating when evaluating the likelihood. --toysFrequentist the nuisance parameters in each toy are set to their nominal values which are obtained after fitting first to the data , with POIs fixed, before generating the data. For evaluating likelihoods, the constraint terms are instead randomised within their Gaussian constraint pdfs around the post-fit nuisance parameter values. If you are using toysFrequentist , be aware that the values set by --setParameters will be ignored for the toy generation as the post-fit values will instead be used (except for any parameter which is also a parameter of interest). You can override this behaviour and choose the nominal values for toy generation for any parameter by adding the option --bypassFrequentistFit which will skip the initial fit to data or by loading a snapshot (see below). Warning The methods such as AsymptoticLimits and HybridNew --LHCmode LHC-limits , the \"nominal\" nuisance parameter values are taken from fits to the data and are therefore not \"blind\" to the observed data by default (following the fully frequentist paradigm). See the detailed documentation on these methods for avoiding this and running in a completely \"blind\" mode. Generate only It is also possible to generate the toys first and then feed them to the Methods in combine. This can be done using -M GenerateOnly --saveToys . The toys can then be read and used with the other methods by specifying --toysFile=higgsCombineTest.GenerateOnly... and using the same options for the toy generation. Warning Some Methods also use toys within the method itself (eg AsymptoticLimits and HybridNew ). For these, you should not specify the toy generation with -t or the options above and instead follow the specific instructions. Loading snapshots Snapshots from workspaces can be loaded and used in order to generate toys using the option --snapshotName <name of snapshot> . This will first set the parameters to the values in the snapshot before any other parameter options are set and toys are generated. See the section on saving post-fit workspaces for creating workspaces with post-fit snapshots from MultiDimFit . Here are a few examples of calculations with toys from post-fit workspaces using a workspace with r, m_{H} r, m_{H} as parameters of interest Throw post-fit toy with b from s+b(floating r,m_{H} r,m_{H} ) fit, s with r=1.0 , m=best fit MH , using nuisance values and constraints re-centered on s+b(floating r,m_{H} r,m_{H} ) fit values (aka frequentist post-fit expected) and compute post-fit expected r uncertainty profiling MH combine higgsCombinemumhfit.MultiDimFit.mH125.root --snapshotName MultiDimFit -M MultiDimFit --verbose 9 -n randomtest --toysFrequentist --bypassFrequentistFit -t -1 --expectSignal=1 -P r --floatOtherPOIs=1 --algo singles Throw post-fit toy with b from s+b(floating r,m_{H} r,m_{H} ) fit, s with r=1.0, m=128.0 , using nuisance values and constraints re-centered on s+b(floating r,m_{H} r,m_{H} ) fit values (aka frequentist post-fit expected) and compute post-fit expected significance (with MH fixed at 128 implicitly) combine higgsCombinemumhfit.MultiDimFit.mH125.root -m 128 --snapshotName MultiDimFit -M ProfileLikelihood --significance --verbose 9 -n randomtest --toysFrequentist --bypassFrequentistFit --overrideSnapshotMass -t -1 --expectSignal=1 --redefineSignalPOIs r --freezeParameters MH Throw post-fit toy with b from s+b(floating r,m_{H} r,m_{H} ) fit, s with r=0.0 , using nuisance values and constraints re-centered on s+b(floating r,m_{H} r,m_{H} ) fit values (aka frequentist post-fit expected) and compute post-fit expected and observed asymptotic limit (with MH fixed at 128 implicitly) combine higgsCombinemumhfit.MultiDimFit.mH125.root -m 128 --snapshotName MultiDimFit -M AsymptoticLimits --verbose 9 -n randomtest --bypassFrequentistFit --overrideSnapshotMass--redefineSignalPOIs r --freezeParameters MH combineTool for job submission For longer tasks which cannot be run locally, several methods in combine can be split to run on the LSF batch or the Grid . The splitting and submission is handled using the combineTool (see this getting started section to get the tool) Submission to Condor The syntax for running on condor with the tool is combineTool.py -M ALGO [options] --job-mode condor --sub-opts='CLASSADS' --task-name NAME [--dry-run] with options being the usual list of combine options. The help option -h will give a list of both combine and combineTool sets of options. This can be used with several different methods from combine . The --sub-opts option takes a string with the different ClassAds that you want to set, separated by \\n as argument (e.g. '+JobFlavour=\"espresso\"\\nRequestCpus=1' ). The --dry-run option will show what will be run without actually doing so / submitting the jobs. For example, to generate toys (eg for use with limit setting) users running on lxplus at CERN the condor mode can be used eg combineTool.py -d workspace.root -M HybridNew --LHCmode LHC-limits --clsAcc 0 -T 2000 -s -1 --singlePoint 0.2:2.0:0.05 --saveHybridResult -m 125 --job-mode condor --task-name condor-test --sub-opts='+JobFlavour=\"tomorrow\"' The --singlePoint option is over-ridden so that this will produce a script for each value of the POI in the range 0.2 to 2.0 in steps of 0.05. You can merge multiple points into a script using --merge - e.g adding --merge 10 to the above command will mean that each job contains at most 10 of the values. The scripts are labelled by the --task-name option. These will be submitted directly to condor adding any options in --sub-opts to the condor submit script. Make sure multiple options are separated by \\n . The jobs will run and produce output in the current directory . Below is an example for splitting points in a multi-dimensional likelihood scan. Splitting jobs for a multi-dimensional likelihood scan The option --split-points issues the command to split the jobs for MultiDimFit when using --algo grid . The following example will split the jobs such that there are 10 points in each of the jobs, which will be submitted to the 8nh queue. combineTool.py datacard.txt -M MultiDimFit --algo grid --points 50 --rMin 0 --rMax 1 --job-mode condor --split-points 10 --sub-opts='+JobFlavour=\"workday\"' --task-name mytask -n mytask Remember, any usual options (such as redefining POIs or freezing parameters) are passed to combine and can be added to the command line for combineTool . Info The option -n NAME should be included to avoid overwriting output files as the jobs will be run inside the directory from which the command is issued. Running combine jobs on the Grid For more CPU-intensive tasks, for example determining limits for complex models using toys, it is generally not feasible to compute all the results interactively. Instead, these jobs can be submitted to the Grid. In this example we will use the HybridNew method of combine to determine an upper limit for a sub-channel of the Run 1 SM H\\rightarrow\\tau\\tau H\\rightarrow\\tau\\tau analysis. For full documentation, see the section on computing limits with toys . With this model it would take too long to find the limit in one go, so instead we create a set of jobs in which each one throws toys and builds up the test statistic distributions for a fixed value of the signal strength. These jobs can then be submitted to a batch system or to the Grid using crab3 . From the set of output distributions it is possible to extract the expected and observed limits. For this we will use combineTool.py First we need to build a workspace from the H\\rightarrow\\tau\\tau H\\rightarrow\\tau\\tau datacard , $ text2workspace.py data/tutorials/htt/125/htt_mt.txt -m 125 $ mv data/tutorials/htt/125/htt_mt.root ./ To get an idea of the range of signal strength values we will need to build test-statistic distributions for we will first use the AsymptoticLimits method of combine, $ combine -M Asymptotic htt_mt.root -m 125 << Combine >> [...] -- AsymptoticLimits (CLs) -- Observed Limit: r < 1.7384 Expected 2.5%: r < 0.4394 Expected 16.0%: r < 0.5971 Expected 50.0%: r < 0.8555 Expected 84.0%: r < 1.2340 Expected 97.5%: r < 1.7200 Based on this, a range of 0.2 to 2.0 should be suitable. We can use the same command for generating the distribution of test statistics with combineTool . The --singlePoint option is now enhanced to support expressions that generate a set of calls to combine with different values. The accepted syntax is of the form MIN:MAX:STEPSIZE , and multiple comma-separated expressions can be specified. The script also adds an option --dry-run which will not actually call combine but just prints out the commands that would be run, e.g, combineTool.py -M HybridNew -d htt_mt.root --LHCmode LHC-limits --singlePoint 0.2:2.0:0.2 -T 2000 -s -1 --saveToys --saveHybridResult -m 125 --dry-run ... [DRY-RUN]: combine -d htt_mt.root --LHCmode LHC-limits -T 2000 -s -1 --saveToys --saveHybridResult -M HybridNew -m 125 --singlePoint 0.2 -n .Test.POINT.0.2 [DRY-RUN]: combine -d htt_mt.root --LHCmode LHC-limits -T 2000 -s -1 --saveToys --saveHybridResult -M HybridNew -m 125 --singlePoint 0.4 -n .Test.POINT.0.4 [...] [DRY-RUN]: combine -d htt_mt.root --LHCmode LHC-limits -T 2000 -s -1 --saveToys --saveHybridResult -M HybridNew -m 125 --singlePoint 2.0 -n .Test.POINT.2.0 When the --dry-run option is removed each command will be run in sequence. Grid submission Submission to the grid with crab3 works in a similar way. Before doing so ensure that the crab3 environment has been sourced, then for compatibility reasons source the CMSSW environment again. We will use the example of generating a grid of test-statistic distributions for limits. $ source /cvmfs/cms.cern.ch/crab3/crab.sh; cmsenv $ combineTool.py -d htt_mt.root -M HybridNew --LHCmode LHC-limits --clsAcc 0 -T 2000 -s -1 --singlePoint 0.2:2.0:0.05 --saveToys --saveHybridResult -m 125 --job-mode crab3 --task-name grid-test --custom-crab custom_crab.py The option --custom-crab should point to a python file python containing a function of the form custom_crab(config) that will be used to modify the default crab configuration. You can use this to set the output site to your local grid site, or modify other options such as the voRole, or the site blacklist/whitelist. For example def custom_crab(config): print '>> Customising the crab config' config.Site.storageSite = 'T2_CH_CERN' config.Site.blacklist = ['SOME_SITE', 'SOME_OTHER_SITE'] Again it is possible to use the option --dry-run to see what the complete crab config will look like before actually submitted it. Once submitted the progress can be monitored using the standard crab commands. When all jobs are completed copy the output from your sites storage element to the local output folder. $ crab getoutput -d crab_grid-test # Now we have to un-tar the output files $ cd crab_grid-test/results/ $ for f in *.tar; do tar xf $f; done $ mv higgsCombine*.root ../../ $ cd ../../ These output files should be combined with hadd , after which we invoke combine as usual to calculate observed and expected limits from the merged grid as usual.","title":"Running the tool"},{"location":"part3/runningthetool/#how-to-run-the-tool","text":"The executable combine provided by the package allows to use the Higgs Combination Tool indicating by command line which is the method to use for limit combination and which are user's preferences to run it. To see the entire list of all available options ask for the help: combine --help The option -M allows to chose the method used. There are several groups of statistical methods: Asymptotic likelihood methods: AsymptoticLimits : limits calculated according to the asymptotic formulas in arxiv:1007.1727 Significance : simple profile likelihood approximation, for calculating significances. Bayesian methods: BayesianSimple : performing a classical numerical integration (for simple models only) MarkovChainMC : performing Markov Chain integration, for arbitrarily complex models. Frequentist or hybrid bayesian-frequentist methods: HybridNew : compute modified frequentist limits according to several possible prescriptions Fitting FitDiagnostics : performs maximum likelihood fits to extract the signal yield and provide diagnostic tools such as pre and post-fit models and correlations MultiDimFit : perform maximum likelihood fits in multiple parameters and likelihood scans Miscellaneous other modules that don't compute limits but use the same framework: GoodnessOfFit : perform a goodness of fit test for models including shape information using several GOF estimators ChannelConsistencyCheck : check how consistent are the individual channels of a combination are GenerateOnly : generate random or asimov toy datasets for use as input to other methods The command help is organized into five parts: Main options section indicates how to pass the datacard as input to the tool ( -d datacardName ) and how to choose the statistical method ( -M MethodName ) to compute a limit and level of verbosity for output -v Common statistics options include options common to different statistical methods such as --cl to specify the CL (default is 0.95) or -t to give the number of toy MC extractions required. Common input-output options . Is it possible to specify hypothesis point under analysis using -m or include specific string in output filename --name . Common miscellaneous options . Method specific options sections are dedicated to each method. By providing the Method name with the -M option, only the options for that specific method are shown in addition to the common options Those options reported above are just a sample of all available.The command --help provides documentation of all of them.","title":"How to run the tool"},{"location":"part3/runningthetool/#common-command-line-options","text":"There are a number of useful command line options which can be used to alter the model (or parameters of the model) at run These are the most commonly used, generic options, -H : run first another faster algorithm (e.g. the ProfileLikelihood described below) to get a hint of the limit, allowing the real algorithm to converge more quickly. We strongly recommend to use this option when using MarkovChainMC, HybridNew or FeldmanCousins calculators, unless you know in which range your limit lies and you set it manually (the default is [0, 20] ) --rMax , --rMin : manually restrict the range of signal strengths to consider. For Bayesian limits with MCMC, rMax a rule of thumb is that rMax should be 3-5 times the limit (a too small value of rMax will bias your limit towards low values, since you are restricting the integration range, while a too large value will bias you to higher limits) --setParameters name=value[,name2=value2,...] sets the starting values of the parameters, useful e.g. when generating toy MC or when also setting the parameters as fixed. This option supports the use of regexp via by replacing name with rgx{some regular expression} . --setParameterRanges name=min,max[:name2=min2,max2:...] sets the ranges of the parameters (useful e.g. for scanning in MultiDimFit, or for Bayesian integration). This option supports the use of regexp via by replacing name with rgx{some regular expression} . --redefineSignalPOIs name[,name2,...] redefines the set of parameters of interest. if the parameters where constant in the input workspace, they are re-defined to be floating. nuisances promoted to parameters of interest are removed from the list of nuisances, and thus they are not randomized in methods that randomize nuisances (e.g. HybridNew in non-frequentist mode, or BayesianToyMC, or in toy generation with -t but without --toysFreq ). This doesn't have any impact on algorithms that don't randomize nuisances (e.g. fits, AsymptoticLimits, or HybridNew in fequentist mode) or on algorithms that treat all parameters in the same way (e.g. MarkovChainMC). Note that constraint terms for the nuisances are dropped after promotion to a POI using --redefineSignalPOI . To produce a likelihood scan for a nuisance parameter, using MultiDimFit with --algo grid , you should instead use the --parameters (-P) option which will not cause the loss of the constraint term when scanning. parameters of interest of the input workspace that are not selected by this command become unconstrained nuisance parameters, but they are not added to the list of nuisances so they will not be randomized (see above). --freezeParameters name1[,name2,...] Will freeze the parameters with the given names to their set values. This option supports the use of regexps via by replacing name with rgx{some regular expression} for matching to constrained nuisance parameters or var{some regular expression} for matching to any parameter. For example --freezeParameters rgx{CMS_scale_j.*} will freeze all constrained nuisance parameters with the prefix CMS_scale_j , while --freezeParameters var{.*rate_scale} will freeze any parameter (constrained nuisance or otherwise) with the suffix rate_scale . use the option --freezeParameters allConstrainedNuisances to freeze all nuisance parameters that have a constraint term (i.e not flatParams or rateParams or other freely floating parameters). similarly the option --floatParameters sets the parameter floating. groups of nuisances (constrained or otherwise), as defined in the datacard, can be frozen using --freezeNuisanceGroups . You can also specify to freeze nuisances which are not contained in a particular group using a ^ before the group name ( --freezeNuisanceGroups=^group_name will freeze everything except nuisance parameters in the group \"group_name\".) all constrained nuisance parameters (not flatParam or rateParam ) can be set floating using --floatAllNuisances . Warning Note that the floating/freezing options have a priority ordering from lowest to highest as floatParameters < freezeParameters < freezeNuisanceGroups < floatAllNuisances . Options with higher priority will override those with lower priority. --trackParameters name1[,name2,...] will add a branch to the output tree for each of the named parameters. This option supports the use of regexp via by replacing name with rgx{some regular expression} the name of the branch will be trackedParam_ name . the exact behaviour depends on the method. For example, when using MultiDimFit with the --algo scan , the value of the parameter at each point in the scan will be saved while for FitDiagnostics , only the value at the end of the method will be saved. --trackErrors name1[,name2,...] will add a branch to the output tree for the error of each of the named parameters. This option supports the use of regexp via by replacing name with rgx{some regular expression} the name of the branch will be trackedError_ name . the behaviour is the same as --trackParameters above.","title":"Common command line options"},{"location":"part3/runningthetool/#generic-minimizer-options","text":"Combine uses its own minimizer class which is used to steer Minuit (via RooMinimizer) named the CascadeMinimizer . This allows for sequential minimization which can help in case a particular setting/algo fails. Also, the CascadeMinimizer knows about extra features of Combine such as discrete nuisance parameters. All of the fits which are performed in several of the methods available use this minimizer. This means that the fits can be tuned using these common options, --cminPoiOnlyFit : First, perform a fit floating only the parameters of interest. This can be useful to find, roughly, where the global minimum is. --cminPreScan : Do a scan before first minimization --cminPreFit arg: If set to a value N > 0, the minimizer will perform a pre-fit with strategy (N-1) with frozen nuisance parameters. --cminApproxPreFitTolerance arg : If non-zero, do first a pre-fit with this tolerance (or 10 times the final tolerance, whichever is largest) --cminApproxPreFitStrategy arg : Strategy to use in the pre-fit. The default is strategy 0. --cminDefaultMinimizerType arg : Set the default minimizer Type. Default is Minuit2. --cminDefaultMinimizerAlgo arg : Set the default minimizer Algo. The default is Migrad --cminDefaultMinimizerTolerance arg : Set the default minimizer Tolerance, the default is 0.1 --cminDefaultMinimizerStrategy arg : Set the default minimizer Strategy between 0 (speed), 1 (balance - default ), 2 (robustness). The Minuit documentation for this is pretty sparse but in general, 0 means evaluate the function less often, while 2 will waste function calls to get precise answers. An important note is that Hesse (error/correlation estimation) will be run only if the strategy is 1 or 2. --cminFallbackAlgo arg : Provides a list of fallback algorithms if the default minimizer fails. You can provide multiple ones using the syntax is Type[,algo],strategy[:tolerance] : eg --cminFallbackAlgo Minuit2,Simplex,0:0.1 will fall back to the simplex algo of Minuit2 with strategy 0 and a tolerance 0.1, while --cminFallbackAlgo Minuit2,1 will use the default algo (migrad) of Minuit2 with strategy 1. --cminSetZeroPoint (0/1) : Set the reference of the NLL to 0 when minimizing, this can help faster convergence to the minimum if the NLL itself is large. The default is true (1), set to 0 to turn off. The allowed combinations of minimizer types and minimizer algos are as follows Minimizer Type Minimizer Algo Minuit Migrad , Simplex , Combined , Scan Minuit2 Migrad , Simplex , Combined , Scan GSLMultiMin ConjugateFR , ConjugatePR , BFGS , BFGS2 , SteepestDescent More of these options can be found in the Cascade Minimizer options section when running --help .","title":"Generic Minimizer Options"},{"location":"part3/runningthetool/#output-from-combine","text":"Most methods will print the results of the computation to the screen, however, in addition, combine will also produce a root file containing a tree called limit with these results. The name of this file will be of the format, higgsCombineTest.MethodName.mH$MASS.[word$WORD].root where $WORD is any user defined keyword from the datacard which has been set to a particular value. A few command line options of combine can be used to control this output: The option -n allows you to specify part of the name of the rootfile. e.g. if you do -n HWW the roofile will be called higgsCombineHWW.... instead of higgsCombineTest The option -m allows you to specify the higgs boson mass, which gets written in the filename and also in the tree (this simplifies the bookeeping because you can merge together multiple trees corresponding to different higgs masses using hadd and then use the tree to plot the value of the limit vs mass) (default is m=120) The option -s allows to specify the seed (eg -s 12345 ) used in toy generation. If this option is given, the name of the file will be extended by this seed, eg higgsCombineTest.AsymptoticLimits.mH120.12345.root The option --keyword-value allows you to specify the value of a keyword in the datacard such that $WORD (in the datacard) will be given the value of VALUE in the command --keyword-value WORD=VALUE , eg higgsCombineTest.AsymptoticLimits.mH120.WORDVALUE.12345.root The output file will contain a TDirectory named toys , which will be empty if no toys are generated (see below for details) and a TTree called limit with the following branches; Branch name Type Description limit Double_t Main result of combine run with method dependent meaning limitErr Double_t Estimated uncertainty on the result mh Double_t Value of MH , specified with -m option iToy Int_t Toy number identifier if running with -t iSeed Int_t Seed specified with -s t_cpu Float_t Estimated CPU time for algorithm t_real Float_t Estimated real time for algorithm quantileExpected Float_t Quantile identifier for methods which calculated expected (quantiles) and observed results (eg conversions from \\Delta\\ln L \\Delta\\ln L values) with method dependent meaning. Negative values are reserved for entries which do not related to quantiles of a calculation with the default being set to -1 (usually meaning the observed result). The value of any user defined keyword $WORD which is set using keyword-value described above will also be included as a branch with type string named WORD . The option can be repeated multiple times for multiple keywords. In some cases, the precise meanings of the branches will depend on the Method being used, which is included in this documentation.","title":"Output from combine"},{"location":"part3/runningthetool/#toy-data-generation","text":"By default, each of these methods will be run using the observed data as the input. In several cases (as detailed below), it might be useful to run the tool using toy datasets, including Asimov data. The option -t is used to specify to combine to first generate a toy dataset(s) which will be used in replacement of the real data. There are two versions of this, -t N with N > 0. Combine will generate N toy datasets from the model and re-run the method once per toy. The seed for the toy generation can be modified with the option -s (use -s -1 for a random seed). The output file will contain one entry in the tree for each of these toys. -t -1 will produce an Asimov dataset in which statistical fluctuations are suppressed. The procedure to generate this Asimov dataset depends on which type of analysis you are using, see below for details. The output file will contain the toys (as RooDataSets for the observables, including global observables) in the toys directory if the option --saveToys is provided. If you include this option, the limit TTree in the output will have an entry corresponding to the state of the POI used for the generation of the toy, with the value of quantileExpected set to -2 . You can add additional branches using the --trackParameters option as described in the common command line options section above. Warning The default values of the nuisance parameters (or any parameter) are used to generate the toy. This means that if, for example, you are using parametric shapes and the parameters inside the workspace are set to arbitrary values, those arbitrary values will be used to generate the toy. This behaviour can be modified through the use of the option --setParameters x=value_x,y=value_y... which will set the values of the parameters ( x and y ) before toy generation. You can also load a snap-shot from a previous fit to set the nuisances to their post-fit values (see below).","title":"Toy data generation"},{"location":"part3/runningthetool/#asimov-datasets","text":"If you are using wither -t -1 or using AsymptoticLimits , combine will calculate results based on an Asimov dataset. For counting experiments, the Asimov data will just be set to the total number of expected events (given the values of the nuisance parameters and POIs of the model) For shape analyses with templates, the Asimov dataset will be constructed as a histogram using the same binning which is defined for your analysis. If your model uses parametric shapes (for example when you are using binned data, there are some options as to what Asimov dataset to produce. By default , combine will produce the Asimov data as a histogram using the binning which is associated to each observable (ie as set using RooRealVar::setBins ). If this binning doesn't exist, combine will guess a suitable binning - it is therefore best to use RooRealVar::setBins to associate a binning to each observable, even if your data is unbinned, if you intend to use Asimov datasets. You can also ask combine to use a Pseudo-Asimov dataset, which is created from many weighted unbinned events. Setting --X-rtd TMCSO_AdaptivePseudoAsimov= \\beta \\beta with \\beta>0 \\beta>0 will trigger the internal logic of whether to produce a Pseudo-Asimov dataset. This logic is as follows; For each observable in your dataset, the number of bins, n_{b} n_{b} is determined either from the value of RooRealVar::getBins if it exists or assumed to be 100. If N_{b}=\\prod_{b}n_{b}>5000 N_{b}=\\prod_{b}n_{b}>5000 , the number of expected events N_{ev} N_{ev} is determined. Note if you are combining multiple channels, N_{ev} N_{ev} refers to the number of expected events in a single channel, the logic is separate for each channel. If N_{ev}/N_{b}<0.01 N_{ev}/N_{b}<0.01 then a Pseudo-Asimov dataset is created with the number of events equal to \\beta \\cdot \\mathrm{max}\\{100*N_{ev},1000\\} \\beta \\cdot \\mathrm{max}\\{100*N_{ev},1000\\} . If N_{ev}/N_{b}\\geq 0.01 N_{ev}/N_{b}\\geq 0.01 , then a normal Asimov dataset is produced. If N_{b}\\leq 5000 N_{b}\\leq 5000 then a normal Asimov dataset will be produced The production of a Pseudo-Asimov dataset can be forced by using the option --X-rtd TMCSO_PseudoAsimov=X where X>0 will determine the number of weighted events for the Pseudo-Asimov dataset. You should try different values of X since larger values leads to more events in the Pseudo-Asimov dataset resulting in higher precision but in general the fit will be slower. You can turn off the internal logic by setting --X-rtd TMCSO_AdaptivePseudoAsimov=0 --X-rtd TMCSO_PseudoAsimov=0 thereby forcing histograms to be generated. Info If you set --X-rtd TMCSO_PseudoAsimov=X with X>0 and also turn on --X-rtd TMCSO_AdaptivePseudoAsimov= \\beta \\beta , with \\beta>0 \\beta>0 , the internal logic will be used but this time the default will be to generate Pseudo-Asimov datasets, rather than the normal Asimov ones.","title":"Asimov datasets"},{"location":"part3/runningthetool/#nuisance-parameter-generation","text":"The default method of dealing with systematics is to generate random values (around their nominal values, see above) for the nuisance parameters, according to their prior pdfs centred around their default values, before generating the data. The unconstrained nuisance parameters (eg flatParam or rateParam ) or those with flat priors are not randomised before the data generation. The following are options which define how the toys will be generated, --toysNoSystematics the nuisance parameters in each toy are not randomised when generating the toy datasets - i.e their nominal values are used to generate the data. Note that for methods which profile (fit) the nuisances, the parameters are still floating when evaluating the likelihood. --toysFrequentist the nuisance parameters in each toy are set to their nominal values which are obtained after fitting first to the data , with POIs fixed, before generating the data. For evaluating likelihoods, the constraint terms are instead randomised within their Gaussian constraint pdfs around the post-fit nuisance parameter values. If you are using toysFrequentist , be aware that the values set by --setParameters will be ignored for the toy generation as the post-fit values will instead be used (except for any parameter which is also a parameter of interest). You can override this behaviour and choose the nominal values for toy generation for any parameter by adding the option --bypassFrequentistFit which will skip the initial fit to data or by loading a snapshot (see below). Warning The methods such as AsymptoticLimits and HybridNew --LHCmode LHC-limits , the \"nominal\" nuisance parameter values are taken from fits to the data and are therefore not \"blind\" to the observed data by default (following the fully frequentist paradigm). See the detailed documentation on these methods for avoiding this and running in a completely \"blind\" mode.","title":"Nuisance parameter generation"},{"location":"part3/runningthetool/#generate-only","text":"It is also possible to generate the toys first and then feed them to the Methods in combine. This can be done using -M GenerateOnly --saveToys . The toys can then be read and used with the other methods by specifying --toysFile=higgsCombineTest.GenerateOnly... and using the same options for the toy generation. Warning Some Methods also use toys within the method itself (eg AsymptoticLimits and HybridNew ). For these, you should not specify the toy generation with -t or the options above and instead follow the specific instructions.","title":"Generate only"},{"location":"part3/runningthetool/#loading-snapshots","text":"Snapshots from workspaces can be loaded and used in order to generate toys using the option --snapshotName <name of snapshot> . This will first set the parameters to the values in the snapshot before any other parameter options are set and toys are generated. See the section on saving post-fit workspaces for creating workspaces with post-fit snapshots from MultiDimFit . Here are a few examples of calculations with toys from post-fit workspaces using a workspace with r, m_{H} r, m_{H} as parameters of interest Throw post-fit toy with b from s+b(floating r,m_{H} r,m_{H} ) fit, s with r=1.0 , m=best fit MH , using nuisance values and constraints re-centered on s+b(floating r,m_{H} r,m_{H} ) fit values (aka frequentist post-fit expected) and compute post-fit expected r uncertainty profiling MH combine higgsCombinemumhfit.MultiDimFit.mH125.root --snapshotName MultiDimFit -M MultiDimFit --verbose 9 -n randomtest --toysFrequentist --bypassFrequentistFit -t -1 --expectSignal=1 -P r --floatOtherPOIs=1 --algo singles Throw post-fit toy with b from s+b(floating r,m_{H} r,m_{H} ) fit, s with r=1.0, m=128.0 , using nuisance values and constraints re-centered on s+b(floating r,m_{H} r,m_{H} ) fit values (aka frequentist post-fit expected) and compute post-fit expected significance (with MH fixed at 128 implicitly) combine higgsCombinemumhfit.MultiDimFit.mH125.root -m 128 --snapshotName MultiDimFit -M ProfileLikelihood --significance --verbose 9 -n randomtest --toysFrequentist --bypassFrequentistFit --overrideSnapshotMass -t -1 --expectSignal=1 --redefineSignalPOIs r --freezeParameters MH Throw post-fit toy with b from s+b(floating r,m_{H} r,m_{H} ) fit, s with r=0.0 , using nuisance values and constraints re-centered on s+b(floating r,m_{H} r,m_{H} ) fit values (aka frequentist post-fit expected) and compute post-fit expected and observed asymptotic limit (with MH fixed at 128 implicitly) combine higgsCombinemumhfit.MultiDimFit.mH125.root -m 128 --snapshotName MultiDimFit -M AsymptoticLimits --verbose 9 -n randomtest --bypassFrequentistFit --overrideSnapshotMass--redefineSignalPOIs r --freezeParameters MH","title":"Loading snapshots"},{"location":"part3/runningthetool/#combinetool-for-job-submission","text":"For longer tasks which cannot be run locally, several methods in combine can be split to run on the LSF batch or the Grid . The splitting and submission is handled using the combineTool (see this getting started section to get the tool)","title":"combineTool for job submission"},{"location":"part3/runningthetool/#submission-to-condor","text":"The syntax for running on condor with the tool is combineTool.py -M ALGO [options] --job-mode condor --sub-opts='CLASSADS' --task-name NAME [--dry-run] with options being the usual list of combine options. The help option -h will give a list of both combine and combineTool sets of options. This can be used with several different methods from combine . The --sub-opts option takes a string with the different ClassAds that you want to set, separated by \\n as argument (e.g. '+JobFlavour=\"espresso\"\\nRequestCpus=1' ). The --dry-run option will show what will be run without actually doing so / submitting the jobs. For example, to generate toys (eg for use with limit setting) users running on lxplus at CERN the condor mode can be used eg combineTool.py -d workspace.root -M HybridNew --LHCmode LHC-limits --clsAcc 0 -T 2000 -s -1 --singlePoint 0.2:2.0:0.05 --saveHybridResult -m 125 --job-mode condor --task-name condor-test --sub-opts='+JobFlavour=\"tomorrow\"' The --singlePoint option is over-ridden so that this will produce a script for each value of the POI in the range 0.2 to 2.0 in steps of 0.05. You can merge multiple points into a script using --merge - e.g adding --merge 10 to the above command will mean that each job contains at most 10 of the values. The scripts are labelled by the --task-name option. These will be submitted directly to condor adding any options in --sub-opts to the condor submit script. Make sure multiple options are separated by \\n . The jobs will run and produce output in the current directory . Below is an example for splitting points in a multi-dimensional likelihood scan.","title":"Submission to Condor"},{"location":"part3/runningthetool/#splitting-jobs-for-a-multi-dimensional-likelihood-scan","text":"The option --split-points issues the command to split the jobs for MultiDimFit when using --algo grid . The following example will split the jobs such that there are 10 points in each of the jobs, which will be submitted to the 8nh queue. combineTool.py datacard.txt -M MultiDimFit --algo grid --points 50 --rMin 0 --rMax 1 --job-mode condor --split-points 10 --sub-opts='+JobFlavour=\"workday\"' --task-name mytask -n mytask Remember, any usual options (such as redefining POIs or freezing parameters) are passed to combine and can be added to the command line for combineTool . Info The option -n NAME should be included to avoid overwriting output files as the jobs will be run inside the directory from which the command is issued.","title":"Splitting jobs for a multi-dimensional likelihood scan"},{"location":"part3/runningthetool/#running-combine-jobs-on-the-grid","text":"For more CPU-intensive tasks, for example determining limits for complex models using toys, it is generally not feasible to compute all the results interactively. Instead, these jobs can be submitted to the Grid. In this example we will use the HybridNew method of combine to determine an upper limit for a sub-channel of the Run 1 SM H\\rightarrow\\tau\\tau H\\rightarrow\\tau\\tau analysis. For full documentation, see the section on computing limits with toys . With this model it would take too long to find the limit in one go, so instead we create a set of jobs in which each one throws toys and builds up the test statistic distributions for a fixed value of the signal strength. These jobs can then be submitted to a batch system or to the Grid using crab3 . From the set of output distributions it is possible to extract the expected and observed limits. For this we will use combineTool.py First we need to build a workspace from the H\\rightarrow\\tau\\tau H\\rightarrow\\tau\\tau datacard , $ text2workspace.py data/tutorials/htt/125/htt_mt.txt -m 125 $ mv data/tutorials/htt/125/htt_mt.root ./ To get an idea of the range of signal strength values we will need to build test-statistic distributions for we will first use the AsymptoticLimits method of combine, $ combine -M Asymptotic htt_mt.root -m 125 << Combine >> [...] -- AsymptoticLimits (CLs) -- Observed Limit: r < 1.7384 Expected 2.5%: r < 0.4394 Expected 16.0%: r < 0.5971 Expected 50.0%: r < 0.8555 Expected 84.0%: r < 1.2340 Expected 97.5%: r < 1.7200 Based on this, a range of 0.2 to 2.0 should be suitable. We can use the same command for generating the distribution of test statistics with combineTool . The --singlePoint option is now enhanced to support expressions that generate a set of calls to combine with different values. The accepted syntax is of the form MIN:MAX:STEPSIZE , and multiple comma-separated expressions can be specified. The script also adds an option --dry-run which will not actually call combine but just prints out the commands that would be run, e.g, combineTool.py -M HybridNew -d htt_mt.root --LHCmode LHC-limits --singlePoint 0.2:2.0:0.2 -T 2000 -s -1 --saveToys --saveHybridResult -m 125 --dry-run ... [DRY-RUN]: combine -d htt_mt.root --LHCmode LHC-limits -T 2000 -s -1 --saveToys --saveHybridResult -M HybridNew -m 125 --singlePoint 0.2 -n .Test.POINT.0.2 [DRY-RUN]: combine -d htt_mt.root --LHCmode LHC-limits -T 2000 -s -1 --saveToys --saveHybridResult -M HybridNew -m 125 --singlePoint 0.4 -n .Test.POINT.0.4 [...] [DRY-RUN]: combine -d htt_mt.root --LHCmode LHC-limits -T 2000 -s -1 --saveToys --saveHybridResult -M HybridNew -m 125 --singlePoint 2.0 -n .Test.POINT.2.0 When the --dry-run option is removed each command will be run in sequence.","title":"Running combine jobs on the Grid"},{"location":"part3/runningthetool/#grid-submission","text":"Submission to the grid with crab3 works in a similar way. Before doing so ensure that the crab3 environment has been sourced, then for compatibility reasons source the CMSSW environment again. We will use the example of generating a grid of test-statistic distributions for limits. $ source /cvmfs/cms.cern.ch/crab3/crab.sh; cmsenv $ combineTool.py -d htt_mt.root -M HybridNew --LHCmode LHC-limits --clsAcc 0 -T 2000 -s -1 --singlePoint 0.2:2.0:0.05 --saveToys --saveHybridResult -m 125 --job-mode crab3 --task-name grid-test --custom-crab custom_crab.py The option --custom-crab should point to a python file python containing a function of the form custom_crab(config) that will be used to modify the default crab configuration. You can use this to set the output site to your local grid site, or modify other options such as the voRole, or the site blacklist/whitelist. For example def custom_crab(config): print '>> Customising the crab config' config.Site.storageSite = 'T2_CH_CERN' config.Site.blacklist = ['SOME_SITE', 'SOME_OTHER_SITE'] Again it is possible to use the option --dry-run to see what the complete crab config will look like before actually submitted it. Once submitted the progress can be monitored using the standard crab commands. When all jobs are completed copy the output from your sites storage element to the local output folder. $ crab getoutput -d crab_grid-test # Now we have to un-tar the output files $ cd crab_grid-test/results/ $ for f in *.tar; do tar xf $f; done $ mv higgsCombine*.root ../../ $ cd ../../ These output files should be combined with hadd , after which we invoke combine as usual to calculate observed and expected limits from the merged grid as usual.","title":"Grid submission"},{"location":"part3/validation/","text":"Validating datacards This section covers the main features of the datacard validation tool which helps you spot potential problems with your datacards at an early stage. The tool is implemented in the CombineHarvester/CombineTools subpackage. See the combineTool section of the documentation for checkout instructions. The datacard validation tool contains a number of checks. It is possible to call sub-sets of these checks when creating datacards within CombineHarvester. However, for now we will only describe the usage of the validation tool on already existing datacards. If you create your datacards with CombineHarvester and would like to include the checks at the datacard creation stage, please contact us via https://hypernews.cern.ch/HyperNews/CMS/get/higgs-combination.html . How to use the tool The basic syntax is: ValidateDatacards.py datacard.txt This will write the results of the checks to a json file (default: validation.json ), and will print a summary to the screen, for example: ================================ =======Validation results======= ================================ >>>There were 7800 warnings of type 'up/down templates vary the yield in the same direction' >>>There were 5323 warnings of type 'up/down templates are identical' >>>There were no warnings of type 'At least one of the up/down systematic uncertainty templates is empty' >>>There were 4406 warnings of type 'Uncertainty has normalisation effect of more than 10.0%' >>>There were 8371 warnings of type 'Uncertainty probably has no genuine shape effect' >>>There were no warnings of type 'Empty process' >>>There were no warnings of type 'Bins of the template empty in background' >>>INFO: there were 169 alerts of type 'Small signal process' The meaning of each of these warnings/alerts is discussed below . The following arguments are possible: usage: ValidateDatacards.py [-h] [--printLevel PRINTLEVEL] [--readOnly] [--checkUncertOver CHECKUNCERTOVER] [--reportSigUnder REPORTSIGUNDER] [--jsonFile JSONFILE] [--mass MASS] cards positional arguments: cards Specifies the full path to the datacards to check optional arguments: -h, --help show this help message and exit --printLevel PRINTLEVEL, -p PRINTLEVEL Specify the level of info printing (0-3, default:1) --readOnly If this is enabled, skip validation and only read the output json --checkUncertOver CHECKUNCERTOVER, -c CHECKUNCERTOVER Report uncertainties which have a normalisation effect larger than this fraction (default:0.1) --reportSigUnder REPORTSIGUNDER, -s REPORTSIGUNDER Report signals contributing less than this fraction of the total in a channel (default:0.001) --jsonFile JSONFILE Path to the json file to read/write results from (default:validation.json) --mass MASS Signal mass to use (default:*) printLevel adjusts how much information is printed to the screen. When set to 0, the results are only written to the json file, but not to the screen. When set to 1 (default), the number of warnings/alerts of a given type is printed to the screen. Setting this option to 2 prints the same information as level 1, and additionally which uncertainties are affected (if the check is related to uncertainties) or which processes are affected (if the check is related only to processes). When printLevel is set to 3, the information from level 2 is printed, and additionaly for checks related to uncertainties prints which processes are affected. To print information to screen, the script parses the json file which contains the results of the validation checks, so if you have already run the validation tool and produced this json file, you can simply change the printLevel by re-running the tool with printLevel set to a different value, and enabling the --readOnly option. The options --checkUncertOver and --reportSigUnder will be described in more detail in the section that discusses the checks for which they are relevant. Note: the --mass argument should only be set if you normally use it when running Combine, otherwise you can leave it at the default. The datacard validation tool is primarily intended for shape (histogram)-based analyses. However, when running on a parametric model or counting experiment the checks for small signal processes, empty processes and uncertainties with large normalisation effects will still be performed. Details on checks Uncertainties with large normalisation effect This check highlights nuisance parameters which have a normalisation effect larger than the fraction set by the setting --checkUncertOver . The default value is 0.1, meaning that any uncertainties with a normalisation effect larger than 10% are flagged up. The output file contains the following information for this check: largeNormEff: { <Uncertainty name>: { <analysis category>: { <process>: { \"value_d\":<value> \"value_u\":<value> } } } } Where value_u and value_d are the values of the 'up' and 'down' normalisation effects. At least one of the Up/Down systematic templates is empty For shape uncertainties, this check reports all cases where the up and/or down template(s) are empty, when the nominal template is not. The output file contains the following information for this check: emptySystematicShape: { <Uncertainty name>: { <analysis category>: { <process>: { \"value_d\":<value> \"value_u\":<value> } } } } Where value_u and value_d are the values of the 'up' and 'down' normalisation effects. Identical Up/Down templates This check applies to shape uncertainties only, and will highlight cases where the shape uncertainties have identical Up and Down templates (identical in shape and in normalisation). The information given in the output file for this check is: uncertTemplSame: { <Uncertainty name>: { <analysis category>: { <process>: { \"value_d\":<value> \"value_u\":<value> } } } } Where value_u and value_d are the values of the 'up' and 'down' normalisation effects. Up and Down templates vary the yield in the same direction Again this check only applies to shape uncertainties - it highlights cases where the 'Up' template and the 'Down' template both have the effect of increasing or decreasing the normalisation of a process. The information given in the output file for this check is: uncertVarySameDirect: { <Uncertainty name>: { <analysis category>: { <process>: { \"value_d\":<value> \"value_u\":<value> } } } } Where value_u and value_d are the values of the 'up' and 'down' normalisation effects. Uncertainty probably has no genuine shape effect In this check, applying only to shape uncertainties, the normalised nominal templates are compared with the normalised templates for the 'up' and 'down' systematic variations. The script calculates $$ \\Sigma_i \\frac{2|\\text{up}(i) - \\text{nominal}(i)|}{|\\text{up}(i)| + |\\text{nominal}(i)|}$$ and $$ \\Sigma_i \\frac{2|\\text{down}(i) - \\text{nominal}(i)|}{|\\text{down}(i)| + |\\text{nominal}(i)|} $$ where the sums run over all bins in the histograms, and 'nominal', 'up', and 'down' are the central template and up and down varied templates, all normalised. If both sums are smaller than 0.001, the uncertainty is flagged up as probably not having a genuine shape effect. This means a 0.1% variation in one bin is enough to avoid being reported, but many smaller variations can also sum to be large enough to pass the threshold. It should be noted that the chosen threshold is somewhat arbitrary: if an uncertainty is flagged up as probably having no genuine shape effect you should take this as a starting point to investigate. The information given in the output file for this check is: smallShapeEff: { <Uncertainty name>: { <analysis category>: { <process>: { \"diff_d\":<value> \"diff_u\":<value> } } } } Where diff_d and diff_u are the values of the sums described above for the 'down' variation and the 'up' variation. Empty process If a process is listed in the datacard, but the yield is 0, it is flagged up by this check. The information given in the output file for this check is: emptyProcessShape: { <analysis category>: { <process1>, <process2>, <process3> } } Bins which have signal but no background For shape-based analyses, this checks whether there are any bins in the nominal templates which have signal contributions, but no background contributions. The information given in the output file for this check is: emptyBkgBin: { <analysis category>: { <bin_nr1>, <bin_nr2>, <bin_nr3> } } Small signal process This reports signal processes which contribute less than the fraction specified by --reportSigUnder (default 0.001 = 0.1%) of the total signal in a given category. This produces an alert, not a warning, as it does not hint at a potential problem. However, in analyses with many signal contributions and with long fitting times, it can be helpful to remove signals from a category in which they do not contribute a significant amount. The information given in the output file for this check is: smallSignalProc: { <analysis category>: { <process>: { \"sigrate_tot\":<value> \"procrate\":<value> } } } Where sigrate_tot is the total signal yield in the analysis category and procrate is the yield of signal process <process> . What to do in case of a warning These checks are mostly a tool to help you investigate your datacards: a warning does not necessarily mean there is a mistake in your datacard, but you should use it as a starting point to investigate. Empty processes and emtpy shape uncertainties connected to nonempty processes will most likely be unintended. The same holds for cases where the 'up' and 'down' shape templates are identical. If there are bins which contain signal but no background contributions, this should be corrected. See the FAQ for more information on that point. For other checks it depends on where the check is fired whether there is a problem or not. Some examples: An analysis-specific noncloser uncertainty could be larger than 10%. A theoretical uncertainty in the ttbar normalisation probably not. In an analysis with a selection that requires the presence of exactly 1 jet, 'up' and 'down' variations in the jet energy uncertainty could both change the process normalisation in the same direction. (But they don't have to!) As always: think about whether you expect a check to yield a warning in case of your analysis, and investigate to make sure.","title":"Validating datacards"},{"location":"part3/validation/#validating-datacards","text":"This section covers the main features of the datacard validation tool which helps you spot potential problems with your datacards at an early stage. The tool is implemented in the CombineHarvester/CombineTools subpackage. See the combineTool section of the documentation for checkout instructions. The datacard validation tool contains a number of checks. It is possible to call sub-sets of these checks when creating datacards within CombineHarvester. However, for now we will only describe the usage of the validation tool on already existing datacards. If you create your datacards with CombineHarvester and would like to include the checks at the datacard creation stage, please contact us via https://hypernews.cern.ch/HyperNews/CMS/get/higgs-combination.html .","title":"Validating datacards"},{"location":"part3/validation/#how-to-use-the-tool","text":"The basic syntax is: ValidateDatacards.py datacard.txt This will write the results of the checks to a json file (default: validation.json ), and will print a summary to the screen, for example: ================================ =======Validation results======= ================================ >>>There were 7800 warnings of type 'up/down templates vary the yield in the same direction' >>>There were 5323 warnings of type 'up/down templates are identical' >>>There were no warnings of type 'At least one of the up/down systematic uncertainty templates is empty' >>>There were 4406 warnings of type 'Uncertainty has normalisation effect of more than 10.0%' >>>There were 8371 warnings of type 'Uncertainty probably has no genuine shape effect' >>>There were no warnings of type 'Empty process' >>>There were no warnings of type 'Bins of the template empty in background' >>>INFO: there were 169 alerts of type 'Small signal process' The meaning of each of these warnings/alerts is discussed below . The following arguments are possible: usage: ValidateDatacards.py [-h] [--printLevel PRINTLEVEL] [--readOnly] [--checkUncertOver CHECKUNCERTOVER] [--reportSigUnder REPORTSIGUNDER] [--jsonFile JSONFILE] [--mass MASS] cards positional arguments: cards Specifies the full path to the datacards to check optional arguments: -h, --help show this help message and exit --printLevel PRINTLEVEL, -p PRINTLEVEL Specify the level of info printing (0-3, default:1) --readOnly If this is enabled, skip validation and only read the output json --checkUncertOver CHECKUNCERTOVER, -c CHECKUNCERTOVER Report uncertainties which have a normalisation effect larger than this fraction (default:0.1) --reportSigUnder REPORTSIGUNDER, -s REPORTSIGUNDER Report signals contributing less than this fraction of the total in a channel (default:0.001) --jsonFile JSONFILE Path to the json file to read/write results from (default:validation.json) --mass MASS Signal mass to use (default:*) printLevel adjusts how much information is printed to the screen. When set to 0, the results are only written to the json file, but not to the screen. When set to 1 (default), the number of warnings/alerts of a given type is printed to the screen. Setting this option to 2 prints the same information as level 1, and additionally which uncertainties are affected (if the check is related to uncertainties) or which processes are affected (if the check is related only to processes). When printLevel is set to 3, the information from level 2 is printed, and additionaly for checks related to uncertainties prints which processes are affected. To print information to screen, the script parses the json file which contains the results of the validation checks, so if you have already run the validation tool and produced this json file, you can simply change the printLevel by re-running the tool with printLevel set to a different value, and enabling the --readOnly option. The options --checkUncertOver and --reportSigUnder will be described in more detail in the section that discusses the checks for which they are relevant. Note: the --mass argument should only be set if you normally use it when running Combine, otherwise you can leave it at the default. The datacard validation tool is primarily intended for shape (histogram)-based analyses. However, when running on a parametric model or counting experiment the checks for small signal processes, empty processes and uncertainties with large normalisation effects will still be performed.","title":"How to use the tool"},{"location":"part3/validation/#details-on-checks","text":"","title":"Details on checks"},{"location":"part3/validation/#uncertainties-with-large-normalisation-effect","text":"This check highlights nuisance parameters which have a normalisation effect larger than the fraction set by the setting --checkUncertOver . The default value is 0.1, meaning that any uncertainties with a normalisation effect larger than 10% are flagged up. The output file contains the following information for this check: largeNormEff: { <Uncertainty name>: { <analysis category>: { <process>: { \"value_d\":<value> \"value_u\":<value> } } } } Where value_u and value_d are the values of the 'up' and 'down' normalisation effects.","title":"Uncertainties with large normalisation effect"},{"location":"part3/validation/#at-least-one-of-the-updown-systematic-templates-is-empty","text":"For shape uncertainties, this check reports all cases where the up and/or down template(s) are empty, when the nominal template is not. The output file contains the following information for this check: emptySystematicShape: { <Uncertainty name>: { <analysis category>: { <process>: { \"value_d\":<value> \"value_u\":<value> } } } } Where value_u and value_d are the values of the 'up' and 'down' normalisation effects.","title":"At least one of the Up/Down systematic templates is empty"},{"location":"part3/validation/#identical-updown-templates","text":"This check applies to shape uncertainties only, and will highlight cases where the shape uncertainties have identical Up and Down templates (identical in shape and in normalisation). The information given in the output file for this check is: uncertTemplSame: { <Uncertainty name>: { <analysis category>: { <process>: { \"value_d\":<value> \"value_u\":<value> } } } } Where value_u and value_d are the values of the 'up' and 'down' normalisation effects.","title":"Identical Up/Down templates"},{"location":"part3/validation/#up-and-down-templates-vary-the-yield-in-the-same-direction","text":"Again this check only applies to shape uncertainties - it highlights cases where the 'Up' template and the 'Down' template both have the effect of increasing or decreasing the normalisation of a process. The information given in the output file for this check is: uncertVarySameDirect: { <Uncertainty name>: { <analysis category>: { <process>: { \"value_d\":<value> \"value_u\":<value> } } } } Where value_u and value_d are the values of the 'up' and 'down' normalisation effects.","title":"Up and Down templates vary the yield in the same direction"},{"location":"part3/validation/#uncertainty-probably-has-no-genuine-shape-effect","text":"In this check, applying only to shape uncertainties, the normalised nominal templates are compared with the normalised templates for the 'up' and 'down' systematic variations. The script calculates $$ \\Sigma_i \\frac{2|\\text{up}(i) - \\text{nominal}(i)|}{|\\text{up}(i)| + |\\text{nominal}(i)|}$$ and $$ \\Sigma_i \\frac{2|\\text{down}(i) - \\text{nominal}(i)|}{|\\text{down}(i)| + |\\text{nominal}(i)|} $$ where the sums run over all bins in the histograms, and 'nominal', 'up', and 'down' are the central template and up and down varied templates, all normalised. If both sums are smaller than 0.001, the uncertainty is flagged up as probably not having a genuine shape effect. This means a 0.1% variation in one bin is enough to avoid being reported, but many smaller variations can also sum to be large enough to pass the threshold. It should be noted that the chosen threshold is somewhat arbitrary: if an uncertainty is flagged up as probably having no genuine shape effect you should take this as a starting point to investigate. The information given in the output file for this check is: smallShapeEff: { <Uncertainty name>: { <analysis category>: { <process>: { \"diff_d\":<value> \"diff_u\":<value> } } } } Where diff_d and diff_u are the values of the sums described above for the 'down' variation and the 'up' variation.","title":"Uncertainty probably has no genuine shape effect"},{"location":"part3/validation/#empty-process","text":"If a process is listed in the datacard, but the yield is 0, it is flagged up by this check. The information given in the output file for this check is: emptyProcessShape: { <analysis category>: { <process1>, <process2>, <process3> } }","title":"Empty process"},{"location":"part3/validation/#bins-which-have-signal-but-no-background","text":"For shape-based analyses, this checks whether there are any bins in the nominal templates which have signal contributions, but no background contributions. The information given in the output file for this check is: emptyBkgBin: { <analysis category>: { <bin_nr1>, <bin_nr2>, <bin_nr3> } }","title":"Bins which have signal but no background"},{"location":"part3/validation/#small-signal-process","text":"This reports signal processes which contribute less than the fraction specified by --reportSigUnder (default 0.001 = 0.1%) of the total signal in a given category. This produces an alert, not a warning, as it does not hint at a potential problem. However, in analyses with many signal contributions and with long fitting times, it can be helpful to remove signals from a category in which they do not contribute a significant amount. The information given in the output file for this check is: smallSignalProc: { <analysis category>: { <process>: { \"sigrate_tot\":<value> \"procrate\":<value> } } } Where sigrate_tot is the total signal yield in the analysis category and procrate is the yield of signal process <process> .","title":"Small signal process"},{"location":"part3/validation/#what-to-do-in-case-of-a-warning","text":"These checks are mostly a tool to help you investigate your datacards: a warning does not necessarily mean there is a mistake in your datacard, but you should use it as a starting point to investigate. Empty processes and emtpy shape uncertainties connected to nonempty processes will most likely be unintended. The same holds for cases where the 'up' and 'down' shape templates are identical. If there are bins which contain signal but no background contributions, this should be corrected. See the FAQ for more information on that point. For other checks it depends on where the check is fired whether there is a problem or not. Some examples: An analysis-specific noncloser uncertainty could be larger than 10%. A theoretical uncertainty in the ttbar normalisation probably not. In an analysis with a selection that requires the presence of exactly 1 jet, 'up' and 'down' variations in the jet energy uncertainty could both change the process normalisation in the same direction. (But they don't have to!) As always: think about whether you expect a check to yield a warning in case of your analysis, and investigate to make sure.","title":"What to do in case of a warning"},{"location":"part4/usefullinks/","text":"Useful links and further reading Tutorials and reading material There are several tutorials which have been run over the last few years with instructions and examples for running the combine tool. Tutorial Sessions 1st tutorial 17th Nov 2015 . 2nd tutorial 30th Nov 2016 . 3rd tutorial 29th Nov 2017 4th tutorial 31st Oct 2018 - Latest for 81x-root606 branch. 5th tutorial 2nd-4th Dec 2019 6th tutorial 14th-16th Dec 2020 - Latest for 102x branch Worked examples from Higgs analyses using combine The CMS DAS at CERN 2014 The CMS DAS at DESY 2018 Conventions to be used when preparing inputs for Higgs combinations CMS AN-2011/298 Procedure for the LHC Higgs boson search combination in summer 2011. This describes in more detail some of the methods used in Combine. Citations There is no document currently which can be cited for using the combine tool, however you can use the following publications for the procedures we use, Summer 2011 public ATLAS-CMS note for any Frequentist limit setting procedures with toys or Bayesian limits, constructing likelihoods, descriptions of nuisance parameter options (like log-normals ( lnN ) or gamma ( gmN ), and for definitions of test-statistics. CCGV paper if you use any of the asymptotic (eg with -M AsymptoticLimits or -M Significance approximations for limits/p-values. If you use the Barlow-Beeston approach to MC stat (bin-by-bin) uncertainties, please cite their paper Barlow-Beeston . You should also cite this note if you use the autoMCStats directive to produce a single parameter per bin. If you use shape uncertainties for template ( TH1 or RooDataHist ) based datacards, you can cite this note from J. Conway. If you are extracting uncertainties from LH scans - i.e using -2\\Delta Log{L}=1 -2\\Delta Log{L}=1 etc for the 1 \\sigma \\sigma intervals, you can cite either the ATLAS+CMS or CMS Higgs paper. There is also a long list of citation recommendations from the CMS Statistics Committee pages. Combine based packages SWGuideHiggs2TauLimits ATGCRooStats CombineHarvester Contacts Hypernews forum : hn-cms-higgs-combination https://hypernews.cern.ch/HyperNews/CMS/get/higgs-combination.html CMS Statistics Committee You can find much more statistics theory and reccomendations on various statistical procedures in the CMS Statistics Committee Twiki Pages FAQ Why does combine have trouble with bins that have zero expected contents? If you're computing only upper limits, and your zero-prediction bins are all empty in data, then you can just set the background to a very small value instead of zero as anyway the computation is regular for background going to zero (e.g. a counting experiment with B\\leq1 B\\leq1 will have essentially the same expected limit and observed limit as one with B=0 B=0 ). If you're computing anything else, e.g. p-values, or if your zero-prediction bins are not empty in data, you're out of luck, and you should find a way to get a reasonable background prediction there (and set an uncertainty on it, as per the point above) How can an uncertainty be added to a zero quantity? You can put an uncertainty even on a zero event yield if you use a gamma distribution. That's in fact the more proper way of doing it if the prediction of zero comes from the limited size of your MC or data sample used to compute it. Why does changing the observation in data affect my expected limit? The expected limit (if using either the default behaviour of -M AsymptoticLimits or using the LHC-limits style limit setting with toys) uses the post-fit expectation of the background model to generate toys. This means that first the model is fit to the observed data before toy generation. See the sections on blind limits and toy generation to avoid this behavior. How can I deal with an interference term which involves a negative contribution? You will need to set up a specific PhysicsModel to deal with this, however you can see this section to implement such a model which can incorperate a negative contribution to the physics process How does combine work? That is not a question which can be answered without someone's head exploding so please try to formulate something specific. What does fit status XYZ mean? Combine reports the fit status in some routines (for example in the FitDiagnostics method). These are typically the status of the last call from Minuit. For details on the meanings of these status codes see the Minuit2Minimizer documentation page. Why does my fit not converge? There are several reasons why some fits may not converge. Often some indication can be obtained from the RooFitResult or status which you will see information from when using the --verbose X (with X>2 X>2 ) option. Sometimes however, it can be that the likelihood for your data is very unusual. You can get a rough idea about what the likelihood looks like as a function of your parameters (POIs and nuisances) using combineTool.py -M FastScan -w myworkspace.root (use --help for options). We have seen often that fits in combine using RooCBShape as a parametric function will fail. This is related to an optimisation that fails. You can try to fix the problem as described in this issue: https://github.com/cms-analysis/HiggsAnalysis-CombinedLimit/issues/347 (i.e add the option --X-rtd ADDNLL_CBNLL=0 ). Why does the fit/fits take so long? The minimisation routines are common to many methods in combine. You can tune the fitting using the generic optimisation command line options described here . For example, setting the default minimizer strategy to 0 can greatly improve the speed since this avoids running Hesse. In calculations such as AsymptoticLimits , Hesse is not needed and hence this can be done, however, for FitDiagnostics the uncertainties and correlations are part of the output so using strategy 0 may not be particularly accurate. Why are the results for my counting experiment so slow or unstable? There is a known issue with counting experiments with large numbers of events which will cause unstable fits or even the fit to fail. You can avoid this by creating a \"fake\" shape datacard (see this section from the setting up the datacards page). The simplest way to do this is to run combineCards.py -S mycountingcard.txt > myshapecard.txt . You may still find that your parameter uncertainties are not correct when you have large numbers of events. This can be often fixed using the --robustHesse option. An example of this issue is detailed here . Why do some of my nuisance parameters have uncertainties > 1? When running -M FitDiagnostics you may find that the post-fit uncertainties of the nuisances are > 1 > 1 (or larger than their pre-fit values). If this is the case, you should first check if the same is true when adding the option --minos all which will invoke minos to scan the likelihood as a function of these parameters to determine the crossing at -2\\times\\Delta\\log\\mathcal{L}=1 -2\\times\\Delta\\log\\mathcal{L}=1 rather than relying on the estimate from Hesse. However, this is not guaranteed to succeed, in which case you can scan the likelihood yourself using MultiDimFit (see here ) and specifying the option --poi X where X is your nuisance parameter. How can I avoid using the data? For almost all methods, you can use toy data (or an Asimov dataset) in place of the real data for your results to be blind. You should be careful however as in some methods, such as -M AsymptoticLimits or -M HybridNew --LHCmode LHC-limits or any other method using the option --toysFrequentist , the data will be used to determine the most likely nuisance parameter values (to determine the so-called a-posteriori expectation). See the section on toy data generation for details on this. What if my nuisance parameters have correlations which are not 0 or 1? Combine is designed under the assumption that each source of nuisance parameter is uncorrelated with the other sources. If you have a case where some pair (or set) of nuisances have some known correlation structure, you can compute the eigenvectors of their correlation matrix and provide these diagonalised nuisances to combine. You can also model partial correlations , between different channels or data taking periods, of a given nuisance parameter using the combineTool as described in this page . My nuisances are (artificially) constrained and/or the impact plot show some strange behaviour, especially after including MC statistical uncertainties. What can I do? Depending on the details of the analysis, several solutions can be adopted to mitigate these effects. We advise to run the validation tools at first, to identify possible redundant shape uncertainties that can be safely eliminated or replaced with lnN ones. Any remaining artificial constrain should be studies. Possible mitigating strategies can be to (a) smooth the templates or (b) adopt some rebinning in order to reduce statistical fluctuations in the templates. A description of possible strategies and effects can be found in this talk by Margaret Eminizer What do CLs, CLs+b and CLb in the code mean? The names CLs+b and CLb are rather outdated and should instead be referred to as p-values - p_{\\mu} p_{\\mu} and 1-p_{b} 1-p_{b} , respectively. We use the CLs (which itself is not a p-value) criterion often in High energy physics as it is designed to avoid excluding a signal model when the sensitivity is low (and protects against excluding due to underfluctuations in the data). Typically, when excluding a signal model the p-value p_{\\mu} p_{\\mu} often refers to the p-value under the signal+background hypothesis, assuming a particular value of the signal stregth ( \\mu \\mu ) while p_{b} p_{b} is the p-value under the background only hypothesis. You can find more details and definitions of the CLs criterion and p_{\\mu} p_{\\mu} and p_{b} p_{b} in section 39.4.2.4 the 2016 PDG review .","title":"Links & FAQ"},{"location":"part4/usefullinks/#useful-links-and-further-reading","text":"","title":"Useful links and further reading"},{"location":"part4/usefullinks/#tutorials-and-reading-material","text":"There are several tutorials which have been run over the last few years with instructions and examples for running the combine tool. Tutorial Sessions 1st tutorial 17th Nov 2015 . 2nd tutorial 30th Nov 2016 . 3rd tutorial 29th Nov 2017 4th tutorial 31st Oct 2018 - Latest for 81x-root606 branch. 5th tutorial 2nd-4th Dec 2019 6th tutorial 14th-16th Dec 2020 - Latest for 102x branch Worked examples from Higgs analyses using combine The CMS DAS at CERN 2014 The CMS DAS at DESY 2018 Conventions to be used when preparing inputs for Higgs combinations CMS AN-2011/298 Procedure for the LHC Higgs boson search combination in summer 2011. This describes in more detail some of the methods used in Combine.","title":"Tutorials and reading material"},{"location":"part4/usefullinks/#citations","text":"There is no document currently which can be cited for using the combine tool, however you can use the following publications for the procedures we use, Summer 2011 public ATLAS-CMS note for any Frequentist limit setting procedures with toys or Bayesian limits, constructing likelihoods, descriptions of nuisance parameter options (like log-normals ( lnN ) or gamma ( gmN ), and for definitions of test-statistics. CCGV paper if you use any of the asymptotic (eg with -M AsymptoticLimits or -M Significance approximations for limits/p-values. If you use the Barlow-Beeston approach to MC stat (bin-by-bin) uncertainties, please cite their paper Barlow-Beeston . You should also cite this note if you use the autoMCStats directive to produce a single parameter per bin. If you use shape uncertainties for template ( TH1 or RooDataHist ) based datacards, you can cite this note from J. Conway. If you are extracting uncertainties from LH scans - i.e using -2\\Delta Log{L}=1 -2\\Delta Log{L}=1 etc for the 1 \\sigma \\sigma intervals, you can cite either the ATLAS+CMS or CMS Higgs paper. There is also a long list of citation recommendations from the CMS Statistics Committee pages.","title":"Citations"},{"location":"part4/usefullinks/#combine-based-packages","text":"SWGuideHiggs2TauLimits ATGCRooStats CombineHarvester","title":"Combine based packages"},{"location":"part4/usefullinks/#contacts","text":"Hypernews forum : hn-cms-higgs-combination https://hypernews.cern.ch/HyperNews/CMS/get/higgs-combination.html","title":"Contacts"},{"location":"part4/usefullinks/#cms-statistics-committee","text":"You can find much more statistics theory and reccomendations on various statistical procedures in the CMS Statistics Committee Twiki Pages","title":"CMS Statistics Committee"},{"location":"part4/usefullinks/#faq","text":"Why does combine have trouble with bins that have zero expected contents? If you're computing only upper limits, and your zero-prediction bins are all empty in data, then you can just set the background to a very small value instead of zero as anyway the computation is regular for background going to zero (e.g. a counting experiment with B\\leq1 B\\leq1 will have essentially the same expected limit and observed limit as one with B=0 B=0 ). If you're computing anything else, e.g. p-values, or if your zero-prediction bins are not empty in data, you're out of luck, and you should find a way to get a reasonable background prediction there (and set an uncertainty on it, as per the point above) How can an uncertainty be added to a zero quantity? You can put an uncertainty even on a zero event yield if you use a gamma distribution. That's in fact the more proper way of doing it if the prediction of zero comes from the limited size of your MC or data sample used to compute it. Why does changing the observation in data affect my expected limit? The expected limit (if using either the default behaviour of -M AsymptoticLimits or using the LHC-limits style limit setting with toys) uses the post-fit expectation of the background model to generate toys. This means that first the model is fit to the observed data before toy generation. See the sections on blind limits and toy generation to avoid this behavior. How can I deal with an interference term which involves a negative contribution? You will need to set up a specific PhysicsModel to deal with this, however you can see this section to implement such a model which can incorperate a negative contribution to the physics process How does combine work? That is not a question which can be answered without someone's head exploding so please try to formulate something specific. What does fit status XYZ mean? Combine reports the fit status in some routines (for example in the FitDiagnostics method). These are typically the status of the last call from Minuit. For details on the meanings of these status codes see the Minuit2Minimizer documentation page. Why does my fit not converge? There are several reasons why some fits may not converge. Often some indication can be obtained from the RooFitResult or status which you will see information from when using the --verbose X (with X>2 X>2 ) option. Sometimes however, it can be that the likelihood for your data is very unusual. You can get a rough idea about what the likelihood looks like as a function of your parameters (POIs and nuisances) using combineTool.py -M FastScan -w myworkspace.root (use --help for options). We have seen often that fits in combine using RooCBShape as a parametric function will fail. This is related to an optimisation that fails. You can try to fix the problem as described in this issue: https://github.com/cms-analysis/HiggsAnalysis-CombinedLimit/issues/347 (i.e add the option --X-rtd ADDNLL_CBNLL=0 ). Why does the fit/fits take so long? The minimisation routines are common to many methods in combine. You can tune the fitting using the generic optimisation command line options described here . For example, setting the default minimizer strategy to 0 can greatly improve the speed since this avoids running Hesse. In calculations such as AsymptoticLimits , Hesse is not needed and hence this can be done, however, for FitDiagnostics the uncertainties and correlations are part of the output so using strategy 0 may not be particularly accurate. Why are the results for my counting experiment so slow or unstable? There is a known issue with counting experiments with large numbers of events which will cause unstable fits or even the fit to fail. You can avoid this by creating a \"fake\" shape datacard (see this section from the setting up the datacards page). The simplest way to do this is to run combineCards.py -S mycountingcard.txt > myshapecard.txt . You may still find that your parameter uncertainties are not correct when you have large numbers of events. This can be often fixed using the --robustHesse option. An example of this issue is detailed here . Why do some of my nuisance parameters have uncertainties > 1? When running -M FitDiagnostics you may find that the post-fit uncertainties of the nuisances are > 1 > 1 (or larger than their pre-fit values). If this is the case, you should first check if the same is true when adding the option --minos all which will invoke minos to scan the likelihood as a function of these parameters to determine the crossing at -2\\times\\Delta\\log\\mathcal{L}=1 -2\\times\\Delta\\log\\mathcal{L}=1 rather than relying on the estimate from Hesse. However, this is not guaranteed to succeed, in which case you can scan the likelihood yourself using MultiDimFit (see here ) and specifying the option --poi X where X is your nuisance parameter. How can I avoid using the data? For almost all methods, you can use toy data (or an Asimov dataset) in place of the real data for your results to be blind. You should be careful however as in some methods, such as -M AsymptoticLimits or -M HybridNew --LHCmode LHC-limits or any other method using the option --toysFrequentist , the data will be used to determine the most likely nuisance parameter values (to determine the so-called a-posteriori expectation). See the section on toy data generation for details on this. What if my nuisance parameters have correlations which are not 0 or 1? Combine is designed under the assumption that each source of nuisance parameter is uncorrelated with the other sources. If you have a case where some pair (or set) of nuisances have some known correlation structure, you can compute the eigenvectors of their correlation matrix and provide these diagonalised nuisances to combine. You can also model partial correlations , between different channels or data taking periods, of a given nuisance parameter using the combineTool as described in this page . My nuisances are (artificially) constrained and/or the impact plot show some strange behaviour, especially after including MC statistical uncertainties. What can I do? Depending on the details of the analysis, several solutions can be adopted to mitigate these effects. We advise to run the validation tools at first, to identify possible redundant shape uncertainties that can be safely eliminated or replaced with lnN ones. Any remaining artificial constrain should be studies. Possible mitigating strategies can be to (a) smooth the templates or (b) adopt some rebinning in order to reduce statistical fluctuations in the templates. A description of possible strategies and effects can be found in this talk by Margaret Eminizer What do CLs, CLs+b and CLb in the code mean? The names CLs+b and CLb are rather outdated and should instead be referred to as p-values - p_{\\mu} p_{\\mu} and 1-p_{b} 1-p_{b} , respectively. We use the CLs (which itself is not a p-value) criterion often in High energy physics as it is designed to avoid excluding a signal model when the sensitivity is low (and protects against excluding due to underfluctuations in the data). Typically, when excluding a signal model the p-value p_{\\mu} p_{\\mu} often refers to the p-value under the signal+background hypothesis, assuming a particular value of the signal stregth ( \\mu \\mu ) while p_{b} p_{b} is the p-value under the background only hypothesis. You can find more details and definitions of the CLs criterion and p_{\\mu} p_{\\mu} and p_{b} p_{b} in section 39.4.2.4 the 2016 PDG review .","title":"FAQ"},{"location":"part5/longexercise/","text":"Long exercise: main features of Combine This exercise is designed to give a broad overview of the tools available for statistical analysis in CMS using the combine tool. Combine is a high-level tool for building RooFit/RooStats models and running common statistical methods. We will cover the typical aspects of setting up an analysis and producing the results, as well as look at ways in which we can diagnose issues and get a deeper understanding of the statistical model. This is a long exercise - expect to spend some time on it especially if you are new to Combine. If you get stuck while working through this exercise or have questions specifically about the exercise, you can ask them on this mattermost channel . Finally, we also provide some solutions to some of the questions that are asked as part of the exercise. These are available here . For the majority of this course we will work with a simplified version of a real analysis, that nonetheless will have many features of the full analysis. The analysis is a search for an additional heavy neutral Higgs boson decaying to tau lepton pairs. Such a signature is predicted in many extensions of the standard model, in particular the minimal supersymmetric standard model (MSSM). You can read about the analysis in the paper here . The statistical inference makes use of a variable called the total transverse mass ( M_{\\mathrm{T}}^{\\mathrm{tot}} M_{\\mathrm{T}}^{\\mathrm{tot}} ) that provides good discrimination between the resonant high-mass signal and the main backgrounds, which have a falling distribution in this high-mass region. The events selected in the analysis are split into a several categories which target the main di-tau final states as well as the two main production modes: gluon-fusion (ggH) and b-jet associated production (bbH). One example is given below for the fully-hadronic final state in the b-tag category which targets the bbH signal: Initially we will start with the simplest analysis possible: a one-bin counting experiment using just the high M_{\\mathrm{T}}^{\\mathrm{tot}} M_{\\mathrm{T}}^{\\mathrm{tot}} region of this distribution, and from there each section of this exercise will expand on this, introducing a shape-based analysis and adding control regions to constrain the backgrounds. Getting started We need to set up a new CMSSW area and checkout the combine package: export SCRAM_ARCH=slc7_amd64_gcc700 cmsrel CMSSW_10_2_13 cd CMSSW_10_2_13/src cmsenv git clone https://github.com/cms-analysis/HiggsAnalysis-CombinedLimit.git HiggsAnalysis/CombinedLimit cd HiggsAnalysis/CombinedLimit cd $CMSSW_BASE/src/HiggsAnalysis/CombinedLimit git fetch origin git checkout v8.1.0 cd $CMSSW_BASE/src We will also make use another package, CombineHarvester , which contains some high-level tools for working with combine. The following command will download the repository and checkout just the parts of it we need for this tutorial: bash <(curl -s https://raw.githubusercontent.com/cms-analysis/CombineHarvester/master/CombineTools/scripts/sparse-checkout-https.sh) Now make sure the CMSSW area is compiled: scramv1 b clean; scramv1 b Finally we will checkout the working directory for these tutorials - this contains all the inputs needed to run the exercises below: cd $CMSSW_BASE/src git clone https://gitlab.cern.ch/agilbert/cms-das-stats.git cd cms-das-stats Part 1: A one-bin counting experiment Topics covered in this section: A: Computing limits using the asymptotic approximation Advanced section: B: Computing limits with toys We will begin with a simplified version of a datacard from the MSSM \\phi\\rightarrow\\tau\\tau \\phi\\rightarrow\\tau\\tau analysis that has been converted to a one-bin counting experiment, as described above. While the full analysis considers a range of signal mass hypotheses, we will start by considering just one: m_{\\phi} m_{\\phi} =800GeV. Click the text below to study the datacard ( datacard_part1.txt in the cms-das-stats directory): Show datacard imax 1 number of bins jmax 4 number of processes minus 1 kmax * number of nuisance parameters -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- bin signal_region observation 10.0 -------------------------------------------------------------------------------- bin signal_region signal_region signal_region signal_region signal_region process ttbar diboson Ztautau jetFakes bbHtautau process 1 2 3 4 0 rate 4.43803 3.18309 3.7804 1.63396 0.711064 -------------------------------------------------------------------------------- CMS_eff_b lnN 1.02 1.02 1.02 - 1.02 CMS_eff_t lnN 1.12 1.12 1.12 - 1.12 CMS_eff_t_highpt lnN 1.1 1.1 1.1 - 1.1 acceptance_Ztautau lnN - - 1.08 - - acceptance_bbH lnN - - - - 1.05 acceptance_ttbar lnN 1.005 - - - - lumi_13TeV lnN 1.025 1.025 1.025 - 1.025 norm_jetFakes lnN - - - 1.2 - xsec_Ztautau lnN - - 1.04 - - xsec_diboson lnN - 1.05 - - - xsec_ttbar lnN 1.06 - - - - The layout of the datacard is as follows: At the top are the numbers imax , jmax and kmax representing the number of bins, processes and nuisance parameters respectively. Here a \"bin\" can refer to a literal single event count as in this example, or a full distribution we are fitting, in general with many histogram bins, as we will see later. We will refer to both as \"channels\" from now on. It is possible to replace these numbers with * and they will be deduced automatically. The first line starting with bin gives a unique label to each channel, and the following line starting with observation gives the number of events observed in data. In the remaining part of the card there are several columns: each one represents one process in one channel. The first four lines labelled bin , process , process and rate give the channel label, the process label, a process identifier ( <=0 for signal, >0 for background) and the number of expected events respectively. The remaining lines describe sources of systematic uncertainty. Each line gives the name of the uncertainty, (which will become the name of the nuisance parameter inside our RooFit model), the type of uncertainty (\"lnN\" = log-normal normalisation uncertainty) and the effect on each process in each channel. E.g. a 20% uncertainty on the yield is written as 1.20. It is also possible to add a hash symbol ( # ) at the start of a line, which combine will then ignore when it reads the card. We can now run combine directly using this datacard as input. The general format for running combine is: combine -M [method] [datacard] [additional options...] A: Computing limits using the asymptotic approximation As we are searching for a signal process that does not exist in the standard mode, it's natural to set an upper limit on the cross section times branching fraction of the process (assuming our dataset does not contain a significant discovery of new physics). Combine has dedicated method for calculating upper limits. The most commonly used one is AsymptoticLimits , which implements the CLs criterion and uses the profile likelihood ratio as the test statistic. As the name implies, the test statistic distributions are determined analytically in the asymptotic approximation, so there is no need for more time-intensive toy throwing and fitting. Try running the following command: combine -M AsymptoticLimits datacard_part1.txt -n .part1A You should see the results of the observed and expected limit calculations printed to the screen. Here we have added an extra option, -n .part1A , which is short for --name , and is used to label the output file combine produces, which in this case will be called higgsCombine.part1A.AsymptoticLimits.mH120.root . The file name depends on the options we ran with, and is of the form: higgsCombine[name].[method].mH[mass].root . The file contains a TTree called limit which stores the numerical values returned by the limit computation. Note that in our case we did not set a signal mass when running combine (i.e. -m 800 ), so the output file just uses the default value of 120 . This does not affect our result in any way though, just the label that is used on the output file. The limits are given on a parameter called r . This is the default parameter of interest (POI) that is added to the model automatically. It is a linear scaling of the normalisation of all signal processes given in the datacard, i.e. if s_{i,j} s_{i,j} is the nominal number of signal events in channel i i for signal process j j , then the normalisation of that signal in the model is given as r\\cdot s_{i,j}(\\vec{\\theta}) r\\cdot s_{i,j}(\\vec{\\theta}) , where \\vec{\\theta} \\vec{\\theta} represents the set of nuisance parameters which may also affect the signal normalisation. We therefore have some choice in the interpretation of r: for the measurement of a process with a well defined SM prediction we may enter this as the nominal yield in the datacard, such that r=1 r=1 corresponds to this SM expectation, whereas for setting limits on BSM processes we may choose the nominal yield to correspond to some cross section, e.g. 1 pb, such that we can interpret the limit as a cross section limit directly. In this example the signal has been normalised to a cross section times branching fraction of 1 fb. The expected limit is given under the background-only hypothesis. The median value under this hypothesis as well as the quantiles needed to give the 68% and 95% intervals are also calculated. These are all the ingredients needed to produce the standard limit plots you will see in many CMS results, for example the \\sigma \\times \\mathcal{B} \\sigma \\times \\mathcal{B} limits for the \\text{bb}\\phi\\rightarrow\\tau\\tau \\text{bb}\\phi\\rightarrow\\tau\\tau process: In this case we only computed the values for one signal mass hypothesis, indicated by a red dashed line. Tasks and questions: There are some important uncertainties missing from the datacard above. Add the uncertainty on the luminosity (name: lumi_13TeV ) which has a 2.5% effect on all processes (except the jetFakes , which are taken from data), and uncertainties on the inclusive cross sections of the Ztautau and ttbar processes (with names xsec_Ztautau and xsec_diboson ) which are 4% and 6% respectively. Try changing the values of some uncertainties (up or down, or removing them altogether) - how do the expected and observed limits change? Now try changing the number of observed events. The observed limit will naturally change, but the expected does too - why might this be? There are other command line options we can supply to combine which will change its behaviour when run. You can see the full set of supported options by doing combine -h . Many options are specific to a given method, but others are more general and are applicable to all methods. Throughout this tutorial we will highlight some of the most useful options you may need to use, for example: The range on the signal strength modifier: --rMin=X and --rMax=Y : In RooFit parameters can optionally have a range specified. The implication of this is that their values cannot be adjusted beyond the limits of this range. The min and max values can be adjusted though, and we might need to do this for our POI r if the order of magnitude of our measurement is different from the default range of [0, 20] . This will be discussed again later in the tutorial. Verbosity: -v X : By default combine does not usually produce much output on the screen other the main result at the end. However, much more detailed information can be printed by setting the -v N with N larger than zero. For example at -v 3 the logs from the minimizer, Minuit, will also be printed. These are very useful for debugging problems with the fit. Advanced section: B: Computing limits with toys Now we will look at computing limits without the asymptotic approximation, so instead using toy datasets to determine the test statistic distributions under the signal+background and background-only hypotheses. This can be necessary if we are searching for signal in bins with a small number of events expected. In combine we will use the HybridNew method to calculate limits using toys. This mode is capable of calculating limits with several different test statistics and with fine-grained control over how the toy datasets are generated internally. To calculate LHC-style profile likelihood limits (i.e. the same as we did with the asymptotic) we set the option --LHCmode LHC-limits . You can read more about the different options in the Combine documentation . Run the following command: combine -M HybridNew datacard_part1.txt --LHCmode LHC-limits -n .part1B --saveHybridResult --fork 0 In contrast to AsymptoticLimits this will only determine the observed limit, and will take around five minutes. There will not be much output to the screen while combine is running. You can add the option -v 1 to get a better idea of what is going on. You should see combine stepping around in r , trying to find the value for which CLs = 0.05, i.e. the 95% CL limit. The --saveHybridResult option will cause the test statistic distributions that are generated at each tested value of r to be saved in the output ROOT file. To get an expected limit add the option --expectedFromGrid X , where X is the desired quantile, e.g. for the median: combine -M HybridNew datacard_part1.txt --LHCmode LHC-limits -n .part1B --saveHybridResult --fork 0 --expectedFromGrid 0.500 Calculate the median expected limit and the 68% range. The 95% range could also be done, but note it will take much longer to run the 0.025 quantile. While combine is running you can move on to the next steps below. Tasks and questions: - In contrast to AsymptoticLimits , with HybridNew each limit comes with an uncertainty. What is the origin of this uncertainty? - How good is the agreement between the asymptotic and toy-based methods? - Why does it take longer to calculate the lower expected quantiles (e.g. 0.025, 0.16)? Think about how the statistical uncertainty on the CLs value depends on CLs+b and CLb. Next plot the test statistic distributions stored in the output file: python $CMSSW_BASE/src/HiggsAnalysis/CombinedLimit/test/plotTestStatCLs.py --input higgsCombine.part1B.HybridNew.mH120.root --poi r --val all --mass 120 This produces a new ROOT file cls_qmu_distributions.root containing the plots, to save them as pdf/png files run this small script and look at the resulting figures: python printTestStatPlots.py cls_qmu_distributions.root Advanced exercises These distributions can be useful in understanding features in the CLs limits, especially in the low statistics regime. To explore this, try reducing the observed and expected yields in the datacard by a factor of 10, and rerun the above steps to compare the observed and expected limits with the asymptotic approach, and plot the test statistic distributions. Tasks and questions: Is the asymptotic limit still a good approximation? You might notice that the test statistic distributions are not smooth but rather have several \"bump\" structures? Where might this come from? Try reducing the size of the systematic uncertainties to make them more pronounced. Note that for more complex models the fitting time can increase significantly, making it infeasible to run all the toy-based limits interactively like this. An alternative strategy is documented here Part 2: A shape-based analysis Topics covered in this section: A: Setting up the datacard B: Running combine for a blind analysis C: Using FitDiagnostics D: MC statistical uncertainties A: Setting up the datacard Now we move to the next step: instead of a one-bin counting experiment we will fit a binned distribution. In a typical analysis we will produce TH1 histograms of some variable sensitive to the presence of signal: one for the data and one for each signal and background processes. Then we add a few extra lines to the datacard to link the declared processes to these shapes which are saved in a ROOT file, for example: Show datacard imax 1 jmax 1 kmax * --------------- shapes * * simple-shapes-TH1_input.root $PROCESS $PROCESS_$SYSTEMATIC shapes signal * simple-shapes-TH1_input.root $PROCESS$MASS $PROCESS$MASS_$SYSTEMATIC --------------- bin bin1 observation 85 ------------------------------ bin bin1 bin1 process signal background process 0 1 rate 10 100 -------------------------------- lumi lnN 1.10 1.0 bgnorm lnN 1.00 1.3 alpha shape - 1 Note that as with the one-bin card, the total nominal rate of a given process must be specified in the rate line of the datacard. This should agree with the value returned by TH1::Integral . However, we can also put a value of -1 and the Integral value will be substituted automatically. There are two other differences with respect to the one-bin card: A new block of lines at the top defining how channels and processes are mapped to the histograms (more than one line can be used) In the list of systematic uncertainties some are marked as shape instead of lnN The syntax of the \"shapes\" line is: shapes [process] [channel] [file] [histogram] [histogram_with_systematics] . It is possible to use the * wildcard to map multiple processes and/or channels with one line. The histogram entries can contain the $PROCESS , $CHANNEL and $MASS place-holders which will be substituted when searching for a given (process, channel) combination. The value of $MASS is specified by the -m argument when combine. By default the observed data process name will be data_obs . Shape uncertainties can be added by supplying two additional histograms for a process, corresponding to the distribution obtained by shifting that parameter up and down by one standard deviation. These shapes will be interpolated quadratically for shifts below 1\\sigma 1\\sigma and linearly beyond. The normalizations are interpolated linearly in log scale just like we do for log-normal uncertainties. The final argument of the \"shapes\" line above should contain the $SYSTEMATIC place-holder which will be substituted by the systematic name given in the datacard. In the list of uncertainties the interpretation of the values for shape lines is a bit different from lnN . The effect can be \"-\" or 0 for no effect, 1 for normal effect, and possibly something different from 1 to test larger or smaller effects (in that case, the unit Gaussian is scaled by that factor before using it as parameter for the interpolation). In this section we will use a datacard corresponding to the full distribution that was shown at the start of section 1, not just the high mass region. Have a look at datacard_part2.txt : this is still currently a one-bin counting experiment, however the yields are much higher since we now consider the full range of M_{\\mathrm{T}}^{\\mathrm{tot}} M_{\\mathrm{T}}^{\\mathrm{tot}} . If you run the asymptotic limit calculation on this you should find the sensitivity is significantly worse than before. The first task is to convert this to a shape analysis: the file datacard_part2.shapes.root contains all the necessary histograms, including those for the relevant shape systematic uncertainties. Add the relevant shapes lines to the top of the datacard (after the kmax line) to map the processes to the correct TH1s in this file. Hint: you will need a different line for the signal process. Compared to the counting experiment we must also consider the effect of uncertainties that change the shape of the distribution. Some, like CMS_eff_t_highpt , were present before, as it has both a shape and normalisation effect. Others are primarily shape effects so were not included before. Add the following shape uncertainties: top_pt_ttbar_shape affecting ttbar ,the tau energy scale uncertainties CMS_scale_t_1prong0pi0_13TeV , CMS_scale_t_1prong1pi0_13TeV and CMS_scale_t_3prong0pi0_13TeV affecting all processes except jetFakes , and CMS_eff_t_highpt also affecting the same processes. Once this is done you can run the asymptotic limit calculation on this datacard. From now on we will convert the text datacard into a RooFit workspace ourselves instead of combine doing it internally every time we run. This is a good idea for more complex analyses since the conversion step can take a notable amount of time. For this we use the text2workspace.py command: text2workspace.py datacard_part2.txt -m 800 -o workspace_part2.root And then we can use this as input to combine instead of the text datacard: combine -M AsymptoticLimits workspace_part2.root -m 800 Tasks and questions: Verify that the sensitivity of the shape analysis is indeed improved over the counting analysis in the first part. Advanced task : You can open the workspace ROOT file interactively and print the contents: w->Print(); . Each process is represented by a PDF object that depends on the shape morphing nuisance parameters. From the workspace, choose a process and shape uncertainty, and make a plot overlaying the nominal shape with different values of the shape morphing nuisance parameter. You can change the value of a parameter with w->var(\"X\")->setVal(Y) , and access a particular pdf with w->pdf(\"Z\") . PDF objects in RooFit have a createHistogram method that requires the name of the observable (the variable defining the x-axis) - this is called CMS_th1x in combine datacards. Feel free to ask for help with this! B: Running combine for a blind analysis Most analyses are developed and optimised while we are \"blind\" to the region of data where we expect our signal to be. With AsymptoticLimits we can choose just to run the expected limit ( --run expected ), so as not to calculate the observed. However the data is still used, even for the expected, since in the frequentist approach a background-only fit to the data is performed to define the Asimov dataset used to calculate the expected limits. To skip this fit to data and use the pre-fit state of the model the option --run blind or --noFitAsimov can be used. Task: Compare the expected limits calculated with --run expected and --run blind . Why are they different? A more general way of blinding is to use combine's toy and Asimov dataset generating functionality. You can read more about this here . These options can be used with any method in combine, not just AsymptoticLimits . Task: Calculate a blind limit by generating a background-only Asimov with the -t -1 option instead of using the AsymptoticLimits specific options. You should find the observed limit is the same as the expected. Then see what happens if you inject a signal into the Asimov dataset using the --expectSignal [X] option. C: Using FitDiagnostics We will now explore one of the most commonly used modes of combine: FitDiagnostics . As well as allowing us to make a measurement of some physical quantity (as opposed to just setting a limit on it), this method is useful to gain additional information about the model and the behaviour of the fit. It performs two fits: A \"background-only\" (b-only) fit: first POI (usually \"r\") fixed to zero A \"signal+background\" (s+b) fit: all POIs are floating With the s+b fit combine will report the best-fit value of our signal strength modifier r . As well as the usual output file, a file named fitdiagnostics.root is produced which contains additional information. In particular it includes two RooFitResult objects, one for the b-only and one for the s+b fit, which store the fitted values of all the nuisance parameters (NPs) and POIs as well as estimates of their uncertainties. The covariance matrix from both fits is also included, from which we can learn about the correlations between parameters. Run the FitDiagnostics method on our workspace: combine -M FitDiagnostics workspace_part2.root -m 800 --rMin -20 --rMax 20 Open the resulting fitDiagnostics.root interactively and print the contents of the s+b RooFitResult: root [1] fit_s->Print() Show output RooFitResult: minimized FCN value: -4.7666, estimated distance to minimum: 3.31389e-05 covariance matrix quality: Full, accurate covariance matrix Status : MINIMIZE=0 HESSE=0 Floating Parameter FinalValue +/- Error -------------------- -------------------------- CMS_eff_b -4.3559e-02 +/- 9.87e-01 CMS_eff_t -2.6382e-01 +/- 7.27e-01 CMS_eff_t_highpt -4.7214e-01 +/- 9.56e-01 CMS_scale_t_1prong0pi0_13TeV -1.5884e-01 +/- 5.89e-01 CMS_scale_t_1prong1pi0_13TeV -1.6512e-01 +/- 4.91e-01 CMS_scale_t_3prong0pi0_13TeV -3.0668e-01 +/- 6.03e-01 acceptance_Ztautau -3.1059e-01 +/- 8.57e-01 acceptance_bbH -5.8325e-04 +/- 9.94e-01 acceptance_ttbar 4.7839e-03 +/- 9.94e-01 lumi_13TeV -5.4684e-02 +/- 9.83e-01 norm_jetFakes -9.3975e-02 +/- 2.54e-01 r -2.7327e+00 +/- 2.57e+00 top_pt_ttbar_shape 1.7614e-01 +/- 6.97e-01 xsec_Ztautau -1.5991e-01 +/- 9.61e-01 xsec_diboson 3.8745e-02 +/- 9.94e-01 xsec_ttbar 5.8025e-02 +/- 9.41e-01 There are several useful pieces of information here. At the top the status codes from the fits that were performed is given. In this case we can see that two algorithms were run: MINIMIZE and HESSE , both of which returned a successful status code (0). Both of these are routines in the Minuit2 minimization package - the default minimizer used in RooFit. The first performs the main fit to the data, and the second calculates the covariance matrix at the best-fit point. It is important to always check this second step was successful and the message \"Full, accurate covariance matrix\" is printed, otherwise the parameter uncertainties can be very inaccurate, even if the fit itself was successful. Underneath this the best-fit values ( \\theta \\theta ) and symmetrised uncertainties for all the floating parameters are given. For all the constrained nuisance parameters a convention is used by which the nominal value ( \\theta_I \\theta_I ) is zero, corresponding to the mean of a Gaussian constraint PDF with width 1.0, such that the parameter values \\pm 1.0 \\pm 1.0 correspond to the \\pm 1\\sigma \\pm 1\\sigma input uncertainties. A more useful way of looking at this is to compare the pre- and post-fit values of the parameters, to see how much the fit to data has shifted and constrained these parameters with respect to the input uncertainty. The script diffNuisances.py can be used for this: python diffNuisances.py fitDiagnostics.root --all Show output name b-only fit s+b fit rho CMS_eff_b -0.04, 0.99 -0.04, 0.99 +0.01 CMS_eff_t * -0.24, 0.73* * -0.26, 0.73* +0.06 CMS_eff_t_highpt * -0.56, 0.93* * -0.47, 0.96* +0.03 CMS_scale_t_1prong0pi0_13TeV * -0.17, 0.58* * -0.16, 0.59* -0.04 CMS_scale_t_1prong1pi0_13TeV ! -0.12, 0.45! ! -0.17, 0.49! +0.21 CMS_scale_t_3prong0pi0_13TeV * -0.31, 0.60* * -0.31, 0.60* +0.02 acceptance_Ztautau * -0.31, 0.86* * -0.31, 0.86* -0.05 acceptance_bbH +0.00, 0.99 -0.00, 0.99 +0.05 acceptance_ttbar +0.01, 0.99 +0.00, 0.99 +0.00 lumi_13TeV -0.05, 0.98 -0.05, 0.98 +0.01 norm_jetFakes ! -0.09, 0.25! ! -0.09, 0.25! -0.05 top_pt_ttbar_shape * +0.24, 0.69* * +0.18, 0.70* +0.23 xsec_Ztautau -0.16, 0.96 -0.16, 0.96 -0.02 xsec_diboson +0.03, 0.99 +0.04, 0.99 -0.02 xsec_ttbar +0.08, 0.94 +0.06, 0.94 +0.02 The numbers in each column are respectively \\frac{\\theta-\\theta_I}{\\sigma_I} \\frac{\\theta-\\theta_I}{\\sigma_I} (often called the pull , though note that more than one definition is in use for this), where \\sigma_I \\sigma_I is the input uncertainty; and the ratio of the post-fit to the pre-fit uncertainty \\frac{\\sigma}{\\sigma_I} \\frac{\\sigma}{\\sigma_I} . Tasks and questions: Which parameter has the largest pull? Which has the tightest constraint? Should we be concerned when a parameter is more strongly constrained than the input uncertainty (i.e. \\frac{\\sigma}{\\sigma_I}<1.0 \\frac{\\sigma}{\\sigma_I}<1.0 )? Check the pulls and constraints on a b-only and s+b asimov dataset instead. This check is required for all analyses in the Higgs PAG. It serves both as a closure test (do we fit exactly what signal strength we input?) and a way to check whether there are any infeasibly strong constraints while the analysis is still blind (typical example: something has probably gone wrong if we constrain the luminosity uncertainty to 10% of the input!) Advanced task: Sometimes there are problems in the fit model that aren't apparent from only fitting the Asimov dataset, but will appear when fitting randomised data. Follow the exercise on toy-by-toy diagnostics here to explore the tools available for this. D: MC statistical uncertainties So far there is an important source of uncertainty we have neglected. Our estimates of the backgrounds come either from MC simulation or from sideband regions in data, and in both cases these estimates are subject to a statistical uncertainty on the number of simulated or data events. In principle we should include an independent statistical uncertainty for every bin of every process in our model. It's important to note that combine/RooFit does not take this into account automatically - statistical fluctuations of the data are implicitly accounted for in the likelihood formalism, but statistical uncertainties in the model must be specified by us. One way to implement these uncertainties is to create a shape uncertainty for each bin of each process, in which the up and down histograms have the contents of the bin shifted up and down by the 1\\sigma 1\\sigma uncertainty. However this makes the likelihood evaluation computationally inefficient, and can lead to a large number of nuisance parameters in more complex models. Instead we will use a feature in combine called autoMCStats that creates these automatically from the datacard, and uses a technique called \"Barlow-Beeston-lite\" to reduce the number of systematic uncertainties that are created. This works on the assumption that for high MC event counts we can model the uncertainty with a Gaussian distribution. Given the uncertainties in different bins are independent, the total uncertainty of several processes in a particular bin is just the sum of N N individual Gaussians, which is itself a Gaussian distribution. So instead of N N nuisance parameters we need only one. This breaks down when the number of events is small and we are not in the Gaussian regime. The autoMCStats tool has a threshold setting on the number of events below which the the Barlow-Beeston-lite approach is not used, and instead a Poisson PDF is used to model per-process uncertainties in that bin. After reading the full documentation on autoMCStats here , add the corresponding line to your datacard. Start by setting a threshold of 0, i.e. [channel] autoMCStats 0 , to force the use of Barlow-Beeston-lite in all bins. Tasks and questions: Check how much the cross section measurement and uncertainties change using FitDiagnostics . It is also useful to check how the expected uncertainty changes using an Asimov dataset, say with r=10 injected. Advanced task: See what happens if the Poisson threshold is increased. Based on your results, what threshold would you recommend for this analysis? Part 3: Adding control regions Topics covered in this section: A: Use of rateParams B: Nuisance parameter impacts C: Post-fit distributions D: Calculating the significance E: Signal strength measurement and uncertainty breakdown F: Use of channel masking In a modern analysis it is typical for some or all of the backgrounds to be estimated using the data, instead of relying purely on MC simulation. This can take many forms, but a common approach is to use \"control regions\" (CRs) that are pure and/or have higher statistics for a given process. These are defined by event selections that are similar to, but non-overlapping with, the signal region. In our \\phi\\rightarrow\\tau\\tau \\phi\\rightarrow\\tau\\tau example the \\text{Z}\\rightarrow\\tau\\tau \\text{Z}\\rightarrow\\tau\\tau background normalisation can be calibrated using a \\text{Z}\\rightarrow\\mu\\mu \\text{Z}\\rightarrow\\mu\\mu CR, and the \\text{t}\\bar{\\text{t}} \\text{t}\\bar{\\text{t}} background using an e+\\mu e+\\mu CR. By comparing the number of data events in these CRs to our MC expectation we can obtain scale factors to apply to the corresponding backgrounds in the signal region (SR). The idea is that the data will gives us a more accurate prediction of the background with less systematic uncertainties. For example, we can remove the cross section and acceptance uncertainties in the SR, since we are no longer using the MC prediction (with a caveat discussed below). While we could simply derive these correction factors and apply them to our signal region datacard and better way is to include these regions in our fit model and tie the normalisations of the backgrounds in the CR and SR together. This has a number of advantages: Automatically handles the statistical uncertainty due to the number of data events in the CR Allows for the presence of some signal contamination in the CR to be handled correctly The CRs are typically not 100% pure in the background they're meant to control - other backgrounds may be present, with their own systematic uncertainties, some of which may be correlated with the SR or other CRs. Propagating these effects through to the SR \"by hand\" can become very challenging. In this section we will continue to use the same SR as in the previous one, however we will switch to a lower signal mass hypothesis, m_{\\phi}=200 m_{\\phi}=200 GeV, as its sensitivity depends more strongly on the background prediction than the high mass signal, so is better for illustrating the use of CRs. Here the nominal signal ( r=1 ) has been normalised to a cross section of 1 pb. The SR datacard for the 200 GeV signal is datacard_part3.txt . Two further datacards are provided: datacard_part3_ttbar_cr.txt and datacard_part3_DY_cr.txt which represent the CRs for the Drell-Yan and \\text{t}\\bar{\\text{t}} \\text{t}\\bar{\\text{t}} processes as described above. The cross section and acceptance uncertainties for these processes have pre-emptively been removed from the SR card. However we cannot get away with neglecting acceptance effects altogether. We are still implicitly using the MC simulation to predict to the ratio of events in the CR and SR, and this ratio will in general carry a theoretical acceptance uncertainty. If the CRs are well chosen then this uncertainty should be smaller than the direct acceptance uncertainty in the SR however. The uncertainties acceptance_ttbar_cr and acceptance_DY_cr have been added to these datacards cover this effect. Task: Calculate the ratio of CR to SR events for these two processes, as well as their CR purity to verify that these are useful CRs. The next step is to combine these datacards into one, which is done with the combineCards.py script: combineCards.py signal_region=datacard_part3.txt ttbar_cr=datacard_part3_ttbar_cr.txt DY_cr=datacard_part3_DY_cr.txt &> part3_combined.txt Each argument is of the form [new channel name]=[datacard.txt] . The new datacard is written to the screen by default, so we redirect the output into our new datacard file. The output looks like: Show datacard imax 3 number of bins jmax 8 number of processes minus 1 kmax 15 number of nuisance parameters ---------------------------------------------------------------------------------------------------------------------------------- shapes * DY_cr datacard_part3_DY_cr.shapes.root DY_control_region/$PROCESS DY_control_region/$PROCESS_$SYSTEMATIC shapes * signal_region datacard_part3.shapes.root signal_region/$PROCESS signal_region/$PROCESS_$SYSTEMATIC shapes bbHtautau signal_region datacard_part3.shapes.root signal_region/bbHtautau$MASS signal_region/bbHtautau$MASS_$SYSTEMATIC shapes * ttbar_cr datacard_part3_ttbar_cr.shapes.root tt_control_region/$PROCESS tt_control_region/$PROCESS_$SYSTEMATIC ---------------------------------------------------------------------------------------------------------------------------------- bin signal_region ttbar_cr DY_cr observation 3416 79251 365754 ---------------------------------------------------------------------------------------------------------------------------------- bin signal_region signal_region signal_region signal_region signal_region ttbar_cr ttbar_cr ttbar_cr ttbar_cr ttbar_cr DY_cr DY_cr DY_cr DY_cr DY_cr DY_cr process bbHtautau ttbar diboson Ztautau jetFakes Ztautau ttbar VV W QCD W Ztautau VV QCD ttbar Zmumu process 0 1 2 3 4 3 1 5 6 7 6 3 5 7 1 8 rate 198.521 683.017 96.5185 742.649 2048.94 150.025 67280.4 10589.6 597.336 308.965 59.9999 115.34 5273.43 141.725 34341.1 305423 ---------------------------------------------------------------------------------------------------------------------------------- CMS_eff_b lnN 1.02 1.02 1.02 1.02 - - - - - - - - - - - - CMS_eff_e lnN - - - - - 1.02 1.02 1.02 1.02 - - - - - - - The [new channel name]= part of the input arguments is not required, but it gives us control over how the channels in the combined card will be named, otherwise default values like ch1 , ch2 etc will be used. A: Use of rateParams We now have a combined datacard that we can run text2workspace.py on and start doing fits, however there is still one important ingredient missing. Right now the yields of the Ztautau process in the SR and Zmumu in the CR are not connected to each other in any way, and similarly for the ttbar processes. In the fit both would be adjusted by the nuisance parameters only, and constrained to the nominal yields. To remedy this we introduce rateParam directives to the datacard. A rateParam is a new free parameter that multiples the yield of a given process, just in the same way the signal strength r multiplies the signal yield. The syntax of a rateParam line in the datacard is [name] rateParam [channel] [process] [init] [min,max] where name is the chosen name for the parameter, channel and process specify which (channel, process) combination it should affect, init gives the initial value, and optionally [min,max] specifies the ranges on the RooRealVar that will be created. The channel and process arguments support the use of the wildcard * to match multiple entries. Task: Add two rateParam s with nominal values of 1.0 to the end of the combined datacard named rate_ttbar and rate_Zll . The former should affect the ttbar process in all channels, and the latter should affect the Ztautau and Zmumu processes in all channels. Set ranges of [0,5] to both. Note that a rateParam name can be repeated to apply it to multiple processes, e.g.: rateScale rateParam * procA 1.0 rateScale rateParam * procB 1.0 is perfectly valid and only one rateParam will be created. These parameters will allow the yields to float in the fit without prior constraint (unlike a regular lnN or shape systematic), with the yields in the CRs and SR tied together. Tasks and questions: Run text2workspace.py on this combined card and then use FitDiagnostics on an Asimov dataset with r=1 to get the expected uncertainty. Suggested command line options: --rMin 0 --rMax 2 Using the RooFitResult in the fitdiagnostics.root file, check the post-fit value of the rateParams. To what level are the normalisations of the DY and ttbar processes constrained? To compare to the previous approach of fitting the SR only, with cross section and acceptance uncertainties restored, an additional card is provided: datacard_part3_nocrs.txt . Run the same fit on this card to verify the improvement of the SR+CR approach B: Nuisance parameter impacts It is often useful to examine in detail the effects the systematic uncertainties have on the signal strength measurement. This is often referred to as calculating the \"impact\" of each uncertainty. What this means is to determine the shift in the signal strength, with respect to the best-fit, that is induced if a given nuisance parameter is shifted by its \\pm1\\sigma \\pm1\\sigma post-fit uncertainty values. If the signal strength shifts a lot, it tells us that it has a strong dependency on this systematic uncertainty. In fact, what we are measuring here is strongly related to the correlation coefficient between the signal strength and the nuisance parameter. The MultiDimFit method has an algorithm for calculating the impact for a given systematic: --algo impact -P [parameter name] , but it is typical to use a higher-level script, combineTool.py (part of the CombineHarvester package you checked out at the beginning) to automatically run the impacts for all parameters. Full documentation on this is given here . There is a three step process for running this. First we perform an initial fit for the signal strength and its uncertainty: combineTool.py -M Impacts -d workspace_part3.root -m 200 --rMin -1 --rMax 2 --robustFit 1 --doInitialFit Then we run the impacts for all the nuisance parameters: combineTool.py -M Impacts -d workspace_part3.root -m 200 --rMin -1 --rMax 2 --robustFit 1 --doFits This will take a little bit of time. When finished we collect all the output and convert it to a json file: combineTool.py -M Impacts -d workspace_part3.root -m 200 --rMin -1 --rMax 2 --robustFit 1 --output impacts.json We can then make a plot showing the pulls and parameter impacts, sorted by the largest impact: plotImpacts.py -i impacts.json -o impacts Tasks and questions: Identify the most important uncertainties using the impacts tool. In the plot, some parameters do not show a pull, but rather just a numerical value - why? C: Post-fit distributions Another thing the FitDiagnostics mode can help us with is visualising the distributions we are fitting, and the uncertainties on those distributions, both before the fit is performed (\"pre-fit\") and after (\"post-fit\"). The pre-fit can give us some idea of how well our uncertainties cover any data-MC discrepancy, and the post-fit if discrepancies remain after the fit to data (as well as possibly letting us see the presence of a significant signal!). To produce these distributions add the --saveShapes and --saveWithUncertainties options when running FitDiagnostics : combine -M FitDiagnostics workspace_part3.root -m 200 --rMin -1 --rMax 2 --saveShapes --saveWithUncertainties Combine will produce pre- and post-fit distributions (for fit_s and fit_b) in the fitdiagnostics.root output file: Tasks and questions: Make a plot showing the expected background and signal contributions using the output from FitDiagnostics - do this for both the pre-fit and post-fit. You will find a script postFitPlot.py in the cms-das-stats repository that can help you get started. The bin errors on the TH1s in the fitdiagnostics file are determined from the systematic uncertainties. In the post-fit these take into account the additional constraints on the nuisance parameters as well as any correlations. Why is the uncertainty on the post-fit so much smaller than on the pre-fit? D: Calculating the significance In the event that you observe a deviation from your null hypothesis, in this case the b-only hypothesis, combine can be used to calculate the p-value or significance. To do this using the asymptotic approximation simply do: combine -M Significance workspace_part3.root -m 200 --rMin -1 --rMax 2 To calculate the expected significance for a given signal strength we can just generate an Asimov dataset first: combine -M Significance workspace_part3.root -m 200 --rMin -1 --rMax 5 -t -1 --expectSignal 1.5 Note that the Asimov dataset generated this way uses the nominal values of all model parameters to define the dataset. Another option is to add --toysFrequentist , which causes a fit to the data to be performed first (with r frozen to the --expectSignal value) and then any subsequent Asimov datasets or toys are generated using the post-fit values of the model parameters. In general this will result in a different value for the expected significance due to changes in the background normalisation and shape induced by the fit to data: combine -M Significance workspace_part3.root -m 200 --rMin -1 --rMax 5 -t -1 --expectSignal 1.5 --toysFrequentist Tasks and questions: Note how much the expected significance changes with the --toysFrequentist option. Does the change make sense given the difference in the post-fit and pre-fit distributions you looked at in the previous section? Advanced task It is also possible to calculate the significance using toys with HybridNew (details here ) if we are in a situation where the asymptotic approximation is not reliable or if we just want to verify the result. Why might this be challenging for a high significance, say larger than 5\\sigma 5\\sigma ? E: Signal strength measurement and uncertainty breakdown We have seen that with FitDiagnostics we can make a measurement of the best-fit signal strength and uncertainty. In the asymptotic approximation we find an interval at the \\alpha \\alpha CL around the best fit by identifying the parameter values at which our test statistic q=\u22122\\Delta \\ln L q=\u22122\\Delta \\ln L equals a critical value. This value is the \\alpha \\alpha quantile of the \\chi^2 \\chi^2 distribution with one degree of freedom. In the expression for q we calculate the difference in the profile likelihood between some fixed point and the best-fit. Depending on what we want to do with the measurement, e.g. whether it will be published in a journal, we may want to choose a more precise method for finding these intervals. There are a number of ways that parameter uncertainties are estimated in combine, and some are more precise than others: Covariance matrix: calculated by the Minuit HESSE routine, this gives a symmetric uncertainty by definition and is only accurate when the profile likelihood for this parameter is symmetric and parabolic. Minos error: calculated by the Minuit MINOS route - performs a search for the upper and lower values of the parameter that give the critical value of q q for the desired CL. Return an asymmetric interval. This is what FitDiagnostics does by default, but only for the parameter of interest. Usually accurate but prone to fail on more complex models and not easy to control the tolerance for terminating the search. RobustFit error: a custom implementation in combine similar to Minos that returns an asymmetric interval, but with more control over the precision. Enabled by adding --robustFit 1 when running FitDiagnostics . Explicit scan of the profile likelihood on a chosen grid of parameter values. Interpolation between points to find parameter values corresponding to appropriate d. It is a good idea to use this for important measurements since we can see by eye that there are no unexpected features in the shape of the likelihood curve. In this section we will look at the last approach, using the MultiDimFit mode of combine. By default this mode just performs a single fit to the data: combine -M MultiDimFit workspace_part3.root -n .part3E -m 200 --rMin -1 --rMax 2 You should see the best-fit value of the signal strength reported and nothing else. By adding the --algo X option combine will run an additional algorithm after this best fit. Here we will use --algo grid , which performs a scan of the likelihood with r fixed to a set of different values. The set of points will be equally spaced between the --rMin and --rMax values, and the number of points is controlled with --points N : combine -M MultiDimFit workspace_part3.root -n .part3E -m 200 --rMin -1 --rMax 2 --algo grid --points 30 The results of the scan are written into the output file, if opened interactively should see: Show output root [1] limit->Scan(\"r:deltaNLL\") ************************************ * Row * r * deltaNLL * ************************************ * 0 * 0.5007491 * 0 * * 1 * -0.949999 * 5.0564951 * * 2 * -0.850000 * 4.4238934 * * 3 * -0.75 * 3.8231189 * * 4 * -0.649999 * 3.2575576 * * 5 * -0.550000 * 2.7291250 * * 6 * -0.449999 * 2.2421045 * * 7 * -0.349999 * 1.7985963 * * 8 * -0.25 * 1.4009944 * * 9 * -0.150000 * 1.0515967 * * 10 * -0.050000 * 0.7510029 * * 11 * 0.0500000 * 0.5008214 * * 12 * 0.1500000 * 0.3021477 * * 13 * 0.25 * 0.1533777 * * 14 * 0.3499999 * 0.0552866 * * 15 * 0.4499999 * 0.0067232 * * 16 * 0.5500000 * 0.0062941 * * 17 * 0.6499999 * 0.0515425 * * 18 * 0.75 * 0.1421113 * * 19 * 0.8500000 * 0.2767190 * * 20 * 0.9499999 * 0.4476667 * * 21 * 1.0499999 * 0.6578899 * * 22 * 1.1499999 * 0.9042779 * * 23 * 1.25 * 1.1839065 * * 24 * 1.3500000 * 1.4943070 * * 25 * 1.4500000 * 1.8335367 * * 26 * 1.5499999 * 2.1992974 * * 27 * 1.6499999 * 2.5904724 * * 28 * 1.75 * 3.0033872 * * 29 * 1.8500000 * 3.4358899 * * 30 * 1.9500000 * 3.8883462 * ************************************ To turn this into a plot run: python plot1DScan.py higgsCombine.part3E.MultiDimFit.mH200.root -o single_scan This script will also perform a spline interpolation of the points to give accurate values for the uncertainties. In the next step we will split this total uncertainty into two components. It is typical to separate the contribution from statistics and systematics, and sometimes even split the systematic part into different components. This gives us an idea of which aspects of the uncertainty dominate. The statistical component is usually defined as the uncertainty we would have if all the systematic uncertainties went to zero. We can emulate this effect by freezing all the nuisance parameters when we do the scan in r , such that they do not vary in the fit. This is achieved by adding the --freezeParameters allConstrainedNuisances option. It would also work if the parameters are specified explicitly, e.g. --freezeParameters CMS_eff_t,lumi_13TeV,..., but the allConstrainedNuisances option is more concise. Run the scan again with the systematics frozen, and use the plotting script to overlay this curve with the previous one: combine -M MultiDimFit workspace_part3.root -n .part3E.freezeAll -m 200 --rMin -1 --rMax 2 --algo grid --points 30 --freezeParameters allConstrainedNuisances python plot1DScan.py higgsCombine.part3E.MultiDimFit.mH200.root --others 'higgsCombine.part3E.freezeAll.MultiDimFit.mH200.root:FreezeAll:2' -o freeze_first_attempt This doesn't look quite right - the best-fit has been shifted because unfortunately the --freezeParameters option acts before the initial fit, whereas we only want to add it for the scan after this fit. To remedy this we can use a feature of combine that lets us save a \"snapshot\" of the best-fit parameter values, and reuse this snapshot in subsequent fits. First we perform a single fit, adding the --saveWorkspace option: combine -M MultiDimFit workspace_part3.root -n .part3E.snapshot -m 200 --rMin -1 --rMax 2 --saveWorkspace The output file will now contain a copy of our workspace from the input, and this copy will contain a snapshot of the best-fit parameter values. We can now run the frozen scan again, but instead using this copy of the workspace as input, and restoring the snapshot that was saved: combine -M MultiDimFit higgsCombine.part3E.snapshot.MultiDimFit.mH200.root -n .part3E.freezeAll -m 200 --rMin -1 --rMax 2 --algo grid --points 30 --freezeParameters allConstrainedNuisances --snapshotName MultiDimFit python plot1DScan.py higgsCombine.part3E.MultiDimFit.mH200.root --others 'higgsCombine.part3E.freezeAll.MultiDimFit.mH200.root:FreezeAll:2' -o freeze_second_attempt --breakdown Syst,Stat Now the plot should look correct: We added the --breakdown Syst,Stat option to the plotting script to make it calculate the systematic component, which is defined simply as $\\sigma_{\\text{syst}} = \\sqrt{\\sigma^2_{\\text{tot}} - \\sigma^2_{\\text{stat}}}. To split the systematic uncertainty into different components we just need to run another scan with a subset of the systematics frozen. For example, say we want to split this into experimental and theoretical uncertainties, we would calculate the uncertainties as: \\sigma_{\\text{theory}} = \\sqrt{\\sigma^2_{\\text{tot}} - \\sigma^2_{\\text{fr.theory}}} \\sigma_{\\text{theory}} = \\sqrt{\\sigma^2_{\\text{tot}} - \\sigma^2_{\\text{fr.theory}}} \\sigma_{\\text{expt}} = \\sqrt{\\sigma^2_{\\text{fr.theory}} - \\sigma^2_{\\text{fr.theory+expt}}} \\sigma_{\\text{expt}} = \\sqrt{\\sigma^2_{\\text{fr.theory}} - \\sigma^2_{\\text{fr.theory+expt}}} \\sigma_{\\text{stat}} = \\sigma_{\\text{fr.theory+expt}} \\sigma_{\\text{stat}} = \\sigma_{\\text{fr.theory+expt}} where fr.=freeze. While it is perfectly fine to just list the relevant nuisance parameters in the --freezeParameters argument for the \\sigma_{\\text{fr.theory}} \\sigma_{\\text{fr.theory}} scan, a convenient way can be to define a named group of parameters in the text datacard and then freeze all parameters in this group with --freezeNuisanceGroups . The syntax for defining a group is: [group name] group = uncertainty_1 uncertainty_2 ... uncertainty_N Tasks and questions: Take our stat+syst split one step further and separate the systematic part into two: one part for hadronic tau uncertainties and one for all others. Do this by defining a tauID group in the datacard including the following parameters: CMS_eff_t , CMS_eff_t_highpt , and the three CMS_scale_t_X uncertainties. To plot this and calculate the split via the relations above you can just add further arguments to the --others option in the plot1DScan.py script. Each is of the form: '[file]:[label]:[color]' . The --breakdown argument should also be extended to three terms. How important are these tau-related uncertainties compared to the others? F: Use of channel masking We will now return briefly to the topic of blinding. We've seen that we can compute expected results by performing any combine method on an Asimov dataset generated using -t -1 . This is useful, because we can optimise our analysis without introducing any accidental bias that might come from looking at the data in the signal region. However our control regions have been chosen specifically to be signal-free, and it would be useful to use the data here to set the normalisation of our backgrounds even while the signal region remains blinded. Unfortunately there's no easy way to generate a partial Asimov dataset just for the signal region, but instead we can use a feature called \"channel masking\" to remove specific channels from the likelihood evaluation. One useful application of this feature is to make post-fit plots of the signal region from a control-region-only fit. To use the masking we first need to rerun text2workspace.py with an extra option that will create variables named like mask_[channel] in the workspace: text2workspace.py part3_combined.txt -m 200 -o workspace_part3_with_masks.root --channel-masks These parameters have a default value of 0 which means the channel is not masked. By setting it to 1 the channel is masked from the likelihood evaluation. Task: Run the same FitDiagnostics command as before to save the post-fit shapes, but add an option --setParameters mask_signal_region=1 . Note that the s+b fit will probably fail in this case, since we are no longer fitting a channel that contains signal, however the b-only fit should work fine. Task: Compare the expected background distribution and uncertainty to the pre-fit, and to the background distribution from the full fit you made before. Part 4: Physics models Topics covered in this section: A: Writing a simple physics model B: Performing and plotting 2D likelihood scans With combine we are not limited to parametrising the signal with a single scaling parameter r . In fact we can define any arbitrary scaling using whatever functions and parameters we would like. For example, when measuring the couplings of the Higgs boson to the different SM particles we would introduce a POI for each coupling parameter, for example \\kappa_{\\text{W}} \\kappa_{\\text{W}} , \\kappa_{\\text{Z}} \\kappa_{\\text{Z}} , \\kappa_{\\tau} \\kappa_{\\tau} etc. We would then generate scaling terms for each i\\rightarrow \\text{H}\\rightarrow j i\\rightarrow \\text{H}\\rightarrow j process in terms of how the cross section ( \\sigma_i(\\kappa) \\sigma_i(\\kappa) ) and branching ratio ( \\frac{\\Gamma_i(\\kappa)}{\\Gamma_{\\text{tot}}(\\kappa)} \\frac{\\Gamma_i(\\kappa)}{\\Gamma_{\\text{tot}}(\\kappa)} ) scale relative to the SM prediction. This parametrisation of the signal (and possibly backgrounds too) is specified in a physics model . This is a python class that is used by text2workspace.py to construct the model in terms of RooFit objects. There is documentation on using phyiscs models here . A: Writing a simple physics model An example physics model that just implements a single parameter r is given in DASModel.py : Show DASModel.py from HiggsAnalysis.CombinedLimit.PhysicsModel import * class DASModel(PhysicsModel): def doParametersOfInterest(self): \"\"\"Create POI and other parameters, and define the POI set.\"\"\" self.modelBuilder.doVar(\"r[0,0,10]\") self.modelBuilder.doSet(\"POI\", \",\".join(['r'])) def getYieldScale(self, bin, process): \"Return the name of a RooAbsReal to scale this yield by or the two special values 1 and 0 (don't scale, and set to zero)\" if self.DC.isSignal[process]: print 'Scaling %s/%s by r' % (bin, process) return \"r\" return 1 dasModel = DASModel() In this we override two methods of the basic PhysicsModel class: doParametersOfInterest and getYieldScale . In the first we define our POI variables, using the doVar function which accepts the RooWorkspace factory syntax for creating variables, and then define all our POIs in a set via the doSet function. The second function will be called for every process in every channel (bin), and using the corresponding strings we have to specify how that process should be scaled. Here we check if the process was declared as signal in the datacard, and if so scale it by r , otherwise if it is a background no scaling is applied ( 1 ). To use the physics model with text2workspace.py first copy it to the python directory in the combine package: cp DASModel.py $CMSSW_BASE/src/HiggsAnalysis/CombinedLimit/python/ In this section we will use the full datacards from the MSSM analysis. Have a look in part4/200/combined.txt . You will notice that there are now two signal processes declared: ggH and bbH . In the MSSM these cross sections can vary independently depending on the exact parameters of the model, so it is useful to be able to measure them independently too. First run text2workspace.py as follows, adding the -P option to specify the physics model, then verify the result of the fit: text2workspace.py part4/200/combined.txt -P HiggsAnalysis.CombinedLimit.DASModel:dasModel -m 200 -o workspace_part4.root combine -M MultiDimFit workspace_part4.root -n .part4A -m 200 --rMin 0 --rMax 2 Tasks and questions: Modify the physics model to scale the ggH and bbH processes by r_ggH and r_bbH separately. Then rerun the MultiDimFit command - you should see the result for both signal strengths printed. B: Performing and plotting 2D likelihood scans For a model with two POIs it is often useful to look at the how well we are able to measure both simultaneously. A natural extension of determining 1D confidence intervals on a single parameter like we did in part 3D is to determine confidence level regions in 2D. To do this we also use combine in a similar way, with -M MultiDimFit --algo grid . When two POIs are found combine will scan a 2D grid of points instead of a 1D array. Tasks and questions: Run a 2D likelihood scan in r_ggH and r_bbH . You can start with around 100 points but may need to increase this later too see more detail in the resulting plot. Have a look at the output limit tree, it should have branches for each POI as well as the usual deltaNLL value. You can use TTree::Draw to plot a 2D histogram of deltaNLL with r_ggH and r_bbH on the axes.","title":"Exercise: main features"},{"location":"part5/longexercise/#long-exercise-main-features-of-combine","text":"This exercise is designed to give a broad overview of the tools available for statistical analysis in CMS using the combine tool. Combine is a high-level tool for building RooFit/RooStats models and running common statistical methods. We will cover the typical aspects of setting up an analysis and producing the results, as well as look at ways in which we can diagnose issues and get a deeper understanding of the statistical model. This is a long exercise - expect to spend some time on it especially if you are new to Combine. If you get stuck while working through this exercise or have questions specifically about the exercise, you can ask them on this mattermost channel . Finally, we also provide some solutions to some of the questions that are asked as part of the exercise. These are available here . For the majority of this course we will work with a simplified version of a real analysis, that nonetheless will have many features of the full analysis. The analysis is a search for an additional heavy neutral Higgs boson decaying to tau lepton pairs. Such a signature is predicted in many extensions of the standard model, in particular the minimal supersymmetric standard model (MSSM). You can read about the analysis in the paper here . The statistical inference makes use of a variable called the total transverse mass ( M_{\\mathrm{T}}^{\\mathrm{tot}} M_{\\mathrm{T}}^{\\mathrm{tot}} ) that provides good discrimination between the resonant high-mass signal and the main backgrounds, which have a falling distribution in this high-mass region. The events selected in the analysis are split into a several categories which target the main di-tau final states as well as the two main production modes: gluon-fusion (ggH) and b-jet associated production (bbH). One example is given below for the fully-hadronic final state in the b-tag category which targets the bbH signal: Initially we will start with the simplest analysis possible: a one-bin counting experiment using just the high M_{\\mathrm{T}}^{\\mathrm{tot}} M_{\\mathrm{T}}^{\\mathrm{tot}} region of this distribution, and from there each section of this exercise will expand on this, introducing a shape-based analysis and adding control regions to constrain the backgrounds.","title":"Long exercise: main features of Combine"},{"location":"part5/longexercise/#getting-started","text":"We need to set up a new CMSSW area and checkout the combine package: export SCRAM_ARCH=slc7_amd64_gcc700 cmsrel CMSSW_10_2_13 cd CMSSW_10_2_13/src cmsenv git clone https://github.com/cms-analysis/HiggsAnalysis-CombinedLimit.git HiggsAnalysis/CombinedLimit cd HiggsAnalysis/CombinedLimit cd $CMSSW_BASE/src/HiggsAnalysis/CombinedLimit git fetch origin git checkout v8.1.0 cd $CMSSW_BASE/src We will also make use another package, CombineHarvester , which contains some high-level tools for working with combine. The following command will download the repository and checkout just the parts of it we need for this tutorial: bash <(curl -s https://raw.githubusercontent.com/cms-analysis/CombineHarvester/master/CombineTools/scripts/sparse-checkout-https.sh) Now make sure the CMSSW area is compiled: scramv1 b clean; scramv1 b Finally we will checkout the working directory for these tutorials - this contains all the inputs needed to run the exercises below: cd $CMSSW_BASE/src git clone https://gitlab.cern.ch/agilbert/cms-das-stats.git cd cms-das-stats","title":"Getting started"},{"location":"part5/longexercise/#part-1-a-one-bin-counting-experiment","text":"Topics covered in this section: A: Computing limits using the asymptotic approximation Advanced section: B: Computing limits with toys We will begin with a simplified version of a datacard from the MSSM \\phi\\rightarrow\\tau\\tau \\phi\\rightarrow\\tau\\tau analysis that has been converted to a one-bin counting experiment, as described above. While the full analysis considers a range of signal mass hypotheses, we will start by considering just one: m_{\\phi} m_{\\phi} =800GeV. Click the text below to study the datacard ( datacard_part1.txt in the cms-das-stats directory): Show datacard imax 1 number of bins jmax 4 number of processes minus 1 kmax * number of nuisance parameters -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- bin signal_region observation 10.0 -------------------------------------------------------------------------------- bin signal_region signal_region signal_region signal_region signal_region process ttbar diboson Ztautau jetFakes bbHtautau process 1 2 3 4 0 rate 4.43803 3.18309 3.7804 1.63396 0.711064 -------------------------------------------------------------------------------- CMS_eff_b lnN 1.02 1.02 1.02 - 1.02 CMS_eff_t lnN 1.12 1.12 1.12 - 1.12 CMS_eff_t_highpt lnN 1.1 1.1 1.1 - 1.1 acceptance_Ztautau lnN - - 1.08 - - acceptance_bbH lnN - - - - 1.05 acceptance_ttbar lnN 1.005 - - - - lumi_13TeV lnN 1.025 1.025 1.025 - 1.025 norm_jetFakes lnN - - - 1.2 - xsec_Ztautau lnN - - 1.04 - - xsec_diboson lnN - 1.05 - - - xsec_ttbar lnN 1.06 - - - - The layout of the datacard is as follows: At the top are the numbers imax , jmax and kmax representing the number of bins, processes and nuisance parameters respectively. Here a \"bin\" can refer to a literal single event count as in this example, or a full distribution we are fitting, in general with many histogram bins, as we will see later. We will refer to both as \"channels\" from now on. It is possible to replace these numbers with * and they will be deduced automatically. The first line starting with bin gives a unique label to each channel, and the following line starting with observation gives the number of events observed in data. In the remaining part of the card there are several columns: each one represents one process in one channel. The first four lines labelled bin , process , process and rate give the channel label, the process label, a process identifier ( <=0 for signal, >0 for background) and the number of expected events respectively. The remaining lines describe sources of systematic uncertainty. Each line gives the name of the uncertainty, (which will become the name of the nuisance parameter inside our RooFit model), the type of uncertainty (\"lnN\" = log-normal normalisation uncertainty) and the effect on each process in each channel. E.g. a 20% uncertainty on the yield is written as 1.20. It is also possible to add a hash symbol ( # ) at the start of a line, which combine will then ignore when it reads the card. We can now run combine directly using this datacard as input. The general format for running combine is: combine -M [method] [datacard] [additional options...]","title":"Part 1: A one-bin counting experiment"},{"location":"part5/longexercise/#a-computing-limits-using-the-asymptotic-approximation","text":"As we are searching for a signal process that does not exist in the standard mode, it's natural to set an upper limit on the cross section times branching fraction of the process (assuming our dataset does not contain a significant discovery of new physics). Combine has dedicated method for calculating upper limits. The most commonly used one is AsymptoticLimits , which implements the CLs criterion and uses the profile likelihood ratio as the test statistic. As the name implies, the test statistic distributions are determined analytically in the asymptotic approximation, so there is no need for more time-intensive toy throwing and fitting. Try running the following command: combine -M AsymptoticLimits datacard_part1.txt -n .part1A You should see the results of the observed and expected limit calculations printed to the screen. Here we have added an extra option, -n .part1A , which is short for --name , and is used to label the output file combine produces, which in this case will be called higgsCombine.part1A.AsymptoticLimits.mH120.root . The file name depends on the options we ran with, and is of the form: higgsCombine[name].[method].mH[mass].root . The file contains a TTree called limit which stores the numerical values returned by the limit computation. Note that in our case we did not set a signal mass when running combine (i.e. -m 800 ), so the output file just uses the default value of 120 . This does not affect our result in any way though, just the label that is used on the output file. The limits are given on a parameter called r . This is the default parameter of interest (POI) that is added to the model automatically. It is a linear scaling of the normalisation of all signal processes given in the datacard, i.e. if s_{i,j} s_{i,j} is the nominal number of signal events in channel i i for signal process j j , then the normalisation of that signal in the model is given as r\\cdot s_{i,j}(\\vec{\\theta}) r\\cdot s_{i,j}(\\vec{\\theta}) , where \\vec{\\theta} \\vec{\\theta} represents the set of nuisance parameters which may also affect the signal normalisation. We therefore have some choice in the interpretation of r: for the measurement of a process with a well defined SM prediction we may enter this as the nominal yield in the datacard, such that r=1 r=1 corresponds to this SM expectation, whereas for setting limits on BSM processes we may choose the nominal yield to correspond to some cross section, e.g. 1 pb, such that we can interpret the limit as a cross section limit directly. In this example the signal has been normalised to a cross section times branching fraction of 1 fb. The expected limit is given under the background-only hypothesis. The median value under this hypothesis as well as the quantiles needed to give the 68% and 95% intervals are also calculated. These are all the ingredients needed to produce the standard limit plots you will see in many CMS results, for example the \\sigma \\times \\mathcal{B} \\sigma \\times \\mathcal{B} limits for the \\text{bb}\\phi\\rightarrow\\tau\\tau \\text{bb}\\phi\\rightarrow\\tau\\tau process: In this case we only computed the values for one signal mass hypothesis, indicated by a red dashed line. Tasks and questions: There are some important uncertainties missing from the datacard above. Add the uncertainty on the luminosity (name: lumi_13TeV ) which has a 2.5% effect on all processes (except the jetFakes , which are taken from data), and uncertainties on the inclusive cross sections of the Ztautau and ttbar processes (with names xsec_Ztautau and xsec_diboson ) which are 4% and 6% respectively. Try changing the values of some uncertainties (up or down, or removing them altogether) - how do the expected and observed limits change? Now try changing the number of observed events. The observed limit will naturally change, but the expected does too - why might this be? There are other command line options we can supply to combine which will change its behaviour when run. You can see the full set of supported options by doing combine -h . Many options are specific to a given method, but others are more general and are applicable to all methods. Throughout this tutorial we will highlight some of the most useful options you may need to use, for example: The range on the signal strength modifier: --rMin=X and --rMax=Y : In RooFit parameters can optionally have a range specified. The implication of this is that their values cannot be adjusted beyond the limits of this range. The min and max values can be adjusted though, and we might need to do this for our POI r if the order of magnitude of our measurement is different from the default range of [0, 20] . This will be discussed again later in the tutorial. Verbosity: -v X : By default combine does not usually produce much output on the screen other the main result at the end. However, much more detailed information can be printed by setting the -v N with N larger than zero. For example at -v 3 the logs from the minimizer, Minuit, will also be printed. These are very useful for debugging problems with the fit.","title":"A: Computing limits using the asymptotic approximation"},{"location":"part5/longexercise/#advanced-section-b-computing-limits-with-toys","text":"Now we will look at computing limits without the asymptotic approximation, so instead using toy datasets to determine the test statistic distributions under the signal+background and background-only hypotheses. This can be necessary if we are searching for signal in bins with a small number of events expected. In combine we will use the HybridNew method to calculate limits using toys. This mode is capable of calculating limits with several different test statistics and with fine-grained control over how the toy datasets are generated internally. To calculate LHC-style profile likelihood limits (i.e. the same as we did with the asymptotic) we set the option --LHCmode LHC-limits . You can read more about the different options in the Combine documentation . Run the following command: combine -M HybridNew datacard_part1.txt --LHCmode LHC-limits -n .part1B --saveHybridResult --fork 0 In contrast to AsymptoticLimits this will only determine the observed limit, and will take around five minutes. There will not be much output to the screen while combine is running. You can add the option -v 1 to get a better idea of what is going on. You should see combine stepping around in r , trying to find the value for which CLs = 0.05, i.e. the 95% CL limit. The --saveHybridResult option will cause the test statistic distributions that are generated at each tested value of r to be saved in the output ROOT file. To get an expected limit add the option --expectedFromGrid X , where X is the desired quantile, e.g. for the median: combine -M HybridNew datacard_part1.txt --LHCmode LHC-limits -n .part1B --saveHybridResult --fork 0 --expectedFromGrid 0.500 Calculate the median expected limit and the 68% range. The 95% range could also be done, but note it will take much longer to run the 0.025 quantile. While combine is running you can move on to the next steps below. Tasks and questions: - In contrast to AsymptoticLimits , with HybridNew each limit comes with an uncertainty. What is the origin of this uncertainty? - How good is the agreement between the asymptotic and toy-based methods? - Why does it take longer to calculate the lower expected quantiles (e.g. 0.025, 0.16)? Think about how the statistical uncertainty on the CLs value depends on CLs+b and CLb. Next plot the test statistic distributions stored in the output file: python $CMSSW_BASE/src/HiggsAnalysis/CombinedLimit/test/plotTestStatCLs.py --input higgsCombine.part1B.HybridNew.mH120.root --poi r --val all --mass 120 This produces a new ROOT file cls_qmu_distributions.root containing the plots, to save them as pdf/png files run this small script and look at the resulting figures: python printTestStatPlots.py cls_qmu_distributions.root","title":"Advanced section: B: Computing limits with toys"},{"location":"part5/longexercise/#advanced-exercises","text":"These distributions can be useful in understanding features in the CLs limits, especially in the low statistics regime. To explore this, try reducing the observed and expected yields in the datacard by a factor of 10, and rerun the above steps to compare the observed and expected limits with the asymptotic approach, and plot the test statistic distributions. Tasks and questions: Is the asymptotic limit still a good approximation? You might notice that the test statistic distributions are not smooth but rather have several \"bump\" structures? Where might this come from? Try reducing the size of the systematic uncertainties to make them more pronounced. Note that for more complex models the fitting time can increase significantly, making it infeasible to run all the toy-based limits interactively like this. An alternative strategy is documented here","title":"Advanced exercises"},{"location":"part5/longexercise/#part-2-a-shape-based-analysis","text":"Topics covered in this section: A: Setting up the datacard B: Running combine for a blind analysis C: Using FitDiagnostics D: MC statistical uncertainties","title":"Part 2: A shape-based analysis"},{"location":"part5/longexercise/#a-setting-up-the-datacard","text":"Now we move to the next step: instead of a one-bin counting experiment we will fit a binned distribution. In a typical analysis we will produce TH1 histograms of some variable sensitive to the presence of signal: one for the data and one for each signal and background processes. Then we add a few extra lines to the datacard to link the declared processes to these shapes which are saved in a ROOT file, for example: Show datacard imax 1 jmax 1 kmax * --------------- shapes * * simple-shapes-TH1_input.root $PROCESS $PROCESS_$SYSTEMATIC shapes signal * simple-shapes-TH1_input.root $PROCESS$MASS $PROCESS$MASS_$SYSTEMATIC --------------- bin bin1 observation 85 ------------------------------ bin bin1 bin1 process signal background process 0 1 rate 10 100 -------------------------------- lumi lnN 1.10 1.0 bgnorm lnN 1.00 1.3 alpha shape - 1 Note that as with the one-bin card, the total nominal rate of a given process must be specified in the rate line of the datacard. This should agree with the value returned by TH1::Integral . However, we can also put a value of -1 and the Integral value will be substituted automatically. There are two other differences with respect to the one-bin card: A new block of lines at the top defining how channels and processes are mapped to the histograms (more than one line can be used) In the list of systematic uncertainties some are marked as shape instead of lnN The syntax of the \"shapes\" line is: shapes [process] [channel] [file] [histogram] [histogram_with_systematics] . It is possible to use the * wildcard to map multiple processes and/or channels with one line. The histogram entries can contain the $PROCESS , $CHANNEL and $MASS place-holders which will be substituted when searching for a given (process, channel) combination. The value of $MASS is specified by the -m argument when combine. By default the observed data process name will be data_obs . Shape uncertainties can be added by supplying two additional histograms for a process, corresponding to the distribution obtained by shifting that parameter up and down by one standard deviation. These shapes will be interpolated quadratically for shifts below 1\\sigma 1\\sigma and linearly beyond. The normalizations are interpolated linearly in log scale just like we do for log-normal uncertainties. The final argument of the \"shapes\" line above should contain the $SYSTEMATIC place-holder which will be substituted by the systematic name given in the datacard. In the list of uncertainties the interpretation of the values for shape lines is a bit different from lnN . The effect can be \"-\" or 0 for no effect, 1 for normal effect, and possibly something different from 1 to test larger or smaller effects (in that case, the unit Gaussian is scaled by that factor before using it as parameter for the interpolation). In this section we will use a datacard corresponding to the full distribution that was shown at the start of section 1, not just the high mass region. Have a look at datacard_part2.txt : this is still currently a one-bin counting experiment, however the yields are much higher since we now consider the full range of M_{\\mathrm{T}}^{\\mathrm{tot}} M_{\\mathrm{T}}^{\\mathrm{tot}} . If you run the asymptotic limit calculation on this you should find the sensitivity is significantly worse than before. The first task is to convert this to a shape analysis: the file datacard_part2.shapes.root contains all the necessary histograms, including those for the relevant shape systematic uncertainties. Add the relevant shapes lines to the top of the datacard (after the kmax line) to map the processes to the correct TH1s in this file. Hint: you will need a different line for the signal process. Compared to the counting experiment we must also consider the effect of uncertainties that change the shape of the distribution. Some, like CMS_eff_t_highpt , were present before, as it has both a shape and normalisation effect. Others are primarily shape effects so were not included before. Add the following shape uncertainties: top_pt_ttbar_shape affecting ttbar ,the tau energy scale uncertainties CMS_scale_t_1prong0pi0_13TeV , CMS_scale_t_1prong1pi0_13TeV and CMS_scale_t_3prong0pi0_13TeV affecting all processes except jetFakes , and CMS_eff_t_highpt also affecting the same processes. Once this is done you can run the asymptotic limit calculation on this datacard. From now on we will convert the text datacard into a RooFit workspace ourselves instead of combine doing it internally every time we run. This is a good idea for more complex analyses since the conversion step can take a notable amount of time. For this we use the text2workspace.py command: text2workspace.py datacard_part2.txt -m 800 -o workspace_part2.root And then we can use this as input to combine instead of the text datacard: combine -M AsymptoticLimits workspace_part2.root -m 800 Tasks and questions: Verify that the sensitivity of the shape analysis is indeed improved over the counting analysis in the first part. Advanced task : You can open the workspace ROOT file interactively and print the contents: w->Print(); . Each process is represented by a PDF object that depends on the shape morphing nuisance parameters. From the workspace, choose a process and shape uncertainty, and make a plot overlaying the nominal shape with different values of the shape morphing nuisance parameter. You can change the value of a parameter with w->var(\"X\")->setVal(Y) , and access a particular pdf with w->pdf(\"Z\") . PDF objects in RooFit have a createHistogram method that requires the name of the observable (the variable defining the x-axis) - this is called CMS_th1x in combine datacards. Feel free to ask for help with this!","title":"A: Setting up the datacard"},{"location":"part5/longexercise/#b-running-combine-for-a-blind-analysis","text":"Most analyses are developed and optimised while we are \"blind\" to the region of data where we expect our signal to be. With AsymptoticLimits we can choose just to run the expected limit ( --run expected ), so as not to calculate the observed. However the data is still used, even for the expected, since in the frequentist approach a background-only fit to the data is performed to define the Asimov dataset used to calculate the expected limits. To skip this fit to data and use the pre-fit state of the model the option --run blind or --noFitAsimov can be used. Task: Compare the expected limits calculated with --run expected and --run blind . Why are they different? A more general way of blinding is to use combine's toy and Asimov dataset generating functionality. You can read more about this here . These options can be used with any method in combine, not just AsymptoticLimits . Task: Calculate a blind limit by generating a background-only Asimov with the -t -1 option instead of using the AsymptoticLimits specific options. You should find the observed limit is the same as the expected. Then see what happens if you inject a signal into the Asimov dataset using the --expectSignal [X] option.","title":"B: Running combine for a blind analysis"},{"location":"part5/longexercise/#c-using-fitdiagnostics","text":"We will now explore one of the most commonly used modes of combine: FitDiagnostics . As well as allowing us to make a measurement of some physical quantity (as opposed to just setting a limit on it), this method is useful to gain additional information about the model and the behaviour of the fit. It performs two fits: A \"background-only\" (b-only) fit: first POI (usually \"r\") fixed to zero A \"signal+background\" (s+b) fit: all POIs are floating With the s+b fit combine will report the best-fit value of our signal strength modifier r . As well as the usual output file, a file named fitdiagnostics.root is produced which contains additional information. In particular it includes two RooFitResult objects, one for the b-only and one for the s+b fit, which store the fitted values of all the nuisance parameters (NPs) and POIs as well as estimates of their uncertainties. The covariance matrix from both fits is also included, from which we can learn about the correlations between parameters. Run the FitDiagnostics method on our workspace: combine -M FitDiagnostics workspace_part2.root -m 800 --rMin -20 --rMax 20 Open the resulting fitDiagnostics.root interactively and print the contents of the s+b RooFitResult: root [1] fit_s->Print() Show output RooFitResult: minimized FCN value: -4.7666, estimated distance to minimum: 3.31389e-05 covariance matrix quality: Full, accurate covariance matrix Status : MINIMIZE=0 HESSE=0 Floating Parameter FinalValue +/- Error -------------------- -------------------------- CMS_eff_b -4.3559e-02 +/- 9.87e-01 CMS_eff_t -2.6382e-01 +/- 7.27e-01 CMS_eff_t_highpt -4.7214e-01 +/- 9.56e-01 CMS_scale_t_1prong0pi0_13TeV -1.5884e-01 +/- 5.89e-01 CMS_scale_t_1prong1pi0_13TeV -1.6512e-01 +/- 4.91e-01 CMS_scale_t_3prong0pi0_13TeV -3.0668e-01 +/- 6.03e-01 acceptance_Ztautau -3.1059e-01 +/- 8.57e-01 acceptance_bbH -5.8325e-04 +/- 9.94e-01 acceptance_ttbar 4.7839e-03 +/- 9.94e-01 lumi_13TeV -5.4684e-02 +/- 9.83e-01 norm_jetFakes -9.3975e-02 +/- 2.54e-01 r -2.7327e+00 +/- 2.57e+00 top_pt_ttbar_shape 1.7614e-01 +/- 6.97e-01 xsec_Ztautau -1.5991e-01 +/- 9.61e-01 xsec_diboson 3.8745e-02 +/- 9.94e-01 xsec_ttbar 5.8025e-02 +/- 9.41e-01 There are several useful pieces of information here. At the top the status codes from the fits that were performed is given. In this case we can see that two algorithms were run: MINIMIZE and HESSE , both of which returned a successful status code (0). Both of these are routines in the Minuit2 minimization package - the default minimizer used in RooFit. The first performs the main fit to the data, and the second calculates the covariance matrix at the best-fit point. It is important to always check this second step was successful and the message \"Full, accurate covariance matrix\" is printed, otherwise the parameter uncertainties can be very inaccurate, even if the fit itself was successful. Underneath this the best-fit values ( \\theta \\theta ) and symmetrised uncertainties for all the floating parameters are given. For all the constrained nuisance parameters a convention is used by which the nominal value ( \\theta_I \\theta_I ) is zero, corresponding to the mean of a Gaussian constraint PDF with width 1.0, such that the parameter values \\pm 1.0 \\pm 1.0 correspond to the \\pm 1\\sigma \\pm 1\\sigma input uncertainties. A more useful way of looking at this is to compare the pre- and post-fit values of the parameters, to see how much the fit to data has shifted and constrained these parameters with respect to the input uncertainty. The script diffNuisances.py can be used for this: python diffNuisances.py fitDiagnostics.root --all Show output name b-only fit s+b fit rho CMS_eff_b -0.04, 0.99 -0.04, 0.99 +0.01 CMS_eff_t * -0.24, 0.73* * -0.26, 0.73* +0.06 CMS_eff_t_highpt * -0.56, 0.93* * -0.47, 0.96* +0.03 CMS_scale_t_1prong0pi0_13TeV * -0.17, 0.58* * -0.16, 0.59* -0.04 CMS_scale_t_1prong1pi0_13TeV ! -0.12, 0.45! ! -0.17, 0.49! +0.21 CMS_scale_t_3prong0pi0_13TeV * -0.31, 0.60* * -0.31, 0.60* +0.02 acceptance_Ztautau * -0.31, 0.86* * -0.31, 0.86* -0.05 acceptance_bbH +0.00, 0.99 -0.00, 0.99 +0.05 acceptance_ttbar +0.01, 0.99 +0.00, 0.99 +0.00 lumi_13TeV -0.05, 0.98 -0.05, 0.98 +0.01 norm_jetFakes ! -0.09, 0.25! ! -0.09, 0.25! -0.05 top_pt_ttbar_shape * +0.24, 0.69* * +0.18, 0.70* +0.23 xsec_Ztautau -0.16, 0.96 -0.16, 0.96 -0.02 xsec_diboson +0.03, 0.99 +0.04, 0.99 -0.02 xsec_ttbar +0.08, 0.94 +0.06, 0.94 +0.02 The numbers in each column are respectively \\frac{\\theta-\\theta_I}{\\sigma_I} \\frac{\\theta-\\theta_I}{\\sigma_I} (often called the pull , though note that more than one definition is in use for this), where \\sigma_I \\sigma_I is the input uncertainty; and the ratio of the post-fit to the pre-fit uncertainty \\frac{\\sigma}{\\sigma_I} \\frac{\\sigma}{\\sigma_I} . Tasks and questions: Which parameter has the largest pull? Which has the tightest constraint? Should we be concerned when a parameter is more strongly constrained than the input uncertainty (i.e. \\frac{\\sigma}{\\sigma_I}<1.0 \\frac{\\sigma}{\\sigma_I}<1.0 )? Check the pulls and constraints on a b-only and s+b asimov dataset instead. This check is required for all analyses in the Higgs PAG. It serves both as a closure test (do we fit exactly what signal strength we input?) and a way to check whether there are any infeasibly strong constraints while the analysis is still blind (typical example: something has probably gone wrong if we constrain the luminosity uncertainty to 10% of the input!) Advanced task: Sometimes there are problems in the fit model that aren't apparent from only fitting the Asimov dataset, but will appear when fitting randomised data. Follow the exercise on toy-by-toy diagnostics here to explore the tools available for this.","title":"C: Using FitDiagnostics"},{"location":"part5/longexercise/#d-mc-statistical-uncertainties","text":"So far there is an important source of uncertainty we have neglected. Our estimates of the backgrounds come either from MC simulation or from sideband regions in data, and in both cases these estimates are subject to a statistical uncertainty on the number of simulated or data events. In principle we should include an independent statistical uncertainty for every bin of every process in our model. It's important to note that combine/RooFit does not take this into account automatically - statistical fluctuations of the data are implicitly accounted for in the likelihood formalism, but statistical uncertainties in the model must be specified by us. One way to implement these uncertainties is to create a shape uncertainty for each bin of each process, in which the up and down histograms have the contents of the bin shifted up and down by the 1\\sigma 1\\sigma uncertainty. However this makes the likelihood evaluation computationally inefficient, and can lead to a large number of nuisance parameters in more complex models. Instead we will use a feature in combine called autoMCStats that creates these automatically from the datacard, and uses a technique called \"Barlow-Beeston-lite\" to reduce the number of systematic uncertainties that are created. This works on the assumption that for high MC event counts we can model the uncertainty with a Gaussian distribution. Given the uncertainties in different bins are independent, the total uncertainty of several processes in a particular bin is just the sum of N N individual Gaussians, which is itself a Gaussian distribution. So instead of N N nuisance parameters we need only one. This breaks down when the number of events is small and we are not in the Gaussian regime. The autoMCStats tool has a threshold setting on the number of events below which the the Barlow-Beeston-lite approach is not used, and instead a Poisson PDF is used to model per-process uncertainties in that bin. After reading the full documentation on autoMCStats here , add the corresponding line to your datacard. Start by setting a threshold of 0, i.e. [channel] autoMCStats 0 , to force the use of Barlow-Beeston-lite in all bins. Tasks and questions: Check how much the cross section measurement and uncertainties change using FitDiagnostics . It is also useful to check how the expected uncertainty changes using an Asimov dataset, say with r=10 injected. Advanced task: See what happens if the Poisson threshold is increased. Based on your results, what threshold would you recommend for this analysis?","title":"D: MC statistical uncertainties"},{"location":"part5/longexercise/#part-3-adding-control-regions","text":"Topics covered in this section: A: Use of rateParams B: Nuisance parameter impacts C: Post-fit distributions D: Calculating the significance E: Signal strength measurement and uncertainty breakdown F: Use of channel masking In a modern analysis it is typical for some or all of the backgrounds to be estimated using the data, instead of relying purely on MC simulation. This can take many forms, but a common approach is to use \"control regions\" (CRs) that are pure and/or have higher statistics for a given process. These are defined by event selections that are similar to, but non-overlapping with, the signal region. In our \\phi\\rightarrow\\tau\\tau \\phi\\rightarrow\\tau\\tau example the \\text{Z}\\rightarrow\\tau\\tau \\text{Z}\\rightarrow\\tau\\tau background normalisation can be calibrated using a \\text{Z}\\rightarrow\\mu\\mu \\text{Z}\\rightarrow\\mu\\mu CR, and the \\text{t}\\bar{\\text{t}} \\text{t}\\bar{\\text{t}} background using an e+\\mu e+\\mu CR. By comparing the number of data events in these CRs to our MC expectation we can obtain scale factors to apply to the corresponding backgrounds in the signal region (SR). The idea is that the data will gives us a more accurate prediction of the background with less systematic uncertainties. For example, we can remove the cross section and acceptance uncertainties in the SR, since we are no longer using the MC prediction (with a caveat discussed below). While we could simply derive these correction factors and apply them to our signal region datacard and better way is to include these regions in our fit model and tie the normalisations of the backgrounds in the CR and SR together. This has a number of advantages: Automatically handles the statistical uncertainty due to the number of data events in the CR Allows for the presence of some signal contamination in the CR to be handled correctly The CRs are typically not 100% pure in the background they're meant to control - other backgrounds may be present, with their own systematic uncertainties, some of which may be correlated with the SR or other CRs. Propagating these effects through to the SR \"by hand\" can become very challenging. In this section we will continue to use the same SR as in the previous one, however we will switch to a lower signal mass hypothesis, m_{\\phi}=200 m_{\\phi}=200 GeV, as its sensitivity depends more strongly on the background prediction than the high mass signal, so is better for illustrating the use of CRs. Here the nominal signal ( r=1 ) has been normalised to a cross section of 1 pb. The SR datacard for the 200 GeV signal is datacard_part3.txt . Two further datacards are provided: datacard_part3_ttbar_cr.txt and datacard_part3_DY_cr.txt which represent the CRs for the Drell-Yan and \\text{t}\\bar{\\text{t}} \\text{t}\\bar{\\text{t}} processes as described above. The cross section and acceptance uncertainties for these processes have pre-emptively been removed from the SR card. However we cannot get away with neglecting acceptance effects altogether. We are still implicitly using the MC simulation to predict to the ratio of events in the CR and SR, and this ratio will in general carry a theoretical acceptance uncertainty. If the CRs are well chosen then this uncertainty should be smaller than the direct acceptance uncertainty in the SR however. The uncertainties acceptance_ttbar_cr and acceptance_DY_cr have been added to these datacards cover this effect. Task: Calculate the ratio of CR to SR events for these two processes, as well as their CR purity to verify that these are useful CRs. The next step is to combine these datacards into one, which is done with the combineCards.py script: combineCards.py signal_region=datacard_part3.txt ttbar_cr=datacard_part3_ttbar_cr.txt DY_cr=datacard_part3_DY_cr.txt &> part3_combined.txt Each argument is of the form [new channel name]=[datacard.txt] . The new datacard is written to the screen by default, so we redirect the output into our new datacard file. The output looks like: Show datacard imax 3 number of bins jmax 8 number of processes minus 1 kmax 15 number of nuisance parameters ---------------------------------------------------------------------------------------------------------------------------------- shapes * DY_cr datacard_part3_DY_cr.shapes.root DY_control_region/$PROCESS DY_control_region/$PROCESS_$SYSTEMATIC shapes * signal_region datacard_part3.shapes.root signal_region/$PROCESS signal_region/$PROCESS_$SYSTEMATIC shapes bbHtautau signal_region datacard_part3.shapes.root signal_region/bbHtautau$MASS signal_region/bbHtautau$MASS_$SYSTEMATIC shapes * ttbar_cr datacard_part3_ttbar_cr.shapes.root tt_control_region/$PROCESS tt_control_region/$PROCESS_$SYSTEMATIC ---------------------------------------------------------------------------------------------------------------------------------- bin signal_region ttbar_cr DY_cr observation 3416 79251 365754 ---------------------------------------------------------------------------------------------------------------------------------- bin signal_region signal_region signal_region signal_region signal_region ttbar_cr ttbar_cr ttbar_cr ttbar_cr ttbar_cr DY_cr DY_cr DY_cr DY_cr DY_cr DY_cr process bbHtautau ttbar diboson Ztautau jetFakes Ztautau ttbar VV W QCD W Ztautau VV QCD ttbar Zmumu process 0 1 2 3 4 3 1 5 6 7 6 3 5 7 1 8 rate 198.521 683.017 96.5185 742.649 2048.94 150.025 67280.4 10589.6 597.336 308.965 59.9999 115.34 5273.43 141.725 34341.1 305423 ---------------------------------------------------------------------------------------------------------------------------------- CMS_eff_b lnN 1.02 1.02 1.02 1.02 - - - - - - - - - - - - CMS_eff_e lnN - - - - - 1.02 1.02 1.02 1.02 - - - - - - - The [new channel name]= part of the input arguments is not required, but it gives us control over how the channels in the combined card will be named, otherwise default values like ch1 , ch2 etc will be used.","title":"Part 3: Adding control regions"},{"location":"part5/longexercise/#a-use-of-rateparams","text":"We now have a combined datacard that we can run text2workspace.py on and start doing fits, however there is still one important ingredient missing. Right now the yields of the Ztautau process in the SR and Zmumu in the CR are not connected to each other in any way, and similarly for the ttbar processes. In the fit both would be adjusted by the nuisance parameters only, and constrained to the nominal yields. To remedy this we introduce rateParam directives to the datacard. A rateParam is a new free parameter that multiples the yield of a given process, just in the same way the signal strength r multiplies the signal yield. The syntax of a rateParam line in the datacard is [name] rateParam [channel] [process] [init] [min,max] where name is the chosen name for the parameter, channel and process specify which (channel, process) combination it should affect, init gives the initial value, and optionally [min,max] specifies the ranges on the RooRealVar that will be created. The channel and process arguments support the use of the wildcard * to match multiple entries. Task: Add two rateParam s with nominal values of 1.0 to the end of the combined datacard named rate_ttbar and rate_Zll . The former should affect the ttbar process in all channels, and the latter should affect the Ztautau and Zmumu processes in all channels. Set ranges of [0,5] to both. Note that a rateParam name can be repeated to apply it to multiple processes, e.g.: rateScale rateParam * procA 1.0 rateScale rateParam * procB 1.0 is perfectly valid and only one rateParam will be created. These parameters will allow the yields to float in the fit without prior constraint (unlike a regular lnN or shape systematic), with the yields in the CRs and SR tied together. Tasks and questions: Run text2workspace.py on this combined card and then use FitDiagnostics on an Asimov dataset with r=1 to get the expected uncertainty. Suggested command line options: --rMin 0 --rMax 2 Using the RooFitResult in the fitdiagnostics.root file, check the post-fit value of the rateParams. To what level are the normalisations of the DY and ttbar processes constrained? To compare to the previous approach of fitting the SR only, with cross section and acceptance uncertainties restored, an additional card is provided: datacard_part3_nocrs.txt . Run the same fit on this card to verify the improvement of the SR+CR approach","title":"A: Use of rateParams"},{"location":"part5/longexercise/#b-nuisance-parameter-impacts","text":"It is often useful to examine in detail the effects the systematic uncertainties have on the signal strength measurement. This is often referred to as calculating the \"impact\" of each uncertainty. What this means is to determine the shift in the signal strength, with respect to the best-fit, that is induced if a given nuisance parameter is shifted by its \\pm1\\sigma \\pm1\\sigma post-fit uncertainty values. If the signal strength shifts a lot, it tells us that it has a strong dependency on this systematic uncertainty. In fact, what we are measuring here is strongly related to the correlation coefficient between the signal strength and the nuisance parameter. The MultiDimFit method has an algorithm for calculating the impact for a given systematic: --algo impact -P [parameter name] , but it is typical to use a higher-level script, combineTool.py (part of the CombineHarvester package you checked out at the beginning) to automatically run the impacts for all parameters. Full documentation on this is given here . There is a three step process for running this. First we perform an initial fit for the signal strength and its uncertainty: combineTool.py -M Impacts -d workspace_part3.root -m 200 --rMin -1 --rMax 2 --robustFit 1 --doInitialFit Then we run the impacts for all the nuisance parameters: combineTool.py -M Impacts -d workspace_part3.root -m 200 --rMin -1 --rMax 2 --robustFit 1 --doFits This will take a little bit of time. When finished we collect all the output and convert it to a json file: combineTool.py -M Impacts -d workspace_part3.root -m 200 --rMin -1 --rMax 2 --robustFit 1 --output impacts.json We can then make a plot showing the pulls and parameter impacts, sorted by the largest impact: plotImpacts.py -i impacts.json -o impacts Tasks and questions: Identify the most important uncertainties using the impacts tool. In the plot, some parameters do not show a pull, but rather just a numerical value - why?","title":"B: Nuisance parameter impacts"},{"location":"part5/longexercise/#c-post-fit-distributions","text":"Another thing the FitDiagnostics mode can help us with is visualising the distributions we are fitting, and the uncertainties on those distributions, both before the fit is performed (\"pre-fit\") and after (\"post-fit\"). The pre-fit can give us some idea of how well our uncertainties cover any data-MC discrepancy, and the post-fit if discrepancies remain after the fit to data (as well as possibly letting us see the presence of a significant signal!). To produce these distributions add the --saveShapes and --saveWithUncertainties options when running FitDiagnostics : combine -M FitDiagnostics workspace_part3.root -m 200 --rMin -1 --rMax 2 --saveShapes --saveWithUncertainties Combine will produce pre- and post-fit distributions (for fit_s and fit_b) in the fitdiagnostics.root output file: Tasks and questions: Make a plot showing the expected background and signal contributions using the output from FitDiagnostics - do this for both the pre-fit and post-fit. You will find a script postFitPlot.py in the cms-das-stats repository that can help you get started. The bin errors on the TH1s in the fitdiagnostics file are determined from the systematic uncertainties. In the post-fit these take into account the additional constraints on the nuisance parameters as well as any correlations. Why is the uncertainty on the post-fit so much smaller than on the pre-fit?","title":"C: Post-fit distributions"},{"location":"part5/longexercise/#d-calculating-the-significance","text":"In the event that you observe a deviation from your null hypothesis, in this case the b-only hypothesis, combine can be used to calculate the p-value or significance. To do this using the asymptotic approximation simply do: combine -M Significance workspace_part3.root -m 200 --rMin -1 --rMax 2 To calculate the expected significance for a given signal strength we can just generate an Asimov dataset first: combine -M Significance workspace_part3.root -m 200 --rMin -1 --rMax 5 -t -1 --expectSignal 1.5 Note that the Asimov dataset generated this way uses the nominal values of all model parameters to define the dataset. Another option is to add --toysFrequentist , which causes a fit to the data to be performed first (with r frozen to the --expectSignal value) and then any subsequent Asimov datasets or toys are generated using the post-fit values of the model parameters. In general this will result in a different value for the expected significance due to changes in the background normalisation and shape induced by the fit to data: combine -M Significance workspace_part3.root -m 200 --rMin -1 --rMax 5 -t -1 --expectSignal 1.5 --toysFrequentist Tasks and questions: Note how much the expected significance changes with the --toysFrequentist option. Does the change make sense given the difference in the post-fit and pre-fit distributions you looked at in the previous section? Advanced task It is also possible to calculate the significance using toys with HybridNew (details here ) if we are in a situation where the asymptotic approximation is not reliable or if we just want to verify the result. Why might this be challenging for a high significance, say larger than 5\\sigma 5\\sigma ?","title":"D: Calculating the significance"},{"location":"part5/longexercise/#e-signal-strength-measurement-and-uncertainty-breakdown","text":"We have seen that with FitDiagnostics we can make a measurement of the best-fit signal strength and uncertainty. In the asymptotic approximation we find an interval at the \\alpha \\alpha CL around the best fit by identifying the parameter values at which our test statistic q=\u22122\\Delta \\ln L q=\u22122\\Delta \\ln L equals a critical value. This value is the \\alpha \\alpha quantile of the \\chi^2 \\chi^2 distribution with one degree of freedom. In the expression for q we calculate the difference in the profile likelihood between some fixed point and the best-fit. Depending on what we want to do with the measurement, e.g. whether it will be published in a journal, we may want to choose a more precise method for finding these intervals. There are a number of ways that parameter uncertainties are estimated in combine, and some are more precise than others: Covariance matrix: calculated by the Minuit HESSE routine, this gives a symmetric uncertainty by definition and is only accurate when the profile likelihood for this parameter is symmetric and parabolic. Minos error: calculated by the Minuit MINOS route - performs a search for the upper and lower values of the parameter that give the critical value of q q for the desired CL. Return an asymmetric interval. This is what FitDiagnostics does by default, but only for the parameter of interest. Usually accurate but prone to fail on more complex models and not easy to control the tolerance for terminating the search. RobustFit error: a custom implementation in combine similar to Minos that returns an asymmetric interval, but with more control over the precision. Enabled by adding --robustFit 1 when running FitDiagnostics . Explicit scan of the profile likelihood on a chosen grid of parameter values. Interpolation between points to find parameter values corresponding to appropriate d. It is a good idea to use this for important measurements since we can see by eye that there are no unexpected features in the shape of the likelihood curve. In this section we will look at the last approach, using the MultiDimFit mode of combine. By default this mode just performs a single fit to the data: combine -M MultiDimFit workspace_part3.root -n .part3E -m 200 --rMin -1 --rMax 2 You should see the best-fit value of the signal strength reported and nothing else. By adding the --algo X option combine will run an additional algorithm after this best fit. Here we will use --algo grid , which performs a scan of the likelihood with r fixed to a set of different values. The set of points will be equally spaced between the --rMin and --rMax values, and the number of points is controlled with --points N : combine -M MultiDimFit workspace_part3.root -n .part3E -m 200 --rMin -1 --rMax 2 --algo grid --points 30 The results of the scan are written into the output file, if opened interactively should see: Show output root [1] limit->Scan(\"r:deltaNLL\") ************************************ * Row * r * deltaNLL * ************************************ * 0 * 0.5007491 * 0 * * 1 * -0.949999 * 5.0564951 * * 2 * -0.850000 * 4.4238934 * * 3 * -0.75 * 3.8231189 * * 4 * -0.649999 * 3.2575576 * * 5 * -0.550000 * 2.7291250 * * 6 * -0.449999 * 2.2421045 * * 7 * -0.349999 * 1.7985963 * * 8 * -0.25 * 1.4009944 * * 9 * -0.150000 * 1.0515967 * * 10 * -0.050000 * 0.7510029 * * 11 * 0.0500000 * 0.5008214 * * 12 * 0.1500000 * 0.3021477 * * 13 * 0.25 * 0.1533777 * * 14 * 0.3499999 * 0.0552866 * * 15 * 0.4499999 * 0.0067232 * * 16 * 0.5500000 * 0.0062941 * * 17 * 0.6499999 * 0.0515425 * * 18 * 0.75 * 0.1421113 * * 19 * 0.8500000 * 0.2767190 * * 20 * 0.9499999 * 0.4476667 * * 21 * 1.0499999 * 0.6578899 * * 22 * 1.1499999 * 0.9042779 * * 23 * 1.25 * 1.1839065 * * 24 * 1.3500000 * 1.4943070 * * 25 * 1.4500000 * 1.8335367 * * 26 * 1.5499999 * 2.1992974 * * 27 * 1.6499999 * 2.5904724 * * 28 * 1.75 * 3.0033872 * * 29 * 1.8500000 * 3.4358899 * * 30 * 1.9500000 * 3.8883462 * ************************************ To turn this into a plot run: python plot1DScan.py higgsCombine.part3E.MultiDimFit.mH200.root -o single_scan This script will also perform a spline interpolation of the points to give accurate values for the uncertainties. In the next step we will split this total uncertainty into two components. It is typical to separate the contribution from statistics and systematics, and sometimes even split the systematic part into different components. This gives us an idea of which aspects of the uncertainty dominate. The statistical component is usually defined as the uncertainty we would have if all the systematic uncertainties went to zero. We can emulate this effect by freezing all the nuisance parameters when we do the scan in r , such that they do not vary in the fit. This is achieved by adding the --freezeParameters allConstrainedNuisances option. It would also work if the parameters are specified explicitly, e.g. --freezeParameters CMS_eff_t,lumi_13TeV,..., but the allConstrainedNuisances option is more concise. Run the scan again with the systematics frozen, and use the plotting script to overlay this curve with the previous one: combine -M MultiDimFit workspace_part3.root -n .part3E.freezeAll -m 200 --rMin -1 --rMax 2 --algo grid --points 30 --freezeParameters allConstrainedNuisances python plot1DScan.py higgsCombine.part3E.MultiDimFit.mH200.root --others 'higgsCombine.part3E.freezeAll.MultiDimFit.mH200.root:FreezeAll:2' -o freeze_first_attempt This doesn't look quite right - the best-fit has been shifted because unfortunately the --freezeParameters option acts before the initial fit, whereas we only want to add it for the scan after this fit. To remedy this we can use a feature of combine that lets us save a \"snapshot\" of the best-fit parameter values, and reuse this snapshot in subsequent fits. First we perform a single fit, adding the --saveWorkspace option: combine -M MultiDimFit workspace_part3.root -n .part3E.snapshot -m 200 --rMin -1 --rMax 2 --saveWorkspace The output file will now contain a copy of our workspace from the input, and this copy will contain a snapshot of the best-fit parameter values. We can now run the frozen scan again, but instead using this copy of the workspace as input, and restoring the snapshot that was saved: combine -M MultiDimFit higgsCombine.part3E.snapshot.MultiDimFit.mH200.root -n .part3E.freezeAll -m 200 --rMin -1 --rMax 2 --algo grid --points 30 --freezeParameters allConstrainedNuisances --snapshotName MultiDimFit python plot1DScan.py higgsCombine.part3E.MultiDimFit.mH200.root --others 'higgsCombine.part3E.freezeAll.MultiDimFit.mH200.root:FreezeAll:2' -o freeze_second_attempt --breakdown Syst,Stat Now the plot should look correct: We added the --breakdown Syst,Stat option to the plotting script to make it calculate the systematic component, which is defined simply as $\\sigma_{\\text{syst}} = \\sqrt{\\sigma^2_{\\text{tot}} - \\sigma^2_{\\text{stat}}}. To split the systematic uncertainty into different components we just need to run another scan with a subset of the systematics frozen. For example, say we want to split this into experimental and theoretical uncertainties, we would calculate the uncertainties as: \\sigma_{\\text{theory}} = \\sqrt{\\sigma^2_{\\text{tot}} - \\sigma^2_{\\text{fr.theory}}} \\sigma_{\\text{theory}} = \\sqrt{\\sigma^2_{\\text{tot}} - \\sigma^2_{\\text{fr.theory}}} \\sigma_{\\text{expt}} = \\sqrt{\\sigma^2_{\\text{fr.theory}} - \\sigma^2_{\\text{fr.theory+expt}}} \\sigma_{\\text{expt}} = \\sqrt{\\sigma^2_{\\text{fr.theory}} - \\sigma^2_{\\text{fr.theory+expt}}} \\sigma_{\\text{stat}} = \\sigma_{\\text{fr.theory+expt}} \\sigma_{\\text{stat}} = \\sigma_{\\text{fr.theory+expt}} where fr.=freeze. While it is perfectly fine to just list the relevant nuisance parameters in the --freezeParameters argument for the \\sigma_{\\text{fr.theory}} \\sigma_{\\text{fr.theory}} scan, a convenient way can be to define a named group of parameters in the text datacard and then freeze all parameters in this group with --freezeNuisanceGroups . The syntax for defining a group is: [group name] group = uncertainty_1 uncertainty_2 ... uncertainty_N Tasks and questions: Take our stat+syst split one step further and separate the systematic part into two: one part for hadronic tau uncertainties and one for all others. Do this by defining a tauID group in the datacard including the following parameters: CMS_eff_t , CMS_eff_t_highpt , and the three CMS_scale_t_X uncertainties. To plot this and calculate the split via the relations above you can just add further arguments to the --others option in the plot1DScan.py script. Each is of the form: '[file]:[label]:[color]' . The --breakdown argument should also be extended to three terms. How important are these tau-related uncertainties compared to the others?","title":"E: Signal strength measurement and uncertainty breakdown"},{"location":"part5/longexercise/#f-use-of-channel-masking","text":"We will now return briefly to the topic of blinding. We've seen that we can compute expected results by performing any combine method on an Asimov dataset generated using -t -1 . This is useful, because we can optimise our analysis without introducing any accidental bias that might come from looking at the data in the signal region. However our control regions have been chosen specifically to be signal-free, and it would be useful to use the data here to set the normalisation of our backgrounds even while the signal region remains blinded. Unfortunately there's no easy way to generate a partial Asimov dataset just for the signal region, but instead we can use a feature called \"channel masking\" to remove specific channels from the likelihood evaluation. One useful application of this feature is to make post-fit plots of the signal region from a control-region-only fit. To use the masking we first need to rerun text2workspace.py with an extra option that will create variables named like mask_[channel] in the workspace: text2workspace.py part3_combined.txt -m 200 -o workspace_part3_with_masks.root --channel-masks These parameters have a default value of 0 which means the channel is not masked. By setting it to 1 the channel is masked from the likelihood evaluation. Task: Run the same FitDiagnostics command as before to save the post-fit shapes, but add an option --setParameters mask_signal_region=1 . Note that the s+b fit will probably fail in this case, since we are no longer fitting a channel that contains signal, however the b-only fit should work fine. Task: Compare the expected background distribution and uncertainty to the pre-fit, and to the background distribution from the full fit you made before.","title":"F: Use of channel masking"},{"location":"part5/longexercise/#part-4-physics-models","text":"Topics covered in this section: A: Writing a simple physics model B: Performing and plotting 2D likelihood scans With combine we are not limited to parametrising the signal with a single scaling parameter r . In fact we can define any arbitrary scaling using whatever functions and parameters we would like. For example, when measuring the couplings of the Higgs boson to the different SM particles we would introduce a POI for each coupling parameter, for example \\kappa_{\\text{W}} \\kappa_{\\text{W}} , \\kappa_{\\text{Z}} \\kappa_{\\text{Z}} , \\kappa_{\\tau} \\kappa_{\\tau} etc. We would then generate scaling terms for each i\\rightarrow \\text{H}\\rightarrow j i\\rightarrow \\text{H}\\rightarrow j process in terms of how the cross section ( \\sigma_i(\\kappa) \\sigma_i(\\kappa) ) and branching ratio ( \\frac{\\Gamma_i(\\kappa)}{\\Gamma_{\\text{tot}}(\\kappa)} \\frac{\\Gamma_i(\\kappa)}{\\Gamma_{\\text{tot}}(\\kappa)} ) scale relative to the SM prediction. This parametrisation of the signal (and possibly backgrounds too) is specified in a physics model . This is a python class that is used by text2workspace.py to construct the model in terms of RooFit objects. There is documentation on using phyiscs models here .","title":"Part 4: Physics models"},{"location":"part5/longexercise/#a-writing-a-simple-physics-model","text":"An example physics model that just implements a single parameter r is given in DASModel.py : Show DASModel.py from HiggsAnalysis.CombinedLimit.PhysicsModel import * class DASModel(PhysicsModel): def doParametersOfInterest(self): \"\"\"Create POI and other parameters, and define the POI set.\"\"\" self.modelBuilder.doVar(\"r[0,0,10]\") self.modelBuilder.doSet(\"POI\", \",\".join(['r'])) def getYieldScale(self, bin, process): \"Return the name of a RooAbsReal to scale this yield by or the two special values 1 and 0 (don't scale, and set to zero)\" if self.DC.isSignal[process]: print 'Scaling %s/%s by r' % (bin, process) return \"r\" return 1 dasModel = DASModel() In this we override two methods of the basic PhysicsModel class: doParametersOfInterest and getYieldScale . In the first we define our POI variables, using the doVar function which accepts the RooWorkspace factory syntax for creating variables, and then define all our POIs in a set via the doSet function. The second function will be called for every process in every channel (bin), and using the corresponding strings we have to specify how that process should be scaled. Here we check if the process was declared as signal in the datacard, and if so scale it by r , otherwise if it is a background no scaling is applied ( 1 ). To use the physics model with text2workspace.py first copy it to the python directory in the combine package: cp DASModel.py $CMSSW_BASE/src/HiggsAnalysis/CombinedLimit/python/ In this section we will use the full datacards from the MSSM analysis. Have a look in part4/200/combined.txt . You will notice that there are now two signal processes declared: ggH and bbH . In the MSSM these cross sections can vary independently depending on the exact parameters of the model, so it is useful to be able to measure them independently too. First run text2workspace.py as follows, adding the -P option to specify the physics model, then verify the result of the fit: text2workspace.py part4/200/combined.txt -P HiggsAnalysis.CombinedLimit.DASModel:dasModel -m 200 -o workspace_part4.root combine -M MultiDimFit workspace_part4.root -n .part4A -m 200 --rMin 0 --rMax 2 Tasks and questions: Modify the physics model to scale the ggH and bbH processes by r_ggH and r_bbH separately. Then rerun the MultiDimFit command - you should see the result for both signal strengths printed.","title":"A: Writing a simple physics model"},{"location":"part5/longexercise/#b-performing-and-plotting-2d-likelihood-scans","text":"For a model with two POIs it is often useful to look at the how well we are able to measure both simultaneously. A natural extension of determining 1D confidence intervals on a single parameter like we did in part 3D is to determine confidence level regions in 2D. To do this we also use combine in a similar way, with -M MultiDimFit --algo grid . When two POIs are found combine will scan a 2D grid of points instead of a 1D array. Tasks and questions: Run a 2D likelihood scan in r_ggH and r_bbH . You can start with around 100 points but may need to increase this later too see more detail in the resulting plot. Have a look at the output limit tree, it should have branches for each POI as well as the usual deltaNLL value. You can use TTree::Draw to plot a 2D histogram of deltaNLL with r_ggH and r_bbH on the axes.","title":"B: Performing and plotting 2D likelihood scans"},{"location":"part5/longexerciseanswers/","text":"Answers to tasks and questions in long exercise Part 1: A one-bin counting experiment A: Computing limits using the asymptotic approximation Tasks and questions: There are some important uncertainties missing from the datacard above. Add the uncertainty on the luminosity (name: lumi_13TeV ) which has a 2.5% effect on all processes (except the jetFakes , which are taken from data), and uncertainties on the inclusive cross sections of the Ztautau and ttbar processes (with names xsec_Ztautau and xsec_diboson ) which are 4% and 6% respectively. Try changing the values of some uncertainties (up or down, or removing them altogether) - how do the expected and observed limits change? Show answer *Larger uncertainties make the limits worse (ie, higher values of the limit); smaller uncertainties improve the limit (lower values of the limit).* Now try changing the number of observed events. The observed limit will naturally change, but the expected does too - why might this be? Show answer *This is because the expected limit relies on a background-only Asimov dataset that is created* ***after*** *a background-only fit to the data. By changing the observed the pulls on the NPs in this fit also change, and therefore so does the expected sensitivity.* Advanced section: B: Computing limits with toys Tasks and questions: In contrast to AsymptoticLimits , with HybridNew each limit comes with an uncertainty. What is the origin of this uncertainty? Show answer *The uncertainty is statistical, because the values of CLs+b and CLb come from counting the number of toys in the tails of the test statistic distributions.* How good is the agreement between the asymptotic and toy-based methods? Show answer *The agreement should be pretty good in this example, but will generally break down once we get to the level of 0-5 events.* Why does it take longer to calculate the lower expected quantiles (e.g. 0.025, 0.16)? Think about how the statistical uncertainty on the CLs value depends on CLs+b and CLb. Show answer *For this we need the definition of CLs = CLs+b / CLb. The 0.025 expected quantile is by definition where CLb = 0.025, so for a 95% CL limit we have CLs = 0.05, implying we are looking for the value of r where CLs+b = 0.00125. With 1000 s+b toys we would then only expect 1000 * 0.00125 = 1.25 toys in the tail region we have to integrate over. Contrast this to the median limit where 25 toys would be in this region. This means we have to generate a much larger numbers of toys to get the same statistical power.* Advanced exercises Tasks and questions: Is the asymptotic limit still a good approximation? Show answer *A \"good\" approximation is not well defined, but the difference is clearly larger here.* You might notice that the test statistic distributions are not smooth but rather have several \"bump\" structures? Where might this come from? Try reducing the size of the systematic uncertainties to make them more pronounced. Show answer *This bump structure comes from the discrete-ness of the Poisson sampling of the toy datasets. Systematic uncertainties then smear these bumps out, but without systematics we would see delta functions corresponding to the possible integer number of events that could be observed. Once we go to more typical multi-bin analyses with more events and systematic uncertainties these discrete-ness washes out very quickly.* Part 2: A shape-based analysis A: Setting up the datacard Only tasks, no questions in this section B: Running combine for a blind analysis Tasks and questions: Compare the expected limits calculated with --run expected and --run blind. Why are they different? Show answer *When using --run blind combine will create a background-only Asimov dataset without performing a fit to data first. With --run expected, the observed limit isn't shown, but the background-only Asimov dataset used for the limit calculation is still created after a background-only fit to the data* Calculate a blind limit by generating a background-only Asimov with the -t option instead of using the AsymptoticLimits specific options. You should find the observed limit is the same as the expected. Then see what happens if you inject a signal into the Asimov dataset using the --expectSignal [X] option. Show answer *You should see that with a signal injected the observed limit is worse (has a higher value) than the expected limit: for the expected limit the b-only Asimov dataset is still used, but the observed limit is now calculated on the signal + background Asimov dataset, with a signal at the specified cross section [X].* C: Using FitDiagnostics Tasks and questions: Which parameter has the largest pull? Which has the tightest constraint? Show answer `CMS_eff_t_highpt` *should have the largest pull (around 0.47)*, `norm_jetFakes` *has the tightest constraint (to 25% of the input uncertainty).* Should we be concerned when a parameter is more strongly constrained than the input uncertainty (i.e. \\frac{\\sigma}{\\sigma_I}<1.0 \\frac{\\sigma}{\\sigma_I}<1.0 )? Show answer *This is still a hot topic in CMS analyses today, and there isn't a right or wrong answer. Essentially we have to judge if our analysis should really be able to provide more information about this parameter than the external measurement that gave us the input uncertainty. So we would not expect to be able to constrain the luminosity uncertainty for example, but uncertainties specific to the analysis might legitimately be constrained.* D: MC statistical uncertainties Tasks and questions: Check how much the cross section measurement and uncertainties change using FitDiagnostics . Show answer *Without autoMCStats we find:* `Best fit r: -2.73273 -2.13428/+3.38185`*, with autoMCStats:* `Best fit r: -3.07825 -3.17742/+3.7087` It is also useful to check how the expected uncertainty changes using an Asimov dataset, say with r=10 injected. Show answer *Without autoMCStats we find:* `Best fit r: 9.99978 -4.85341/+6.56233`*, with autoMCStats:*`Best fit r: 9.99985 -5.24634/+6.98266` Advanced task: See what happens if the Poisson threshold is increased. Based on your results, what threshold would you recommend for this analysis? Show answer *At first the uncertainties increase, as the threshold increases, and at some point they stabilise. A Poisson threshold at 10 is probably reasonable for this analysis.* Part 3: Adding control regions A: Use of rateParams Tasks and questions: Run text2workspace.py on this combined card and then use FitDiagnostics on an Asimov dataset with r=1 to get the expected uncertainty. Suggested command line options: --rMin 0 --rMax 2 Show answer *As expected uncertainty you should get -0.42542/+0.458748* Using the RooFitResult in the fitdiagnostics.root file, check the post-fit value of the rateParams. To what level are the normalisations of the DY and ttbar processes constrained? Show answer *They are constrained to around 4-5%* To compare to the previous approach of fitting the SR only, with cross section and acceptance uncertainties restored, an additional card is provided: datacard_part3_nocrs.txt . Run the same fit on this card to verify the improvement of the SR+CR approach Show answer *The expected uncertainty is larger with only the SR: -0.463273/+0.499161 compared with -0.42542/+0.458748 in the SR+CR approach.* B: Nuisance parameter impacts Tasks and questions: Identify the most important uncertainties using the impacts tool. Show answer *The most important uncertainty is *`norm_jetFakes`*, followed by two MC statistical uncerainties* (`prop_binsignal_region_bin8` *and* `prop_binsignal_region_bin9`). In the plot, some parameters do not show a pull, but rather just a numerical value - why? Show answer *These are freely floating parameters (rate_ttbar and rate_Zll). They have no prior constraint (and so no pull) - we show the best-fit value + uncertainty directly.* C: Post-fit distributions Tasks and questions: The bin errors on the TH1s in the fitdiagnostics file are determined from the systematic uncertainties. In the post-fit these take into account the additional constraints on the nuisance parameters as well as any correlations. Why is the uncertainty on the post-fit so much smaller than on the pre-fit? Show answer *There are two effects at play here: the nuisance parameters get constrained, and there are anti-correlations between the parameters which also have the effect of reducing the total uncertainty. Note: the post-fit uncertainty could become larger when rateParams are present as they are not taken into account in the pre-fit uncertainty but do enter in the post-fit uncertainty.* D: Calculating the significance Tasks and questions: Advanced task It is also possible to calculate the significance using toys with HybridNew (details here ) if we are in a situation where the asymptotic approximation is not reliable or if we just want to verify the result. Why might this be challenging for a high significance, say larger than 5\\sigma 5\\sigma ? Show answer A significance of $5\\sigma$ corresponds to a p-value of around $3\\cdot 10^{-7}$ - so we need to populate the very tail of the test statistic distribution and this requires generating a large number of toys. E: Signal strength measurement and uncertainty breakdown Tasks and questions: Take our stat+syst split one step further and separate the systematic part into two: one part for hadronic tau uncertainties and one for all others. Do this by defining a tauID group in the datacard including the following parameters: CMS_eff_t , CMS_eff_t_highpt , and the three CMS_scale_t_X uncertainties. Show datacard line You should add this line to the end of the datacard: tauID group = CMS_eff_t CMS_eff_t_highpt CMS_scale_t_1prong0pi0_13TeV CMS_scale_t_1prong1pi0_13TeV CMS_scale_t_3prong0pi0_13TeV To plot this and calculate the split via the relations above you can just add further arguments to the --others option in the plot1DScan.py script. Each is of the form: '[file]:[label]:[color]' . The --breakdown argument should also be extended to three terms. Show code This can be done as: python plot1DScan.py higgsCombine.part3E.MultiDimFit.mH200.root --others 'higgsCombine.part3E.freezeTauID.MultiDimFit.mH200.root:FreezeTauID:4' 'higgsCombine.part3E.freezeAll.MultiDimFit.mH200.root:FreezeAll:2' -o freeze_third_attempt --breakdown TauID,OtherSyst,Stat How important are these tau-related uncertainties compared to the others? Show answer *They are smaller than both the statistical uncertainty and the remaining systematic uncertainties* F: Use of channel masking No specific questions, just tasks","title":"Solutions to long exercise"},{"location":"part5/longexerciseanswers/#answers-to-tasks-and-questions-in-long-exercise","text":"","title":"Answers to tasks and questions in long exercise"},{"location":"part5/longexerciseanswers/#part-1-a-one-bin-counting-experiment","text":"","title":"Part 1: A one-bin counting experiment"},{"location":"part5/longexerciseanswers/#a-computing-limits-using-the-asymptotic-approximation","text":"Tasks and questions: There are some important uncertainties missing from the datacard above. Add the uncertainty on the luminosity (name: lumi_13TeV ) which has a 2.5% effect on all processes (except the jetFakes , which are taken from data), and uncertainties on the inclusive cross sections of the Ztautau and ttbar processes (with names xsec_Ztautau and xsec_diboson ) which are 4% and 6% respectively. Try changing the values of some uncertainties (up or down, or removing them altogether) - how do the expected and observed limits change? Show answer *Larger uncertainties make the limits worse (ie, higher values of the limit); smaller uncertainties improve the limit (lower values of the limit).* Now try changing the number of observed events. The observed limit will naturally change, but the expected does too - why might this be? Show answer *This is because the expected limit relies on a background-only Asimov dataset that is created* ***after*** *a background-only fit to the data. By changing the observed the pulls on the NPs in this fit also change, and therefore so does the expected sensitivity.*","title":"A: Computing limits using the asymptotic approximation"},{"location":"part5/longexerciseanswers/#advanced-section-b-computing-limits-with-toys","text":"Tasks and questions: In contrast to AsymptoticLimits , with HybridNew each limit comes with an uncertainty. What is the origin of this uncertainty? Show answer *The uncertainty is statistical, because the values of CLs+b and CLb come from counting the number of toys in the tails of the test statistic distributions.* How good is the agreement between the asymptotic and toy-based methods? Show answer *The agreement should be pretty good in this example, but will generally break down once we get to the level of 0-5 events.* Why does it take longer to calculate the lower expected quantiles (e.g. 0.025, 0.16)? Think about how the statistical uncertainty on the CLs value depends on CLs+b and CLb. Show answer *For this we need the definition of CLs = CLs+b / CLb. The 0.025 expected quantile is by definition where CLb = 0.025, so for a 95% CL limit we have CLs = 0.05, implying we are looking for the value of r where CLs+b = 0.00125. With 1000 s+b toys we would then only expect 1000 * 0.00125 = 1.25 toys in the tail region we have to integrate over. Contrast this to the median limit where 25 toys would be in this region. This means we have to generate a much larger numbers of toys to get the same statistical power.*","title":"Advanced section: B: Computing limits with toys"},{"location":"part5/longexerciseanswers/#advanced-exercises","text":"Tasks and questions: Is the asymptotic limit still a good approximation? Show answer *A \"good\" approximation is not well defined, but the difference is clearly larger here.* You might notice that the test statistic distributions are not smooth but rather have several \"bump\" structures? Where might this come from? Try reducing the size of the systematic uncertainties to make them more pronounced. Show answer *This bump structure comes from the discrete-ness of the Poisson sampling of the toy datasets. Systematic uncertainties then smear these bumps out, but without systematics we would see delta functions corresponding to the possible integer number of events that could be observed. Once we go to more typical multi-bin analyses with more events and systematic uncertainties these discrete-ness washes out very quickly.*","title":"Advanced exercises"},{"location":"part5/longexerciseanswers/#part-2-a-shape-based-analysis","text":"","title":"Part 2: A shape-based analysis"},{"location":"part5/longexerciseanswers/#a-setting-up-the-datacard","text":"Only tasks, no questions in this section","title":"A: Setting up the datacard"},{"location":"part5/longexerciseanswers/#b-running-combine-for-a-blind-analysis","text":"Tasks and questions: Compare the expected limits calculated with --run expected and --run blind. Why are they different? Show answer *When using --run blind combine will create a background-only Asimov dataset without performing a fit to data first. With --run expected, the observed limit isn't shown, but the background-only Asimov dataset used for the limit calculation is still created after a background-only fit to the data* Calculate a blind limit by generating a background-only Asimov with the -t option instead of using the AsymptoticLimits specific options. You should find the observed limit is the same as the expected. Then see what happens if you inject a signal into the Asimov dataset using the --expectSignal [X] option. Show answer *You should see that with a signal injected the observed limit is worse (has a higher value) than the expected limit: for the expected limit the b-only Asimov dataset is still used, but the observed limit is now calculated on the signal + background Asimov dataset, with a signal at the specified cross section [X].*","title":"B: Running combine for a blind analysis"},{"location":"part5/longexerciseanswers/#c-using-fitdiagnostics","text":"Tasks and questions: Which parameter has the largest pull? Which has the tightest constraint? Show answer `CMS_eff_t_highpt` *should have the largest pull (around 0.47)*, `norm_jetFakes` *has the tightest constraint (to 25% of the input uncertainty).* Should we be concerned when a parameter is more strongly constrained than the input uncertainty (i.e. \\frac{\\sigma}{\\sigma_I}<1.0 \\frac{\\sigma}{\\sigma_I}<1.0 )? Show answer *This is still a hot topic in CMS analyses today, and there isn't a right or wrong answer. Essentially we have to judge if our analysis should really be able to provide more information about this parameter than the external measurement that gave us the input uncertainty. So we would not expect to be able to constrain the luminosity uncertainty for example, but uncertainties specific to the analysis might legitimately be constrained.*","title":"C: Using FitDiagnostics"},{"location":"part5/longexerciseanswers/#d-mc-statistical-uncertainties","text":"Tasks and questions: Check how much the cross section measurement and uncertainties change using FitDiagnostics . Show answer *Without autoMCStats we find:* `Best fit r: -2.73273 -2.13428/+3.38185`*, with autoMCStats:* `Best fit r: -3.07825 -3.17742/+3.7087` It is also useful to check how the expected uncertainty changes using an Asimov dataset, say with r=10 injected. Show answer *Without autoMCStats we find:* `Best fit r: 9.99978 -4.85341/+6.56233`*, with autoMCStats:*`Best fit r: 9.99985 -5.24634/+6.98266` Advanced task: See what happens if the Poisson threshold is increased. Based on your results, what threshold would you recommend for this analysis? Show answer *At first the uncertainties increase, as the threshold increases, and at some point they stabilise. A Poisson threshold at 10 is probably reasonable for this analysis.*","title":"D: MC statistical uncertainties"},{"location":"part5/longexerciseanswers/#part-3-adding-control-regions","text":"","title":"Part 3: Adding control regions"},{"location":"part5/longexerciseanswers/#a-use-of-rateparams","text":"Tasks and questions: Run text2workspace.py on this combined card and then use FitDiagnostics on an Asimov dataset with r=1 to get the expected uncertainty. Suggested command line options: --rMin 0 --rMax 2 Show answer *As expected uncertainty you should get -0.42542/+0.458748* Using the RooFitResult in the fitdiagnostics.root file, check the post-fit value of the rateParams. To what level are the normalisations of the DY and ttbar processes constrained? Show answer *They are constrained to around 4-5%* To compare to the previous approach of fitting the SR only, with cross section and acceptance uncertainties restored, an additional card is provided: datacard_part3_nocrs.txt . Run the same fit on this card to verify the improvement of the SR+CR approach Show answer *The expected uncertainty is larger with only the SR: -0.463273/+0.499161 compared with -0.42542/+0.458748 in the SR+CR approach.*","title":"A: Use of rateParams"},{"location":"part5/longexerciseanswers/#b-nuisance-parameter-impacts","text":"Tasks and questions: Identify the most important uncertainties using the impacts tool. Show answer *The most important uncertainty is *`norm_jetFakes`*, followed by two MC statistical uncerainties* (`prop_binsignal_region_bin8` *and* `prop_binsignal_region_bin9`). In the plot, some parameters do not show a pull, but rather just a numerical value - why? Show answer *These are freely floating parameters (rate_ttbar and rate_Zll). They have no prior constraint (and so no pull) - we show the best-fit value + uncertainty directly.*","title":"B: Nuisance parameter impacts"},{"location":"part5/longexerciseanswers/#c-post-fit-distributions","text":"Tasks and questions: The bin errors on the TH1s in the fitdiagnostics file are determined from the systematic uncertainties. In the post-fit these take into account the additional constraints on the nuisance parameters as well as any correlations. Why is the uncertainty on the post-fit so much smaller than on the pre-fit? Show answer *There are two effects at play here: the nuisance parameters get constrained, and there are anti-correlations between the parameters which also have the effect of reducing the total uncertainty. Note: the post-fit uncertainty could become larger when rateParams are present as they are not taken into account in the pre-fit uncertainty but do enter in the post-fit uncertainty.*","title":"C: Post-fit distributions"},{"location":"part5/longexerciseanswers/#d-calculating-the-significance","text":"Tasks and questions: Advanced task It is also possible to calculate the significance using toys with HybridNew (details here ) if we are in a situation where the asymptotic approximation is not reliable or if we just want to verify the result. Why might this be challenging for a high significance, say larger than 5\\sigma 5\\sigma ? Show answer A significance of $5\\sigma$ corresponds to a p-value of around $3\\cdot 10^{-7}$ - so we need to populate the very tail of the test statistic distribution and this requires generating a large number of toys.","title":"D: Calculating the significance"},{"location":"part5/longexerciseanswers/#e-signal-strength-measurement-and-uncertainty-breakdown","text":"Tasks and questions: Take our stat+syst split one step further and separate the systematic part into two: one part for hadronic tau uncertainties and one for all others. Do this by defining a tauID group in the datacard including the following parameters: CMS_eff_t , CMS_eff_t_highpt , and the three CMS_scale_t_X uncertainties. Show datacard line You should add this line to the end of the datacard: tauID group = CMS_eff_t CMS_eff_t_highpt CMS_scale_t_1prong0pi0_13TeV CMS_scale_t_1prong1pi0_13TeV CMS_scale_t_3prong0pi0_13TeV To plot this and calculate the split via the relations above you can just add further arguments to the --others option in the plot1DScan.py script. Each is of the form: '[file]:[label]:[color]' . The --breakdown argument should also be extended to three terms. Show code This can be done as: python plot1DScan.py higgsCombine.part3E.MultiDimFit.mH200.root --others 'higgsCombine.part3E.freezeTauID.MultiDimFit.mH200.root:FreezeTauID:4' 'higgsCombine.part3E.freezeAll.MultiDimFit.mH200.root:FreezeAll:2' -o freeze_third_attempt --breakdown TauID,OtherSyst,Stat How important are these tau-related uncertainties compared to the others? Show answer *They are smaller than both the statistical uncertainty and the remaining systematic uncertainties*","title":"E: Signal strength measurement and uncertainty breakdown"},{"location":"part5/longexerciseanswers/#f-use-of-channel-masking","text":"No specific questions, just tasks","title":"F: Use of channel masking"},{"location":"part5/roofit/","text":"RooFit RooFit is a OO analysis environment built on ROOT . It has a collection of classes designed to augment root for data modeling. This section covers a few of the basics of RooFit . There are many more tutorials available at this link: https://root.cern.ch/root/html600/tutorials/roofit/index.html Objects In Roofit, any variable, data point, function, PDF (etc.) is represented by a c++ object The most basic of these is the RooRealVar . Let's create one which will represent the mass of some hypothetical particle, we name it and give it an initial starting value and range. RooRealVar MH(\"MH\",\"mass of the Hypothetical Boson (H-boson) in GeV\",125,120,130); MH.Print(); RooRealVar::MH = 125 L(120 - 130) ok, great. This variable is now an object we can play around with. We can access this object and modify it's properties, such as its value. MH.setVal(130); MH.getVal(); In particle detectors we typically don't observe this particle mass but usually define some observable which is sensitive to this mass. Lets assume we can detect and reconstruct the decay products of the H-boson and measure the invariant mass of those particles. We need to make another variable which represents that invariant mass. RooRealVar mass(\"m\",\"m (GeV)\",100,80,200); In the perfect world we would perfectly measure the exact mass of the particle in every single event. However, our detectors are usually far from perfect so there will be some resolution effect. Lets assume the resolution of our measurement of the invariant mass is 10 GeV and call it \"sigma\" RooRealVar sigma(\"resolution\",\"#sigma\",10,0,20); More exotic variables can be constructed out of these RooRealVar s using RooFormulaVars . For example, suppose we wanted to make a function out of the variables which represented the relative resolution as a function of the hypothetical mass MH. RooFormulaVar func(\"R\",\"@0/@1\",RooArgList(sigma,mass)); func.Print(\"v\"); Show --- RooAbsArg --- Value State: DIRTY Shape State: DIRTY Attributes: Address: 0x10e878068 Clients: Servers: (0x10dcd47b0,V-) RooRealVar::resolution \"#sigma\" (0x10dcd4278,V-) RooRealVar::m \"m (GeV)\" Proxies: actualVars -> 1) resolution 2) m --- RooAbsReal --- Plot label is \"R\" --- RooFormula --- Formula: \"@0/@1\" (resolution,m) Notice how there is a list of the variables we passed (the servers or \"actual vars\"). We can now plot the function. RooFit has a special plotting object RooPlot which keeps track of the objects (and their normalisations) which we want to draw. Since RooFit doesn't know the difference between which objects are/aren't dependant, we need to tell it. Right now, we have the relative resolution as R(m,\\sigma) R(m,\\sigma) , whereas we want to plot R(m,\\sigma(m)) R(m,\\sigma(m)) ! TCanvas *can = new TCanvas(); //make the x-axis the \"mass\" RooPlot *plot = mass.frame(); func.plotOn(plot); plot->Draw(); can->Draw(); The main objects we are interested in using from RooFit are probability denisty functions or (PDFs). We can construct the PDF, f(m|M_{H},\\sigma) f(m|M_{H},\\sigma) as a simple Gaussian shape for example or a RooGaussian in RooFit language (think McDonald's logic, everything is a RooSomethingOrOther ) RooGaussian gauss(\"gauss\",\"f(m|M_{H},#sigma)\",mass,MH,sigma); gauss.Print(\"V\"); Show --- RooAbsArg --- Value State: DIRTY Shape State: DIRTY Attributes: Address: 0x10ecf4188 Clients: Servers: (0x10dcd4278,V-) RooRealVar::m \"m (GeV)\" (0x10a08a9d8,V-) RooRealVar::MH \"mass of the Hypothetical Boson (H-boson) in GeV\" (0x10dcd47b0,V-) RooRealVar::resolution \"#sigma\" Proxies: x -> m mean -> MH sigma -> resolution --- RooAbsReal --- Plot label is \"gauss\" --- RooAbsPdf --- Cached value = 0 Notice how the gaussian PDF, like the RooFormulaVar depends on our RooRealVar objects, these are its servers. Its evaluation will depend on their values. The main difference between PDFs and Functions in RooFit is that PDFs are automatically normalised to unitiy , hence they represent a probability density, you don't need to normalise yourself. Lets plot it for the different values of m m . plot = mass.frame(); gauss.plotOn(plot); MH.setVal(120); gauss.plotOn(plot,RooFit::LineColor(kBlue)); MH.setVal(125); gauss.plotOn(plot,RooFit::LineColor(kRed)); MH.setVal(135); gauss.plotOn(plot,RooFit::LineColor(kGreen)); plot->Draw(); can->Update(); can->Draw(); Note that as we change the value of MH , the PDF gets updated at the same time. PDFs can be used to generate Monte Carlo data. One of the benefits of RooFit is that to do so only uses a single line of code! As before, we have to tell RooFit which variables to generate in (e.g which are the observables for an experiment). In this case, each of our events will be a single value of \"mass\" m m . The arguments for the function are the set of observables, follwed by the number of events, RooDataSet *data = (RooDataSet*) gauss.generate(RooArgSet(mass),500); Now we can plot the data as with other RooFit objects. plot = mass.frame(); data->plotOn(plot); gauss.plotOn(plot); gauss.paramOn(plot); plot->Draw(); can->Update(); can->Draw(); Of course we're not in the business of generating MC events, but collecting real data! . Next we will look at using real data in RooFit . Datasets A dataset is essentially just a collection of points in N-dimensional (N-observables) space. There are two basic implementations in RooFit, 1) an \"unbinned\" dataset - RooDataSet 2) a \"binned\" dataset - RooDataHist both of these use the same basic structure as below Lets create an empty dataset where the only observable, the mass. Points can be added to the dataset one by one ... RooDataSet mydata(\"dummy\",\"My dummy dataset\",RooArgSet(mass)); // We've made a dataset with one observable (mass) mass.setVal(123.4); mydata.add(RooArgSet(mass)); mass.setVal(145.2); mydata.add(RooArgSet(mass)); mass.setVal(170.8); mydata.add(RooArgSet(mass)); mydata.Print(); RooDataSet::dummy[m] = 3 entries There are also other ways to manipulate datasets in this way as shown in the diagram below Luckily there are also Constructors for a RooDataSet from a TTree and for a RooDataHist from a TH1 so its simple to convert from your usual ROOT objects. Let's take an example dataset put together already. TFile *file = TFile::Open(\"tutorial.root\"); file->ls(); Show file contents TFile** tutorial.root TFile* tutorial.root KEY: RooWorkspace workspace;1 Tutorial Workspace KEY: TProcessID ProcessID0;1 48737500-e7e5-11e6-be6f-0d0011acbeef Inside the file, there is something called a RooWorkspace . This is just the RooFit way of keeping a persistent link between the objects for a model. It is a very useful way to share data and PDFs/functions etc among CMS collaborators. Let's take a look at it. It contains a RooDataSet and one variable. This time we called our variable (or observable) CMS_hgg_mass , let's assume now that this is the invariant mass of photon pairs where we assume our H-boson decays to photons. RooWorkspace *wspace = (RooWorkspace*) file->Get(\"workspace\"); wspace->Print(\"v\"); Show RooWorkspace(workspace) Tutorial Workspace contents variables --------- (CMS_hgg_mass) datasets -------- RooDataSet::dataset(CMS_hgg_mass) Let's have a look at the data. The RooWorkspace has several accessor functions, we will use the RooWorkspace::data one. There are also RooWorkspace::var , RooWorkspace::function and RooWorkspace::pdf with (hopefully) obvious purposes. RooDataSet *hgg_data = (RooDataSereat*) wspace->data(\"dataset\"); RooRealVar *hgg_mass = (RooRealVar*) wspace->var(\"CMS_hgg_mass\"); plot = hgg_mass->frame(); hgg_data->plotOn(plot,RooFit::Binning(160)); // Here we've picked a certain number of bins just for plotting purposes plot->Draw(); can->Update(); can->Draw(); Likelihoods and Fitting to data The data we have in our file doesn't look like a Gaussian distribution. Instead, we could probably use something like an exponential to describe it. There is an exponential PDF already in RooFit (yep you guessed it) RooExponential . For a pdf, we only need one parameter which is the exponential slope \\alpha \\alpha so our pdf is, f(m|\\alpha) = \\dfrac{1}{N} e^{-\\alpha m} f(m|\\alpha) = \\dfrac{1}{N} e^{-\\alpha m} Where of course, N = \\int_{110}^{150} e^{-\\alpha m} dm N = \\int_{110}^{150} e^{-\\alpha m} dm is the normalisation constant. You can fund a bunch of available RooFit functions here: https://root.cern.ch/root/html/ROOFIT_ROOFIT_Index.html There is also support for a generic pdf in the form of a RooGenericPdf , check this link: https://root.cern.ch/doc/v608/classRooGenericPdf.html Let's create an exponential PDF for our background, RooRealVar alpha(\"alpha\",\"#alpha\",-0.05,-0.2,0.01); RooExponential expo(\"exp\",\"exponential function\",*hgg_mass,alpha); We can use RooFit to tell us to estimate the value of \\alpha \\alpha using this dataset. You will learn more about parameter estimation but for now we will just assume you know about maximising likelihoods. This maximum likelihood estimator is common in HEP and is known to give unbiased estimates for things like distribution means etc. This also introduces the other main use of PDFs in RooFit. They can be used to construct likelihoods easily. The likelihood \\mathcal{L} \\mathcal{L} is defined for a particluar dataset (and model) as being proportional to the probability to observe the data assuming some pdf. For our data, the probability to observe an event with a value in an interval bounded by a and b is given by, P\\left(m~\\epsilon~[a,b] \\right) = \\int_{a}^{b} f(m|\\alpha)dm P\\left(m~\\epsilon~[a,b] \\right) = \\int_{a}^{b} f(m|\\alpha)dm As that interval shrinks we can say this probability just becomes equal to f(m|\\alpha)dm f(m|\\alpha)dm . The probability to observe the dataset we have is given by the product of such probabilities for each of our data points, so that \\mathcal{L}(\\alpha) \\propto \\prod_{i} f(m_{i}|\\alpha) \\mathcal{L}(\\alpha) \\propto \\prod_{i} f(m_{i}|\\alpha) Note that for a specific dataset, the dm dm factors which should be there are constnant. They can therefore be absorbed into the constant of proportionality! The maximum likelihood esitmator for \\alpha \\alpha , usually written as \\hat{\\alpha} \\hat{\\alpha} , is found by maximising \\mathcal{L}(\\alpha) \\mathcal{L}(\\alpha) . Note that this won't depend on the value of the constant of proportionality so we can ignore it. This is true in most scenarios because usually only the ratio of likelihoods is needed, in which the constant factors out. Obviously this multiplication of exponentials can lead to very large (or very small) numbers which can lead to numerical instabilities. To avoid this, we can take logs of the likelihood. Its also common to multiply this by -1 and minimize the resulting N egative L og L ikelihood : \\mathrm{-Log}\\mathcal{L}(\\alpha) \\mathrm{-Log}\\mathcal{L}(\\alpha) . RooFit can construct the NLL for us. RooNLLVar *nll = (RooNLLVar*) expo.createNLL(*hgg_data); nll->Print(\"v\"); Show --- RooAbsArg --- Value State: DIRTY Shape State: DIRTY Attributes: Address: 0x7fdddbe46200 Clients: Servers: (0x11eab5638,V-) RooRealVar::alpha \"#alpha\" Proxies: paramSet -> 1) alpha --- RooAbsReal --- Plot label is \"nll_exp_dataset\" Notice that the NLL object knows which RooRealVar is the parameter because it doesn't find that one in the dataset. This is how RooFit distiguishes between observables and parameters . RooFit has an interface to Minuit via the RooMinimizer class which takes the NLL as an argument. To minimize, we just call the RooMinimizer::minimize() function. Minuit2 is the program and migrad is the minimization routine which uses gradient descent. RooMinimizer minim(*nll); minim.minimize(\"Minuit2\",\"migrad\"); Show ********** ** 1 **SET PRINT 1 ********** ********** ** 2 **SET NOGRAD ********** PARAMETER DEFINITIONS: NO. NAME VALUE STEP SIZE LIMITS 1 alpha -5.00000e-02 2.10000e-02 -2.00000e-01 1.00000e-02 ********** ** 3 **SET ERR 0.5 ********** ********** ** 4 **SET PRINT 1 ********** ********** ** 5 **SET STR 1 ********** NOW USING STRATEGY 1: TRY TO BALANCE SPEED AGAINST RELIABILITY ********** ** 6 **MIGRAD 500 1 ********** FIRST CALL TO USER FUNCTION AT NEW START POINT, WITH IFLAG=4. START MIGRAD MINIMIZATION. STRATEGY 1. CONVERGENCE WHEN EDM .LT. 1.00e-03 FCN=3589.52 FROM MIGRAD STATUS=INITIATE 4 CALLS 5 TOTAL EDM= unknown STRATEGY= 1 NO ERROR MATRIX EXT PARAMETER CURRENT GUESS STEP FIRST NO. NAME VALUE ERROR SIZE DERIVATIVE 1 alpha -5.00000e-02 2.10000e-02 2.24553e-01 -9.91191e+01 ERR DEF= 0.5 MIGRAD MINIMIZATION HAS CONVERGED. MIGRAD WILL VERIFY CONVERGENCE AND ERROR MATRIX. COVARIANCE MATRIX CALCULATED SUCCESSFULLY FCN=3584.68 FROM MIGRAD STATUS=CONVERGED 18 CALLS 19 TOTAL EDM=1.4449e-08 STRATEGY= 1 ERROR MATRIX ACCURATE EXT PARAMETER STEP FIRST NO. NAME VALUE ERROR SIZE DERIVATIVE 1 alpha -4.08262e-02 2.91959e-03 1.33905e-03 -3.70254e-03 ERR DEF= 0.5 EXTERNAL ERROR MATRIX. NDIM= 25 NPAR= 1 ERR DEF=0.5 8.527e-06 RooFit has found the best fit value of alpha for this dataset. It also estimates an uncertainty on alpha using the Hessian matrix from the fit. alpha.Print(\"v\"); --- RooAbsArg --- Value State: clean Shape State: clean Attributes: Address: 0x11eab5638 Clients: (0x11eab5978,V-) RooExponential::exp \"exponential function\" (0x7fdddbe46200,V-) RooNLLVar::nll_exp_dataset \"-log(likelihood)\" (0x7fdddbe95600,V-) RooExponential::exp \"exponential function\" (0x7fdddbe5a400,V-) RooRealIntegral::exp_Int[CMS_hgg_mass] \"Integral of exponential function\" Servers: Proxies: --- RooAbsReal --- Plot label is \"alpha\" --- RooAbsRealLValue --- Fit range is [ -0.2 , 0.01 ] --- RooRealVar --- Error = 0.00291959 Lets plot the resulting exponential on the data. Notice that the value of \\hat{\\alpha} \\hat{\\alpha} is used for the exponential. expo.plotOn(plot); expo.paramOn(plot); plot->Draw(); can->Update(); can->Draw(); It looks like there could be a small region near 125 GeV for which our fit doesn't quite go through the points. Maybe our hypothetical H-boson isn't so hypothetical after all! Let's see what happens if we include some resonant signal into the fit. We can take our Gaussian function again and use that as a signal model. A reasonable value for the resolution of a resonant signal with a mass around 125 GeV decaying to a pair of photons is around a GeV. sigma.setVal(1.); sigma.setConstant(); MH.setVal(125); MH.setConstant(); RooGaussian hgg_signal(\"signal\",\"Gaussian PDF\",*hgg_mass,MH,sigma); By setting these parameters constant, RooFit knows (either when creating the NLL by hand or when using fitTo ) that there is not need to fit for these parameters. We need to add this to our exponential model and fit a \"Sigmal+Background model\" by creating a RooAddPdf . In RooFit there are two ways to add PDFs, recursively where the fraction of yields for the signal and background is a parameter or absolutely where each PDF has its own normalisation. We're going to use the second one. RooRealVar norm_s(\"norm_s\",\"N_{s}\",10,100); RooRealVar norm_b(\"norm_b\",\"N_{b}\",0,1000); const RooArgList components(hgg_signal,expo); const RooArgList coeffs(norm_s,norm_b); RooAddPdf model(\"model\",\"f_{s+b}\",components,coeffs); model.Print(\"v\"); Show --- RooAbsArg --- Value State: DIRTY Shape State: DIRTY Attributes: Address: 0x11ed5d7a8 Clients: Servers: (0x11ed5a0f0,V-) RooGaussian::signal \"Gaussian PDF\" (0x11ed5d058,V-) RooRealVar::norm_s \"N_{s}\" (0x11eab5978,V-) RooExponential::exp \"exponential function\" (0x11ed5d398,V-) RooRealVar::norm_b \"N_{b}\" Proxies: !refCoefNorm -> !pdfs -> 1) signal 2) exp !coefficients -> 1) norm_s 2) norm_b --- RooAbsReal --- Plot label is \"model\" --- RooAbsPdf --- Cached value = 0 Ok now lets fit the model. Note this time we add the option Extended() which tells RooFit that we care about the overall number of observed events in the data n n too. It will add an additional Poisson term in the likelihood to account for this so our likelihood this time looks like, L_{s+b}(N_{s},N_{b},\\alpha) = \\dfrac{ N_{s}+N_{b}^{n} e^{N_{s}+N_{b}} }{n!} \\cdot \\prod_{i}^{n} \\left[ c f_{s}(m_{i}|M_{H},\\sigma)+ (1-c)f_{b}(m_{i}|\\alpha) \\right] L_{s+b}(N_{s},N_{b},\\alpha) = \\dfrac{ N_{s}+N_{b}^{n} e^{N_{s}+N_{b}} }{n!} \\cdot \\prod_{i}^{n} \\left[ c f_{s}(m_{i}|M_{H},\\sigma)+ (1-c)f_{b}(m_{i}|\\alpha) \\right] where c = \\dfrac{ N_{s} }{ N_{s} + N_{b} } c = \\dfrac{ N_{s} }{ N_{s} + N_{b} } , f_{s}(m|M_{H},\\sigma) f_{s}(m|M_{H},\\sigma) is the Gaussian signal pdf and f_{b}(m|\\alpha) f_{b}(m|\\alpha) is the exponential pdf. Remember that M_{H} M_{H} and \\sigma \\sigma are fixed so that they are no longer parameters of the likelihood. There is a simpler interface for maximum likelihood fits which is the RooAbsPdf::fitTo method. With this simple method, RooFit will construct the negative log-likelihood function, from the pdf, and minimize all of the free parameters in one step. model.fitTo(*hgg_data,RooFit::Extended()); model.plotOn(plot,RooFit::Components(\"exp\"),RooFit::LineColor(kGreen)); model.plotOn(plot,RooFit::LineColor(kRed)); model.paramOn(plot); can->Clear(); plot->Draw(); can->Update(); can->Draw(); What about if we also fit for the mass ( M_{H} M_{H} )? we can easily do this by removing the constant setting on MH. MH.setConstant(false); model.fitTo(*hgg_data,RooFit::Extended()); Show output [#1] INFO:Minization -- RooMinimizer::optimizeConst: activating const optimization [#1] INFO:Minization -- The following expressions will be evaluated in cache-and-track mode: (signal,exp) ********** ** 1 **SET PRINT 1 ********** ********** ** 2 **SET NOGRAD ********** PARAMETER DEFINITIONS: NO. NAME VALUE STEP SIZE LIMITS 1 MH 1.25000e+02 1.00000e+00 1.20000e+02 1.30000e+02 2 alpha -4.08793e-02 2.96856e-03 -2.00000e-01 1.00000e-02 3 norm_b 9.67647e+02 3.25747e+01 0.00000e+00 1.00000e+03 MINUIT WARNING IN PARAMETR ============== VARIABLE3 BROUGHT BACK INSIDE LIMITS. 4 norm_s 3.22534e+01 1.16433e+01 1.00000e+01 1.00000e+02 ********** ** 3 **SET ERR 0.5 ********** ********** ** 4 **SET PRINT 1 ********** ********** ** 5 **SET STR 1 ********** NOW USING STRATEGY 1: TRY TO BALANCE SPEED AGAINST RELIABILITY ********** ** 6 **MIGRAD 2000 1 ********** FIRST CALL TO USER FUNCTION AT NEW START POINT, WITH IFLAG=4. START MIGRAD MINIMIZATION. STRATEGY 1. CONVERGENCE WHEN EDM .LT. 1.00e-03 FCN=-2327.53 FROM MIGRAD STATUS=INITIATE 10 CALLS 11 TOTAL EDM= unknown STRATEGY= 1 NO ERROR MATRIX EXT PARAMETER CURRENT GUESS STEP FIRST NO. NAME VALUE ERROR SIZE DERIVATIVE 1 MH 1.25000e+02 1.00000e+00 2.01358e-01 1.12769e+01 2 alpha -4.08793e-02 2.96856e-03 3.30048e-02 -1.22651e-01 3 norm_b 9.67647e+02 3.25747e+01 2.56674e-01 -1.96463e-02 4 norm_s 3.22534e+01 1.16433e+01 3.10258e-01 -8.97036e-04 ERR DEF= 0.5 MIGRAD MINIMIZATION HAS CONVERGED. MIGRAD WILL VERIFY CONVERGENCE AND ERROR MATRIX. COVARIANCE MATRIX CALCULATED SUCCESSFULLY FCN=-2327.96 FROM MIGRAD STATUS=CONVERGED 65 CALLS 66 TOTAL EDM=1.19174e-05 STRATEGY= 1 ERROR MATRIX ACCURATE EXT PARAMETER STEP FIRST NO. NAME VALUE ERROR SIZE DERIVATIVE 1 MH 1.24628e+02 3.98153e-01 2.66539e-03 2.46327e-02 2 alpha -4.07708e-02 2.97195e-03 1.10093e-03 8.33780e-02 3 norm_b 9.66105e+02 3.25772e+01 5.96627e-03 1.83523e-03 4 norm_s 3.39026e+01 1.17380e+01 9.60816e-03 -2.32681e-03 ERR DEF= 0.5 EXTERNAL ERROR MATRIX. NDIM= 25 NPAR= 4 ERR DEF=0.5 1.589e-01 -3.890e-05 1.462e-01 -1.477e-01 -3.890e-05 8.836e-06 -2.020e-04 2.038e-04 1.462e-01 -2.020e-04 1.073e+03 -1.072e+02 -1.477e-01 2.038e-04 -1.072e+02 1.420e+02 PARAMETER CORRELATION COEFFICIENTS NO. GLOBAL 1 2 3 4 1 0.04518 1.000 -0.033 0.011 -0.031 2 0.03317 -0.033 1.000 -0.002 0.006 3 0.27465 0.011 -0.002 1.000 -0.275 4 0.27610 -0.031 0.006 -0.275 1.000 ********** ** 7 **SET ERR 0.5 ********** ********** ** 8 **SET PRINT 1 ********** ********** ** 9 **HESSE 2000 ********** COVARIANCE MATRIX CALCULATED SUCCESSFULLY FCN=-2327.96 FROM HESSE STATUS=OK 23 CALLS 89 TOTAL EDM=1.19078e-05 STRATEGY= 1 ERROR MATRIX ACCURATE EXT PARAMETER INTERNAL INTERNAL NO. NAME VALUE ERROR STEP SIZE VALUE 1 MH 1.24628e+02 3.98106e-01 5.33077e-04 -7.45154e-02 2 alpha -4.07708e-02 2.97195e-03 2.20186e-04 5.42722e-01 3 norm_b 9.66105e+02 3.26003e+01 2.38651e-04 1.20047e+00 4 norm_s 3.39026e+01 1.17445e+01 3.84326e-04 -4.87967e-01 ERR DEF= 0.5 EXTERNAL ERROR MATRIX. NDIM= 25 NPAR= 4 ERR DEF=0.5 1.588e-01 -3.888e-05 1.304e-01 -1.304e-01 -3.888e-05 8.836e-06 -1.954e-04 1.954e-04 1.304e-01 -1.954e-04 1.074e+03 -1.082e+02 -1.304e-01 1.954e-04 -1.082e+02 1.421e+02 PARAMETER CORRELATION COEFFICIENTS NO. GLOBAL 1 2 3 4 1 0.04274 1.000 -0.033 0.010 -0.027 2 0.03314 -0.033 1.000 -0.002 0.006 3 0.27694 0.010 -0.002 1.000 -0.277 4 0.27806 -0.027 0.006 -0.277 1.000 [#1] INFO:Minization -- RooMinimizer::optimizeConst: deactivating const optimization Notice the result for the fitted MH is not 125 and is included in the list of fitted parameters. We can get more information about the fit, via the RooFitResult , using the option Save() . RooFitResult *fit_res = (RooFitResult*) model.fitTo(*hgg_data,RooFit::Extended(),RooFit::Save()); For example, we can get the Correlation Matrix from the fit result... Note that the order of the parameters are the same as listed in the \"Floating Parameter\" list above TMatrixDSym cormat = fit_res->correlationMatrix(); cormat.Print(); 4x4 matrix is as follows | 0 | 1 | 2 | 3 | --------------------------------------------------------- 0 | 1 -0.03282 0.009538 -0.02623 1 | -0.03282 1 -0.001978 0.005439 2 | 0.009538 -0.001978 1 -0.2769 3 | -0.02623 0.005439 -0.2769 1 A nice feature of RooFit is that once we have a PDF, data and results like this, we can import this new model into our RooWorkspace and show off our new discovery to our LHC friends (if we weren't already too late!). We can also save the \"state\" of our parameters for later, by creating a snapshot of the current values. wspace->import(model); RooArgSet *params = model.getParameters(*hgg_data); wspace->saveSnapshot(\"nominal_values\",*params); wspace->Print(\"V\"); Show output RooWorkspace(workspace) Tutorial Workspace contents variables --------- (CMS_hgg_mass,MH,alpha,norm_b,norm_s,resolution) p.d.f.s ------- RooExponential::exp[ x=CMS_hgg_mass c=alpha ] = 0.00248636 RooAddPdf::model[ norm_s * signal + norm_b * exp ] = 0.00240205 RooGaussian::signal[ x=CMS_hgg_mass mean=MH sigma=resolution ] = 5.34013e-110 datasets -------- RooDataSet::dataset(CMS_hgg_mass) parameter snapshots ------------------- nominal_values = (MH=124.627 +/- 0.398094,resolution=1[C],norm_s=33.9097 +/- 11.7445,alpha=-0.040779 +/- 0.00297195,norm_b=966.109 +/- 32.6025) This is exactly what needs to be done when you want to use shape based datacards in combine with parametric models.","title":"RooFit Basics"},{"location":"part5/roofit/#roofit","text":"RooFit is a OO analysis environment built on ROOT . It has a collection of classes designed to augment root for data modeling. This section covers a few of the basics of RooFit . There are many more tutorials available at this link: https://root.cern.ch/root/html600/tutorials/roofit/index.html","title":"RooFit"},{"location":"part5/roofit/#objects","text":"In Roofit, any variable, data point, function, PDF (etc.) is represented by a c++ object The most basic of these is the RooRealVar . Let's create one which will represent the mass of some hypothetical particle, we name it and give it an initial starting value and range. RooRealVar MH(\"MH\",\"mass of the Hypothetical Boson (H-boson) in GeV\",125,120,130); MH.Print(); RooRealVar::MH = 125 L(120 - 130) ok, great. This variable is now an object we can play around with. We can access this object and modify it's properties, such as its value. MH.setVal(130); MH.getVal(); In particle detectors we typically don't observe this particle mass but usually define some observable which is sensitive to this mass. Lets assume we can detect and reconstruct the decay products of the H-boson and measure the invariant mass of those particles. We need to make another variable which represents that invariant mass. RooRealVar mass(\"m\",\"m (GeV)\",100,80,200); In the perfect world we would perfectly measure the exact mass of the particle in every single event. However, our detectors are usually far from perfect so there will be some resolution effect. Lets assume the resolution of our measurement of the invariant mass is 10 GeV and call it \"sigma\" RooRealVar sigma(\"resolution\",\"#sigma\",10,0,20); More exotic variables can be constructed out of these RooRealVar s using RooFormulaVars . For example, suppose we wanted to make a function out of the variables which represented the relative resolution as a function of the hypothetical mass MH. RooFormulaVar func(\"R\",\"@0/@1\",RooArgList(sigma,mass)); func.Print(\"v\"); Show --- RooAbsArg --- Value State: DIRTY Shape State: DIRTY Attributes: Address: 0x10e878068 Clients: Servers: (0x10dcd47b0,V-) RooRealVar::resolution \"#sigma\" (0x10dcd4278,V-) RooRealVar::m \"m (GeV)\" Proxies: actualVars -> 1) resolution 2) m --- RooAbsReal --- Plot label is \"R\" --- RooFormula --- Formula: \"@0/@1\" (resolution,m) Notice how there is a list of the variables we passed (the servers or \"actual vars\"). We can now plot the function. RooFit has a special plotting object RooPlot which keeps track of the objects (and their normalisations) which we want to draw. Since RooFit doesn't know the difference between which objects are/aren't dependant, we need to tell it. Right now, we have the relative resolution as R(m,\\sigma) R(m,\\sigma) , whereas we want to plot R(m,\\sigma(m)) R(m,\\sigma(m)) ! TCanvas *can = new TCanvas(); //make the x-axis the \"mass\" RooPlot *plot = mass.frame(); func.plotOn(plot); plot->Draw(); can->Draw(); The main objects we are interested in using from RooFit are probability denisty functions or (PDFs). We can construct the PDF, f(m|M_{H},\\sigma) f(m|M_{H},\\sigma) as a simple Gaussian shape for example or a RooGaussian in RooFit language (think McDonald's logic, everything is a RooSomethingOrOther ) RooGaussian gauss(\"gauss\",\"f(m|M_{H},#sigma)\",mass,MH,sigma); gauss.Print(\"V\"); Show --- RooAbsArg --- Value State: DIRTY Shape State: DIRTY Attributes: Address: 0x10ecf4188 Clients: Servers: (0x10dcd4278,V-) RooRealVar::m \"m (GeV)\" (0x10a08a9d8,V-) RooRealVar::MH \"mass of the Hypothetical Boson (H-boson) in GeV\" (0x10dcd47b0,V-) RooRealVar::resolution \"#sigma\" Proxies: x -> m mean -> MH sigma -> resolution --- RooAbsReal --- Plot label is \"gauss\" --- RooAbsPdf --- Cached value = 0 Notice how the gaussian PDF, like the RooFormulaVar depends on our RooRealVar objects, these are its servers. Its evaluation will depend on their values. The main difference between PDFs and Functions in RooFit is that PDFs are automatically normalised to unitiy , hence they represent a probability density, you don't need to normalise yourself. Lets plot it for the different values of m m . plot = mass.frame(); gauss.plotOn(plot); MH.setVal(120); gauss.plotOn(plot,RooFit::LineColor(kBlue)); MH.setVal(125); gauss.plotOn(plot,RooFit::LineColor(kRed)); MH.setVal(135); gauss.plotOn(plot,RooFit::LineColor(kGreen)); plot->Draw(); can->Update(); can->Draw(); Note that as we change the value of MH , the PDF gets updated at the same time. PDFs can be used to generate Monte Carlo data. One of the benefits of RooFit is that to do so only uses a single line of code! As before, we have to tell RooFit which variables to generate in (e.g which are the observables for an experiment). In this case, each of our events will be a single value of \"mass\" m m . The arguments for the function are the set of observables, follwed by the number of events, RooDataSet *data = (RooDataSet*) gauss.generate(RooArgSet(mass),500); Now we can plot the data as with other RooFit objects. plot = mass.frame(); data->plotOn(plot); gauss.plotOn(plot); gauss.paramOn(plot); plot->Draw(); can->Update(); can->Draw(); Of course we're not in the business of generating MC events, but collecting real data! . Next we will look at using real data in RooFit .","title":"Objects"},{"location":"part5/roofit/#datasets","text":"A dataset is essentially just a collection of points in N-dimensional (N-observables) space. There are two basic implementations in RooFit, 1) an \"unbinned\" dataset - RooDataSet 2) a \"binned\" dataset - RooDataHist both of these use the same basic structure as below Lets create an empty dataset where the only observable, the mass. Points can be added to the dataset one by one ... RooDataSet mydata(\"dummy\",\"My dummy dataset\",RooArgSet(mass)); // We've made a dataset with one observable (mass) mass.setVal(123.4); mydata.add(RooArgSet(mass)); mass.setVal(145.2); mydata.add(RooArgSet(mass)); mass.setVal(170.8); mydata.add(RooArgSet(mass)); mydata.Print(); RooDataSet::dummy[m] = 3 entries There are also other ways to manipulate datasets in this way as shown in the diagram below Luckily there are also Constructors for a RooDataSet from a TTree and for a RooDataHist from a TH1 so its simple to convert from your usual ROOT objects. Let's take an example dataset put together already. TFile *file = TFile::Open(\"tutorial.root\"); file->ls(); Show file contents TFile** tutorial.root TFile* tutorial.root KEY: RooWorkspace workspace;1 Tutorial Workspace KEY: TProcessID ProcessID0;1 48737500-e7e5-11e6-be6f-0d0011acbeef Inside the file, there is something called a RooWorkspace . This is just the RooFit way of keeping a persistent link between the objects for a model. It is a very useful way to share data and PDFs/functions etc among CMS collaborators. Let's take a look at it. It contains a RooDataSet and one variable. This time we called our variable (or observable) CMS_hgg_mass , let's assume now that this is the invariant mass of photon pairs where we assume our H-boson decays to photons. RooWorkspace *wspace = (RooWorkspace*) file->Get(\"workspace\"); wspace->Print(\"v\"); Show RooWorkspace(workspace) Tutorial Workspace contents variables --------- (CMS_hgg_mass) datasets -------- RooDataSet::dataset(CMS_hgg_mass) Let's have a look at the data. The RooWorkspace has several accessor functions, we will use the RooWorkspace::data one. There are also RooWorkspace::var , RooWorkspace::function and RooWorkspace::pdf with (hopefully) obvious purposes. RooDataSet *hgg_data = (RooDataSereat*) wspace->data(\"dataset\"); RooRealVar *hgg_mass = (RooRealVar*) wspace->var(\"CMS_hgg_mass\"); plot = hgg_mass->frame(); hgg_data->plotOn(plot,RooFit::Binning(160)); // Here we've picked a certain number of bins just for plotting purposes plot->Draw(); can->Update(); can->Draw();","title":"Datasets"},{"location":"part5/roofit/#likelihoods-and-fitting-to-data","text":"The data we have in our file doesn't look like a Gaussian distribution. Instead, we could probably use something like an exponential to describe it. There is an exponential PDF already in RooFit (yep you guessed it) RooExponential . For a pdf, we only need one parameter which is the exponential slope \\alpha \\alpha so our pdf is, f(m|\\alpha) = \\dfrac{1}{N} e^{-\\alpha m} f(m|\\alpha) = \\dfrac{1}{N} e^{-\\alpha m} Where of course, N = \\int_{110}^{150} e^{-\\alpha m} dm N = \\int_{110}^{150} e^{-\\alpha m} dm is the normalisation constant. You can fund a bunch of available RooFit functions here: https://root.cern.ch/root/html/ROOFIT_ROOFIT_Index.html There is also support for a generic pdf in the form of a RooGenericPdf , check this link: https://root.cern.ch/doc/v608/classRooGenericPdf.html Let's create an exponential PDF for our background, RooRealVar alpha(\"alpha\",\"#alpha\",-0.05,-0.2,0.01); RooExponential expo(\"exp\",\"exponential function\",*hgg_mass,alpha); We can use RooFit to tell us to estimate the value of \\alpha \\alpha using this dataset. You will learn more about parameter estimation but for now we will just assume you know about maximising likelihoods. This maximum likelihood estimator is common in HEP and is known to give unbiased estimates for things like distribution means etc. This also introduces the other main use of PDFs in RooFit. They can be used to construct likelihoods easily. The likelihood \\mathcal{L} \\mathcal{L} is defined for a particluar dataset (and model) as being proportional to the probability to observe the data assuming some pdf. For our data, the probability to observe an event with a value in an interval bounded by a and b is given by, P\\left(m~\\epsilon~[a,b] \\right) = \\int_{a}^{b} f(m|\\alpha)dm P\\left(m~\\epsilon~[a,b] \\right) = \\int_{a}^{b} f(m|\\alpha)dm As that interval shrinks we can say this probability just becomes equal to f(m|\\alpha)dm f(m|\\alpha)dm . The probability to observe the dataset we have is given by the product of such probabilities for each of our data points, so that \\mathcal{L}(\\alpha) \\propto \\prod_{i} f(m_{i}|\\alpha) \\mathcal{L}(\\alpha) \\propto \\prod_{i} f(m_{i}|\\alpha) Note that for a specific dataset, the dm dm factors which should be there are constnant. They can therefore be absorbed into the constant of proportionality! The maximum likelihood esitmator for \\alpha \\alpha , usually written as \\hat{\\alpha} \\hat{\\alpha} , is found by maximising \\mathcal{L}(\\alpha) \\mathcal{L}(\\alpha) . Note that this won't depend on the value of the constant of proportionality so we can ignore it. This is true in most scenarios because usually only the ratio of likelihoods is needed, in which the constant factors out. Obviously this multiplication of exponentials can lead to very large (or very small) numbers which can lead to numerical instabilities. To avoid this, we can take logs of the likelihood. Its also common to multiply this by -1 and minimize the resulting N egative L og L ikelihood : \\mathrm{-Log}\\mathcal{L}(\\alpha) \\mathrm{-Log}\\mathcal{L}(\\alpha) . RooFit can construct the NLL for us. RooNLLVar *nll = (RooNLLVar*) expo.createNLL(*hgg_data); nll->Print(\"v\"); Show --- RooAbsArg --- Value State: DIRTY Shape State: DIRTY Attributes: Address: 0x7fdddbe46200 Clients: Servers: (0x11eab5638,V-) RooRealVar::alpha \"#alpha\" Proxies: paramSet -> 1) alpha --- RooAbsReal --- Plot label is \"nll_exp_dataset\" Notice that the NLL object knows which RooRealVar is the parameter because it doesn't find that one in the dataset. This is how RooFit distiguishes between observables and parameters . RooFit has an interface to Minuit via the RooMinimizer class which takes the NLL as an argument. To minimize, we just call the RooMinimizer::minimize() function. Minuit2 is the program and migrad is the minimization routine which uses gradient descent. RooMinimizer minim(*nll); minim.minimize(\"Minuit2\",\"migrad\"); Show ********** ** 1 **SET PRINT 1 ********** ********** ** 2 **SET NOGRAD ********** PARAMETER DEFINITIONS: NO. NAME VALUE STEP SIZE LIMITS 1 alpha -5.00000e-02 2.10000e-02 -2.00000e-01 1.00000e-02 ********** ** 3 **SET ERR 0.5 ********** ********** ** 4 **SET PRINT 1 ********** ********** ** 5 **SET STR 1 ********** NOW USING STRATEGY 1: TRY TO BALANCE SPEED AGAINST RELIABILITY ********** ** 6 **MIGRAD 500 1 ********** FIRST CALL TO USER FUNCTION AT NEW START POINT, WITH IFLAG=4. START MIGRAD MINIMIZATION. STRATEGY 1. CONVERGENCE WHEN EDM .LT. 1.00e-03 FCN=3589.52 FROM MIGRAD STATUS=INITIATE 4 CALLS 5 TOTAL EDM= unknown STRATEGY= 1 NO ERROR MATRIX EXT PARAMETER CURRENT GUESS STEP FIRST NO. NAME VALUE ERROR SIZE DERIVATIVE 1 alpha -5.00000e-02 2.10000e-02 2.24553e-01 -9.91191e+01 ERR DEF= 0.5 MIGRAD MINIMIZATION HAS CONVERGED. MIGRAD WILL VERIFY CONVERGENCE AND ERROR MATRIX. COVARIANCE MATRIX CALCULATED SUCCESSFULLY FCN=3584.68 FROM MIGRAD STATUS=CONVERGED 18 CALLS 19 TOTAL EDM=1.4449e-08 STRATEGY= 1 ERROR MATRIX ACCURATE EXT PARAMETER STEP FIRST NO. NAME VALUE ERROR SIZE DERIVATIVE 1 alpha -4.08262e-02 2.91959e-03 1.33905e-03 -3.70254e-03 ERR DEF= 0.5 EXTERNAL ERROR MATRIX. NDIM= 25 NPAR= 1 ERR DEF=0.5 8.527e-06 RooFit has found the best fit value of alpha for this dataset. It also estimates an uncertainty on alpha using the Hessian matrix from the fit. alpha.Print(\"v\"); --- RooAbsArg --- Value State: clean Shape State: clean Attributes: Address: 0x11eab5638 Clients: (0x11eab5978,V-) RooExponential::exp \"exponential function\" (0x7fdddbe46200,V-) RooNLLVar::nll_exp_dataset \"-log(likelihood)\" (0x7fdddbe95600,V-) RooExponential::exp \"exponential function\" (0x7fdddbe5a400,V-) RooRealIntegral::exp_Int[CMS_hgg_mass] \"Integral of exponential function\" Servers: Proxies: --- RooAbsReal --- Plot label is \"alpha\" --- RooAbsRealLValue --- Fit range is [ -0.2 , 0.01 ] --- RooRealVar --- Error = 0.00291959 Lets plot the resulting exponential on the data. Notice that the value of \\hat{\\alpha} \\hat{\\alpha} is used for the exponential. expo.plotOn(plot); expo.paramOn(plot); plot->Draw(); can->Update(); can->Draw(); It looks like there could be a small region near 125 GeV for which our fit doesn't quite go through the points. Maybe our hypothetical H-boson isn't so hypothetical after all! Let's see what happens if we include some resonant signal into the fit. We can take our Gaussian function again and use that as a signal model. A reasonable value for the resolution of a resonant signal with a mass around 125 GeV decaying to a pair of photons is around a GeV. sigma.setVal(1.); sigma.setConstant(); MH.setVal(125); MH.setConstant(); RooGaussian hgg_signal(\"signal\",\"Gaussian PDF\",*hgg_mass,MH,sigma); By setting these parameters constant, RooFit knows (either when creating the NLL by hand or when using fitTo ) that there is not need to fit for these parameters. We need to add this to our exponential model and fit a \"Sigmal+Background model\" by creating a RooAddPdf . In RooFit there are two ways to add PDFs, recursively where the fraction of yields for the signal and background is a parameter or absolutely where each PDF has its own normalisation. We're going to use the second one. RooRealVar norm_s(\"norm_s\",\"N_{s}\",10,100); RooRealVar norm_b(\"norm_b\",\"N_{b}\",0,1000); const RooArgList components(hgg_signal,expo); const RooArgList coeffs(norm_s,norm_b); RooAddPdf model(\"model\",\"f_{s+b}\",components,coeffs); model.Print(\"v\"); Show --- RooAbsArg --- Value State: DIRTY Shape State: DIRTY Attributes: Address: 0x11ed5d7a8 Clients: Servers: (0x11ed5a0f0,V-) RooGaussian::signal \"Gaussian PDF\" (0x11ed5d058,V-) RooRealVar::norm_s \"N_{s}\" (0x11eab5978,V-) RooExponential::exp \"exponential function\" (0x11ed5d398,V-) RooRealVar::norm_b \"N_{b}\" Proxies: !refCoefNorm -> !pdfs -> 1) signal 2) exp !coefficients -> 1) norm_s 2) norm_b --- RooAbsReal --- Plot label is \"model\" --- RooAbsPdf --- Cached value = 0 Ok now lets fit the model. Note this time we add the option Extended() which tells RooFit that we care about the overall number of observed events in the data n n too. It will add an additional Poisson term in the likelihood to account for this so our likelihood this time looks like, L_{s+b}(N_{s},N_{b},\\alpha) = \\dfrac{ N_{s}+N_{b}^{n} e^{N_{s}+N_{b}} }{n!} \\cdot \\prod_{i}^{n} \\left[ c f_{s}(m_{i}|M_{H},\\sigma)+ (1-c)f_{b}(m_{i}|\\alpha) \\right] L_{s+b}(N_{s},N_{b},\\alpha) = \\dfrac{ N_{s}+N_{b}^{n} e^{N_{s}+N_{b}} }{n!} \\cdot \\prod_{i}^{n} \\left[ c f_{s}(m_{i}|M_{H},\\sigma)+ (1-c)f_{b}(m_{i}|\\alpha) \\right] where c = \\dfrac{ N_{s} }{ N_{s} + N_{b} } c = \\dfrac{ N_{s} }{ N_{s} + N_{b} } , f_{s}(m|M_{H},\\sigma) f_{s}(m|M_{H},\\sigma) is the Gaussian signal pdf and f_{b}(m|\\alpha) f_{b}(m|\\alpha) is the exponential pdf. Remember that M_{H} M_{H} and \\sigma \\sigma are fixed so that they are no longer parameters of the likelihood. There is a simpler interface for maximum likelihood fits which is the RooAbsPdf::fitTo method. With this simple method, RooFit will construct the negative log-likelihood function, from the pdf, and minimize all of the free parameters in one step. model.fitTo(*hgg_data,RooFit::Extended()); model.plotOn(plot,RooFit::Components(\"exp\"),RooFit::LineColor(kGreen)); model.plotOn(plot,RooFit::LineColor(kRed)); model.paramOn(plot); can->Clear(); plot->Draw(); can->Update(); can->Draw(); What about if we also fit for the mass ( M_{H} M_{H} )? we can easily do this by removing the constant setting on MH. MH.setConstant(false); model.fitTo(*hgg_data,RooFit::Extended()); Show output [#1] INFO:Minization -- RooMinimizer::optimizeConst: activating const optimization [#1] INFO:Minization -- The following expressions will be evaluated in cache-and-track mode: (signal,exp) ********** ** 1 **SET PRINT 1 ********** ********** ** 2 **SET NOGRAD ********** PARAMETER DEFINITIONS: NO. NAME VALUE STEP SIZE LIMITS 1 MH 1.25000e+02 1.00000e+00 1.20000e+02 1.30000e+02 2 alpha -4.08793e-02 2.96856e-03 -2.00000e-01 1.00000e-02 3 norm_b 9.67647e+02 3.25747e+01 0.00000e+00 1.00000e+03 MINUIT WARNING IN PARAMETR ============== VARIABLE3 BROUGHT BACK INSIDE LIMITS. 4 norm_s 3.22534e+01 1.16433e+01 1.00000e+01 1.00000e+02 ********** ** 3 **SET ERR 0.5 ********** ********** ** 4 **SET PRINT 1 ********** ********** ** 5 **SET STR 1 ********** NOW USING STRATEGY 1: TRY TO BALANCE SPEED AGAINST RELIABILITY ********** ** 6 **MIGRAD 2000 1 ********** FIRST CALL TO USER FUNCTION AT NEW START POINT, WITH IFLAG=4. START MIGRAD MINIMIZATION. STRATEGY 1. CONVERGENCE WHEN EDM .LT. 1.00e-03 FCN=-2327.53 FROM MIGRAD STATUS=INITIATE 10 CALLS 11 TOTAL EDM= unknown STRATEGY= 1 NO ERROR MATRIX EXT PARAMETER CURRENT GUESS STEP FIRST NO. NAME VALUE ERROR SIZE DERIVATIVE 1 MH 1.25000e+02 1.00000e+00 2.01358e-01 1.12769e+01 2 alpha -4.08793e-02 2.96856e-03 3.30048e-02 -1.22651e-01 3 norm_b 9.67647e+02 3.25747e+01 2.56674e-01 -1.96463e-02 4 norm_s 3.22534e+01 1.16433e+01 3.10258e-01 -8.97036e-04 ERR DEF= 0.5 MIGRAD MINIMIZATION HAS CONVERGED. MIGRAD WILL VERIFY CONVERGENCE AND ERROR MATRIX. COVARIANCE MATRIX CALCULATED SUCCESSFULLY FCN=-2327.96 FROM MIGRAD STATUS=CONVERGED 65 CALLS 66 TOTAL EDM=1.19174e-05 STRATEGY= 1 ERROR MATRIX ACCURATE EXT PARAMETER STEP FIRST NO. NAME VALUE ERROR SIZE DERIVATIVE 1 MH 1.24628e+02 3.98153e-01 2.66539e-03 2.46327e-02 2 alpha -4.07708e-02 2.97195e-03 1.10093e-03 8.33780e-02 3 norm_b 9.66105e+02 3.25772e+01 5.96627e-03 1.83523e-03 4 norm_s 3.39026e+01 1.17380e+01 9.60816e-03 -2.32681e-03 ERR DEF= 0.5 EXTERNAL ERROR MATRIX. NDIM= 25 NPAR= 4 ERR DEF=0.5 1.589e-01 -3.890e-05 1.462e-01 -1.477e-01 -3.890e-05 8.836e-06 -2.020e-04 2.038e-04 1.462e-01 -2.020e-04 1.073e+03 -1.072e+02 -1.477e-01 2.038e-04 -1.072e+02 1.420e+02 PARAMETER CORRELATION COEFFICIENTS NO. GLOBAL 1 2 3 4 1 0.04518 1.000 -0.033 0.011 -0.031 2 0.03317 -0.033 1.000 -0.002 0.006 3 0.27465 0.011 -0.002 1.000 -0.275 4 0.27610 -0.031 0.006 -0.275 1.000 ********** ** 7 **SET ERR 0.5 ********** ********** ** 8 **SET PRINT 1 ********** ********** ** 9 **HESSE 2000 ********** COVARIANCE MATRIX CALCULATED SUCCESSFULLY FCN=-2327.96 FROM HESSE STATUS=OK 23 CALLS 89 TOTAL EDM=1.19078e-05 STRATEGY= 1 ERROR MATRIX ACCURATE EXT PARAMETER INTERNAL INTERNAL NO. NAME VALUE ERROR STEP SIZE VALUE 1 MH 1.24628e+02 3.98106e-01 5.33077e-04 -7.45154e-02 2 alpha -4.07708e-02 2.97195e-03 2.20186e-04 5.42722e-01 3 norm_b 9.66105e+02 3.26003e+01 2.38651e-04 1.20047e+00 4 norm_s 3.39026e+01 1.17445e+01 3.84326e-04 -4.87967e-01 ERR DEF= 0.5 EXTERNAL ERROR MATRIX. NDIM= 25 NPAR= 4 ERR DEF=0.5 1.588e-01 -3.888e-05 1.304e-01 -1.304e-01 -3.888e-05 8.836e-06 -1.954e-04 1.954e-04 1.304e-01 -1.954e-04 1.074e+03 -1.082e+02 -1.304e-01 1.954e-04 -1.082e+02 1.421e+02 PARAMETER CORRELATION COEFFICIENTS NO. GLOBAL 1 2 3 4 1 0.04274 1.000 -0.033 0.010 -0.027 2 0.03314 -0.033 1.000 -0.002 0.006 3 0.27694 0.010 -0.002 1.000 -0.277 4 0.27806 -0.027 0.006 -0.277 1.000 [#1] INFO:Minization -- RooMinimizer::optimizeConst: deactivating const optimization Notice the result for the fitted MH is not 125 and is included in the list of fitted parameters. We can get more information about the fit, via the RooFitResult , using the option Save() . RooFitResult *fit_res = (RooFitResult*) model.fitTo(*hgg_data,RooFit::Extended(),RooFit::Save()); For example, we can get the Correlation Matrix from the fit result... Note that the order of the parameters are the same as listed in the \"Floating Parameter\" list above TMatrixDSym cormat = fit_res->correlationMatrix(); cormat.Print(); 4x4 matrix is as follows | 0 | 1 | 2 | 3 | --------------------------------------------------------- 0 | 1 -0.03282 0.009538 -0.02623 1 | -0.03282 1 -0.001978 0.005439 2 | 0.009538 -0.001978 1 -0.2769 3 | -0.02623 0.005439 -0.2769 1 A nice feature of RooFit is that once we have a PDF, data and results like this, we can import this new model into our RooWorkspace and show off our new discovery to our LHC friends (if we weren't already too late!). We can also save the \"state\" of our parameters for later, by creating a snapshot of the current values. wspace->import(model); RooArgSet *params = model.getParameters(*hgg_data); wspace->saveSnapshot(\"nominal_values\",*params); wspace->Print(\"V\"); Show output RooWorkspace(workspace) Tutorial Workspace contents variables --------- (CMS_hgg_mass,MH,alpha,norm_b,norm_s,resolution) p.d.f.s ------- RooExponential::exp[ x=CMS_hgg_mass c=alpha ] = 0.00248636 RooAddPdf::model[ norm_s * signal + norm_b * exp ] = 0.00240205 RooGaussian::signal[ x=CMS_hgg_mass mean=MH sigma=resolution ] = 5.34013e-110 datasets -------- RooDataSet::dataset(CMS_hgg_mass) parameter snapshots ------------------- nominal_values = (MH=124.627 +/- 0.398094,resolution=1[C],norm_s=33.9097 +/- 11.7445,alpha=-0.040779 +/- 0.00297195,norm_b=966.109 +/- 32.6025) This is exactly what needs to be done when you want to use shape based datacards in combine with parametric models.","title":"Likelihoods and Fitting to data"},{"location":"tutorial2020/exercise/","text":"Getting started We need to set up a new CMSSW area and checkout the combine package: export SCRAM_ARCH=slc7_amd64_gcc700 cmsrel CMSSW_10_2_13 cd CMSSW_10_2_13/src cmsenv git clone https://github.com/cms-analysis/HiggsAnalysis-CombinedLimit.git HiggsAnalysis/CombinedLimit cd HiggsAnalysis/CombinedLimit cd $CMSSW_BASE/src/HiggsAnalysis/CombinedLimit git fetch origin git checkout v8.1.0 cd $CMSSW_BASE/src We will also make use another package, CombineHarvester , which contains some high-level tools for working with combine. The following command will download the repository and checkout just the parts of it we need for this tutorial: bash <(curl -s https://raw.githubusercontent.com/cms-analysis/CombineHarvester/master/CombineTools/scripts/sparse-checkout-https.sh) Now make sure the CMSSW area is compiled: scramv1 b clean; scramv1 b Finally we will checkout the working directory for these tutorials - this contains all the inputs needed to run the exercises below: cd $CMSSW_BASE/src git clone https://gitlab.cern.ch/adewit/combinetutorial-2020 cd combinetutorial-2020 Session structure These exercises are accompanied by introductory presentations given live during the tutorial mornings (and made available for later viewing for those who cannot follow them live) The introductory presentations that are scheduled appear: Before part 1 (recording available here ) Before part 1E (recording available here ) Before part 2 (recording available here ) Before part 4 (recording available here ) All presentations introduce the methods you'll learn about in the following sections and in addition the presentations in bold contain some theoretical background. If you get stuck while working on the exercises, or have specific questions about them, you can ask for help either directly in the zoom room during tutorial hours, or on this mattermost channel . The mattermost channel will remain available after the tutorial, so if you're working on the exercises at a later date you can still ask questions about them there. Part 1: RooFit You can watch the introductory presentation here RooFit is an object-oriented analysis environment built on ROOT, with a collection of classes designed to augment ROOT for data modelling. Combine is in turn built on RooFit, so before learning about Combine, it is useful to get to grips with a few RooFit basics. We will do that in this section. We will use python syntax in this section; you can either call the commands in an interactive python session, or just put them in a .py script. Make sure to do from ROOT import * at the top of your script (or in the interactive session) A: Variables In RooFit, any variable, data point, function, PDF (etc.) is represented by a c++ object. The most basic of these is the RooRealVar. Let's create one which will represent the mass of some hypothetical particle, name it, and give a hypothetical starting value and range. MH = RooRealVar(\"MH\",\"mass of the Hypothetical Boson (H-boson) in GeV\",125,120,130) MH.Print() The output of this is RooRealVar::MH = 125 L(120 - 130) So we now have a RooRealVar called MH, with default value 125 and range 120-130. We can now access the object and change for example its value: MH.setVal(130) print MH.getVal() Which should print the new value, 130. In particle detectors we typically don't observe this particle mass but usually define some observable which is sensitive to this mass. Lets assume we can detect and reconstruct the decay products of the H-boson and measure the invariant mass of those particles. We need to make another variable which represents that invariant mass. Make a RooRealVar \"mass\" with a default value of 100 GeV and a range 80-200 In a perfect world we would perfectly measure the exact mass of the particle in every single event. However, our detectors are usually far from perfect so there will be some resolution effect. Let's assume the resolution of our measurement of the invariant mass is 10 GeV (range 0-20 GeV) and call it \"sigma\" Create this variable \"sigma\" B: Functions and PDFs More exotic variables can be constructed out of these RooRealVar s using RooFormulaVar s. For example, suppose we wanted to make a function out of the variables which represented the relative resolution as a function of the hypothetical mass MH. func = RooFormulaVar(\"R\",\"@0/@1\",RooArgList(sigma,mass)) func.Print(\"v\") The main objects we are interested in using from RooFit are probability density functions (PDFs). We can construct the PDF f(m|M_H,\\sigma) f(m|M_H,\\sigma) as a Gaussian shape, a RooGaussian : gauss = RooGaussian(\"gauss\",\"f(m|M_{H},#sigma)\",mass,MH,sigma) gauss.Print(\"v\") Notice how the gaussian PDF, like the RooFormulaVar depends on our RooRealVar objects, these are its servers. Its evaluation will depend on their values. The main difference between PDFs and functions in RooFit is that PDFs are automatically normalised to unity, hence they represent a probability density, you don't need to normalise yourself. Let's plot it for the different values of M_H M_H . C: Plotting First we need to make a canvas and a RooPlot object. This object needs to know what observable is going to be on the x-axis: can = TCanvas() plot = mass.frame() Now we can plot the gaussian PDF for several mass values. We set M_H M_H to 130 GeV earlier on, so to plot this PDF for M_H M_H of 130 GeV we just do gauss.plotOn(plot,RooFit.LineColor(kGreen+3)) Where we're using kGreen+3 as the line colour. Notice that we need to tell RooFit on which RooPlot object we want to plot our PDF, even if we only have one such object. Let's also plot the PDF with M_H M_H at 120 GeV, in blue, and with M_H M_H 125 GeV, in red: MH.setVal(120) gauss.plotOn(plot,RooFit.LineColor(kBlue)) MH.setVal(125) gauss.plotOn(plot,RooFit.LineColor(kRed)) Finally, let's try adding this PDF for M_H M_H at 115 GeV in bright green. We'll use a dashed line for this one. Afterwards we'll draw the plot and save the canvas. MH.setVal(115) gauss.plotOn(plot,RooFit.LineColor(kGreen),RooFit.LineStyle(2)) plot.Draw() can.Draw() can.SaveAs(\"gaussians.pdf\") Why do the blue and bright green lines overlap? D: Workspaces Next we'll look at how to store RooFit objects and links between them. We can do this with a RooWorkspace . Let's create one and import our PDF, and the RooFormulaVar we created earlier: w = RooWorkspace(\"w\") getattr(w,'import')(gauss) getattr(w,'import')(func) w.writeToFile(\"workspace.root\") Notice that the RooRealVar s we created are also getting imported into the workspace as our gaussian PDF depends on all three of them. Now we can open the file that we've created in root: root workspace.root .ls You should see that our workspace, named w is in the file: TFile** workspace.root TFile* workspace.root KEY: RooWorkspace w;1 w KEY: TProcessID ProcessID0;1 94b05638-d0c4-11ea-a5b3-84978a89beef We can inspect its contents: w->Print() Show output RooWorkspace(w) w contents variables --------- (MH,m,resolution) p.d.f.s ------- RooGaussian::gauss[ x=m mean=MH sigma=resolution ] = 0.135335 functions -------- RooFormulaVar::R[ actualVars=(resolution,m) formula=\"@0/@1\" ] = 0.1 Now we can check the properties of some of the objects, for example: w->pdf(\"gauss\")->Print(\"v\") Show output --- RooAbsArg --- Value State: DIRTY Shape State: DIRTY Attributes: Address: 0x63b62e0 Clients: Servers: (0x6781400,V-) RooRealVar::m \"m (GeV)\" (0x687edd0,V-) RooRealVar::MH \"mass of the Hypothetical Boson (H-boson) in GeV\" (0x6795740,V-) RooRealVar::resolution \"#sigma\" Proxies: x -> m mean -> MH sigma -> resolution --- RooAbsReal --- Plot label is \"gauss\" --- RooAbsPdf --- Cached value = 0 We can also check and change the values of our RooRealVar s w->var(\"MH\")->Print() w->var(\"MH\")->setVal(123) w->var(\"MH\")->getVal() Gives: RooRealVar::MH = 120 L(120 - 130) and (double) 123.00000 Note that if you close the file containing the workspace, open it again and call w->var(\"MH\")->getVal() You will get the value as set when the workspace was created again, in our case that's 120 GeV: (double) 120.00000 Have a look at how to use the workspace syntax in this tutorial and try to create the gaussian pdf and the formula directly in a workspace E: Fitting You can watch the introductory presentation to part 1E here Note: from this point on if you were using the interactive python session, you should switch to a python script. Of course we don't just want to create variables and pdfs, we want to do fits with them. Models that we define in combine make use of likelihood fitting, so let's build a likelihood \\mathcal{L} \\propto p(\\text{data}|\\text{parameters}) \\mathcal{L} \\propto p(\\text{data}|\\text{parameters}) where our parameters are parameters of interest, \\mu \\mu and nuisance parameters \\theta \\theta . The nuisance parameters are constrained by external measurements, so we add constraint terms \\pi(\\vec{\\theta}_0|\\vec{\\theta}) \\pi(\\vec{\\theta}_0|\\vec{\\theta}) So we have \\mathcal{L} \\propto p(\\text{data}|\\mu,\\vec{\\theta})\\cdot \\pi(\\vec{\\theta}_0|\\vec{\\theta}) \\mathcal{L} \\propto p(\\text{data}|\\mu,\\vec{\\theta})\\cdot \\pi(\\vec{\\theta}_0|\\vec{\\theta}) let's try to build the likelihood by hand for a 1-bin counting experiment. The data is the number of observed events N N , and the probability is just a poisson probability p(N|\\lambda) = \\frac{\\lambda^N e^{-\\lambda}}{N!} p(N|\\lambda) = \\frac{\\lambda^N e^{-\\lambda}}{N!} , where \\lambda \\lambda is the number of events expected in our signal+background model: \\lambda = \\mu\\cdot s(\\vec{\\theta}) + b(\\vec{\\theta}) \\lambda = \\mu\\cdot s(\\vec{\\theta}) + b(\\vec{\\theta}) . In the expression, s and b are the numbers of expected signal- and background events, which both depend on the nuisance parameters. Let's start by building a simple likelihood function with one signal process and one background process. We'll assume there are no nuisance parameters for now. The number of observed events in data is 15, the expected number of signal events is 5 and the expected number of background events 8.1. It's easiest to use the RooFit workspace factory to build our model from ROOT import * w = RooWorkspace(\"w\") We need to create an expression for the number of events in our model, \\mu s +b \\mu s +b : w.factory('expr::n(\"mu*s +b\", mu[1.0,0,4], s[5],b[8.1])') Now we can build the likelihood which is just our poisson pdf: w.factory('Poisson::poisN(N[15],n)') To find the best-fit value for our parameter of interest \\mu \\mu we need to maximize the likelihood. In practice it's actually easier to minimize the N egative l og of the l ikelihood, or NLL: w.factory('expr::NLL(\"-log(@0)\",poisN)') We can now use the RooMinimizer to find the minimum of the NLL nll = w.function(\"NLL\") minim = RooMinimizer(nll) minim.setErrorLevel(0.5) minim.minimize(\"Minuit2\",\"migrad\") bestfitnll = nll.getVal() Notice that we need to set the error level to 0.5 to get the uncertainties (relying on Wilks' theorem!) - note that there is a more reliable way of extracting the confidence interval (explicitly rather than relying on migrad). We will discuss this a bit later in this section. Now let's add a nuisance parameter, lumi , which represents the luminosity uncertainty. It has a 2.5% effect on both the signal and the background. The parameter will be log-normally distributed: when it's 0, the normalization of the signal and background are not modified; at +1\\sigma +1\\sigma the signal and background normalizations will be multiplied by 1.025 and at -1\\sigma -1\\sigma they will be divided by 1.025. We should modify the expression for the number of events in our model: w.factory('expr::n(\"mu*s*pow(1.025,lumi) +b*pow(1.025,lumi)\", mu[1.0,0,4], s[5],b[8.1],lumi[0,-4,4])') And we add a unit gaussian constraint w.factory('Gaussian::lumiconstr(lumi,0,1)') Our full likelihood will now be w.factory('PROD::likelihood(poisN,lumiconstr)') and the NLL w.factory('expr::NLL(\"-log(@0)\",likelihood)') Which we can minimize in the same way as before. Now let's extend our model a bit. Don't hesitate to ask for help if you get stuck! Expanding on what was demonstrated above, build the likelihood for N=15 N=15 , a signal process s with expectation 5 events, a background ztt with expectation 3.7 events and a background tt with expectation 4.4 events. The luminosity uncertainty applies to all three processes. The signal process is further subject to a 5% log-normally distributed uncertainty sigth , tt is subject to a 6% log-normally distributed uncertainty ttxs , and ztt is subject to a 4% log-normally distributed uncertainty zttxs . Find the best-fit value and the associated uncertainty Also perform an explicit scan of the \\Delta \\Delta NLL ( = log of profile likelihood ratio) and make a graph of the scan. Some example code can be found below to get you started. Hint: you'll need to perform fits for different values of mu, where mu is fixed. In RooFit you can set a variable to be constant as var(\"VARNAME\").setConstant(True) From the curve that you've created by performing an explicit scan, we can extract the 68% CL interval. You can do so by eye or by writing some code to find the relevant intersections of the curve. gr = TGraph() npoints = 0 for i in range(0,60): npoints+=1 mu=0.05*i ... [perform fits for different values of mu with mu fixed] ... deltanll = ... gr.SetPoint(npoints,mu,deltanll) canv = TCanvas(); gr.Draw(\"ALP\"); canv.SaveAs(\"likelihoodscan.pdf\") Well, this is doable - but we were only looking at a simple one-bin counting experiment. This might become rather cumbersome for large models... [*] [*] We'll now switch to Combine which will make it a lot easier to set up your model and do the statistical analysis than trying to build the likelihood yourself. [*] [*] Side note - RooFit does have additional functionality to help with statistical model building, but we won't go into detail today. Part 2 Measurements of parameters in Combine You can watch the introductory presentation for parts 2 & 3 here For the parts of the tutorial that follow we will work with a simplified version of a real analysis, that nonetheless will have many features of the full analysis. The analysis is a search for an additional heavy neutral Higgs boson decaying to tau lepton pairs. Such a signature is predicted in many extensions of the standard model, in particular the minimal supersymmetric standard model (MSSM). You can read about the analysis in the paper here . The statistical inference makes use of a variable called the total transverse mass ( M_{\\mathrm{T}}^{\\mathrm{tot}} M_{\\mathrm{T}}^{\\mathrm{tot}} ) that provides good discrimination between the resonant high-mass signal and the main backgrounds, which have a falling distribution in this high-mass region. The events selected in the analysis are split into a several categories which target the main di-tau final states as well as the two main production modes: gluon-fusion (ggH) and b-jet associated production (bbH). One example is given below for the fully-hadronic final state in the b-tag category which targets the bbH signal: A: A one-bin counting experiment We will begin by looking at a simplified version of a datacard from the MSSM \\phi\\rightarrow\\tau\\tau \\phi\\rightarrow\\tau\\tau analysis that has been converted to a one-bin counting experiment, as discussed in the introductory presentation to this section. While the full analysis considers a range of signal mass hypotheses, we will start by considering just one: m_{\\phi} m_{\\phi} =800GeV. Click the text below to study the datacard ( datacard_counting_part2.txt in the combinetutorial-2020 directory). It might bear some similarities to the model you just set up by hand. Show datacard imax 1 number of bins jmax 4 number of processes minus 1 kmax * number of nuisance parameters -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- bin signal_region observation 10.0 -------------------------------------------------------------------------------- bin signal_region signal_region signal_region signal_region signal_region process ttbar diboson Ztautau jetFakes bbHtautau process 1 2 3 4 0 rate 4.43803 3.18309 3.7804 1.63396 0.711064 -------------------------------------------------------------------------------- CMS_eff_b lnN 1.02 1.02 1.02 - 1.02 CMS_eff_t lnN 1.12 1.12 1.12 - 1.12 CMS_eff_t_highpt lnN 1.1 1.1 1.1 - 1.1 acceptance_Ztautau lnN - - 1.08 - - acceptance_bbH lnN - - - - 1.05 acceptance_ttbar lnN 1.005 - - - - lumi_13TeV lnN 1.025 1.025 1.025 - 1.025 norm_jetFakes lnN - - - 1.2 - xsec_Ztautau lnN - - 1.04 - - xsec_diboson lnN - 1.05 - - - xsec_ttbar lnN 1.06 - - - - The layout of the datacard is as follows: At the top are the numbers imax , jmax and kmax representing the number of bins, processes and nuisance parameters respectively. Here a \"bin\" can refer to a literal single event count as in this example, or a full distribution we are fitting, in general with many histogram bins, as we will see later. We will refer to both as \"channels\" from now on. It is possible to replace these numbers with * and they will be deduced automatically. The first line starting with bin gives a unique label to each channel, and the following line starting with observation gives the number of events observed in data. In the remaining part of the card there are several columns: each one represents one process in one channel. The first four lines labelled bin , process , process and rate give the channel label, the process label, a process identifier ( <=0 for signal, >0 for background) and the number of expected events respectively. The remaining lines describe sources of systematic uncertainty. Each line gives the name of the uncertainty, (which will become the name of the nuisance parameter inside our RooFit model), the type of uncertainty (\"lnN\" = log-normal normalisation uncertainty) and the effect on each process in each channel. E.g. a 20% uncertainty on the yield is written as 1.20. It is also possible to add a hash symbol ( # ) at the start of a line, which combine will then ignore when it reads the card. We can now run combine directly using this datacard as input. The general format for running combine is: combine -M [method] [datacard] [additional options...] By default, combine will report measurements on a parameter called r , which is the default parameter of interest (POI) that is added to the model automatically. It is a linear scaling of the normalisation of all signal processes given in the datacard, i.e. if s_{i,j} s_{i,j} is the nominal number of signal events in channel i i for signal process j j , then the normalisation of that signal in the model is given as r\\cdot s_{i,j}(\\vec{\\theta}) r\\cdot s_{i,j}(\\vec{\\theta}) , where \\vec{\\theta} \\vec{\\theta} represents the set of nuisance parameters which may also affect the signal normalisation. So r is the equivalent of the parameter \\mu \\mu we introduced in the RooFit section. We have some choice in the interpretation of r: for the measurement of a process with a well defined SM prediction we may enter this as the nominal yield in the datacard, such that r=1 r=1 corresponds to this SM expectation, whereas for setting limits on BSM processes we may choose the nominal yield to correspond to some cross section, e.g. 1 pb, such that we can interpret the limit as a cross section limit directly. In this example the signal has been normalised to a cross section times branching fraction of 1 fb. We will now explore one of the most commonly used modes of combine: FitDiagnostics . As well as allowing us to make a measurement of some physical quantity, this method is useful to gain additional information about the model and the behaviour of the fit. It performs two fits: A \"background-only\" (b-only) fit: first POI (usually \"r\") fixed to zero A \"signal+background\" (s+b) fit: all POIs are floating With the s+b fit combine will report the best-fit value of our signal strength modifier r . Two ouput files will be produced. One of these will be produced regardless of which combine method we use, and the file name depends on the options we ran with. It is of the form: higgsCombine[name].[method].mH[mass].root. The default name is Test , but you can change this with the --name (or -n ) option. The file contains a TTree called limit which stores the numerical values returned by FitDiagnostics. Note that in our case we did not set a signal mass when running combine (i.e. -m 800), so the output file just uses the default value of 120. This does not affect our result in any way though, just the label that is used on the output file. The second file that is produced is a file named fitdiagnostics.root , which contains additional information. In particular it includes two RooFitResult objects, one for the b-only and one for the s+b fit, which store the fitted values of all the nuisance parameters (NPs) and POIs as well as estimates of their uncertainties. The covariance matrix from both fits is also included, from which we can learn about the correlations between parameters. Task: Make a datacard for the simple model that you previously set up directly in RooFit Now run the FitDiagnostics method on this datacard: combine -M FitDiagnostics datacard.txt --forceRecreateNLL Note that the --forceRecreateNLL option is only needed for counting experiments; we will soon look at a shape-based analysis instead where this is not needed. There are other command line options we can supply to combine which will change its behaviour when run. You can see the full set of supported options by doing combine -h . Many options are specific to a given method, but others are more general and are applicable to all methods. Throughout this tutorial we will highlight some of the most useful options you may need to use, for example: The range on the signal strength modifier: --rMin=X and --rMax=Y : In RooFit parameters can optionally have a range specified. The implication of this is that their values cannot be adjusted beyond the limits of this range. The min and max values can be adjusted though, and we might need to do this for our POI r if the order of magnitude of our measurement is different from the default range of [0, 20] . This will be discussed again later in the tutorial. Verbosity: -v X : By default combine does not usually produce much output on the screen other the main result at the end. However, much more detailed information can be printed by setting the -v N with N larger than zero. For example at -v 3 the logs from the minimizer, Minuit, will also be printed. These are very useful for debugging problems with the fit. Task: Verify that the best-fit value and uncertainty on r are the same as what you extracted from the likelihood scan you did by hand in RooFit. Now open the resulting fitDiagnostics.root file interactively and print the contents of the s+b RooFit result: root [1] fit_s->Print() Show output RooFitResult: minimized FCN value: 3.54419, estimated distance to minimum: 4.84873e-07 covariance matrix quality: Full, accurate covariance matrix Status : MINIMIZE=0 HESSE=0 Floating Parameter FinalValue +/- Error -------------------- -------------------------- lumi 1.6988e-04 +/- 9.94e-01 r 1.3792e+00 +/- 7.76e-01 sigth -7.5538e-05 +/- 9.94e-01 ttxs 3.2043e-05 +/- 9.94e-01 zttxs -2.9580e-05 +/- 9.94e-01 There are several useful pieces of information here. At the top the status codes from the fits that were performed is given. In this case we can see that two algorithms were run: MINIMIZE and HESSE , both of which returned a successful status code (0). Both of these are routines in the Minuit2 minimization package - the default minimizer used in RooFit. The first performs the main fit to the data, and the second calculates the covariance matrix at the best-fit point. It is important to always check this second step was successful and the message \"Full, accurate covariance matrix\" is printed, otherwise the parameter uncertainties can be very inaccurate, even if the fit itself was successful. Underneath this the best-fit values ( \\theta \\theta ) and symmetrised uncertainties for all the floating parameters are given. For all the constrained nuisance parameters a convention is used by which the nominal value ( \\theta_I \\theta_I ) is zero, corresponding to the mean of a Gaussian constraint PDF with width 1.0, such that the parameter values \\pm 1.0 \\pm 1.0 correspond to the \\pm 1\\sigma \\pm 1\\sigma input uncertainties. A more useful way of looking at this is to compare the pre- and post-fit values of the parameters, to see how much the fit to data has shifted and constrained these parameters with respect to the input uncertainty. The script diffNuisances.py can be used for this: python diffNuisances.py fitDiagnostics.root --all Show output name b-only fit s+b fit rho lumi +0.17, 0.99 +0.00, 0.99 -0.09 sigth +0.00, 0.99 -0.00, 0.99 -0.09 ttxs +0.22, 0.99 +0.00, 0.99 -0.07 zttxs +0.12, 0.99 -0.00, 0.99 -0.04 The numbers in each column are respectively \\frac{\\theta-\\theta_I}{\\sigma_I} \\frac{\\theta-\\theta_I}{\\sigma_I} (often called the pull , though note that more than one definition is in use for this), where \\sigma_I \\sigma_I is the input uncertainty; and the ratio of the post-fit to the pre-fit uncertainty \\frac{\\sigma}{\\sigma_I} \\frac{\\sigma}{\\sigma_I} . Question: Can you explain why none of the nuisance parameters are pulled in the s+b fit, but in the b-only fit some are? So far we've just been looking at our original single-bin counting experiment; let's make things a bit more interesting and move to a shape-based analysis B: Setting up the datacard for a shape-based analysis Now we move to the next step: instead of a one-bin counting experiment we will fit a binned distribution. In a typical analysis we will produce TH1 histograms of some variable sensitive to the presence of signal: one for the data and one for each signal and background process. Then we add a few extra lines to the datacard to link the declared processes to these shapes which are saved in a ROOT file, for example: Show datacard imax 1 jmax 1 kmax * --------------- shapes * * simple-shapes-TH1_input.root $PROCESS $PROCESS_$SYSTEMATIC shapes sig * simple-shapes-TH1_input.root $PROCESS$MASS $PROCESS$MASS_$SYSTEMATIC --------------- bin bin1 observation 85 ------------------------------ bin bin1 bin1 process sig background process 0 1 rate 10 100 -------------------------------- lumi lnN 1.10 1.0 bgnorm lnN 1.00 1.3 alpha shape - 1 Note that as with the one-bin card, the total nominal rate of a given process must be specified in the rate line of the datacard. This should agree with the value returned by TH1::Integral . However, we can also put a value of -1 and the Integral value will be substituted automatically. There are two other differences with respect to the one-bin card: A new block of lines at the top defining how channels and processes are mapped to the histograms (more than one line can be used) In the list of systematic uncertainties some are marked as shape instead of lnN The syntax of the \"shapes\" line is: shapes [process] [channel] [file] [histogram] [histogram_with_systematics] . It is possible to use the * wildcard to map multiple processes and/or channels with one line. The histogram entries can contain the $PROCESS , $CHANNEL and $MASS place-holders which will be substituted when searching for a given (process, channel) combination. The value of $MASS is specified by the -m argument when combine. By default the observed data process name will be data_obs . Shape uncertainties can be added by supplying two additional histograms for a process, corresponding to the distribution obtained by shifting that parameter up and down by one standard deviation. These shapes will be interpolated with a 6th order polynomial for shifts below 1\\sigma 1\\sigma and linearly beyond. The normalizations are interpolated linearly in log scale just like we do for log-normal uncertainties. The final argument of the \"shapes\" line above should contain the $SYSTEMATIC place-holder which will be substituted by the systematic name given in the datacard. In the list of uncertainties the interpretation of the values for shape lines is a bit different from lnN . The effect can be \"-\" or 0 for no effect, 1 for normal effect, and possibly something different from 1 to test larger or smaller effects (in that case, the unit Gaussian is scaled by that factor before using it as parameter for the interpolation). In this section we will use a datacard corresponding to the full distribution that was shown at the start of section 1, not just the high mass region. Have a look at datacard_shape_part2.txt : this is still currently a one-bin counting experiment, however the yields are much higher since we now consider the full range of M_{\\mathrm{T}}^{\\mathrm{tot}} M_{\\mathrm{T}}^{\\mathrm{tot}} . If you run the asymptotic limit calculation on this you should find the sensitivity is significantly worse than before. The first task is to convert this to a shape analysis: the file datacard_part2.shapes.root contains all the necessary histograms, including those for the relevant shape systematic uncertainties. Add the relevant shapes lines to the top of the datacard (after the kmax line) to map the processes to the correct TH1s in this file. Hint: you will need a different line for the signal process. Compared to the counting experiment we must also consider the effect of uncertainties that change the shape of the distribution. Some, like CMS_eff_t_highpt , were present before, as it has both a shape and normalisation effect. Others are primarily shape effects so were not included before. Add the following shape uncertainties: top_pt_ttbar_shape affecting ttbar ,the tau energy scale uncertainties CMS_scale_t_1prong0pi0_13TeV , CMS_scale_t_1prong1pi0_13TeV and CMS_scale_t_3prong0pi0_13TeV affecting all processes except jetFakes , and CMS_eff_t_highpt also affecting the same processes. Once this is done you can run FitDiagnostics on the datacard. From now on we will convert the text datacard into a RooFit workspace ourselves instead of combine doing it internally every time we run. This is a good idea for more complex analyses since the conversion step can take a notable amount of time. For this we use the text2workspace.py command: text2workspace.py datacard_shape_part2.txt -m 800 -o workspace_part2.root And then we can use this as input to combine instead of the text datacard: combine -M FitDiagnostics --rMin -20 --rMax 20 workspace_part2.root -m 800 Note that we have changed the range of the parameter of interest, r, by using --rMin and --rMax . The default range is 0-20. Tasks and Questions: - Run FitDiagnostics on this new workspace and re-run the diffNuisances script on the output fitDiagnostics.root file. - Which parameter has the largest pull? Which has the tightest constraint? - Should we be concerned when a parameter is more strongly constrained than the input uncertainty (i.e. \\frac{\\sigma}{\\sigma_I}<1.0 \\frac{\\sigma}{\\sigma_I}<1.0 )? C: MC statistical uncertainties So far there is an important source of uncertainty we have neglected. Our estimates of the backgrounds come either from MC simulation or from sideband regions in data, and in both cases these estimates are subject to a statistical uncertainty on the number of simulated or data events. In principle we should include an independent statistical uncertainty for every bin of every process in our model. It's important to note that combine/RooFit does not take this into account automatically - statistical fluctuations of the data are implicitly accounted for in the likelihood formalism, but statistical uncertainties in the model must be specified by us. One way to implement these uncertainties is to create a shape uncertainty for each bin of each process, in which the up and down histograms have the contents of the bin shifted up and down by the 1\\sigma 1\\sigma uncertainty. However this makes the likelihood evaluation computationally inefficient, and can lead to a large number of nuisance parameters in more complex models. Instead we will use a feature in combine called autoMCStats that creates these automatically from the datacard, and uses a technique called \"Barlow-Beeston-lite\" to reduce the number of systematic uncertainties that are created. This works on the assumption that for high MC event counts we can model the uncertainty with a Gaussian distribution. Given the uncertainties in different bins are independent, the total uncertainty of several processes in a particular bin is just the sum of N N individual Gaussians, which is itself a Gaussian distribution. So instead of N N nuisance parameters we need only one. This breaks down when the number of events is small and we are not in the Gaussian regime. The autoMCStats tool has a threshold setting on the number of events below which the the Barlow-Beeston-lite approach is not used, and instead a Poisson PDF is used to model per-process uncertainties in that bin. After reading the full documentation on autoMCStats here , add the corresponding line to your datacard. Start by setting a threshold of 0, i.e. [channel] autoMCStats 0 , to force the use of Barlow-Beeston-lite in all bins. Tasks and questions: Check how much the cross section measurement and uncertainties change using FitDiagnostics . Part 3: Further investigation of shape-based models A: Nuisance parameter impacts It is often useful to examine in detail the effects the systematic uncertainties have on the signal strength measurement. This is often referred to as calculating the \"impact\" of each uncertainty. What this means is to determine the shift in the signal strength, with respect to the best-fit, that is induced if a given nuisance parameter is shifted by its \\pm1\\sigma \\pm1\\sigma post-fit uncertainty values. If the signal strength shifts a lot, it tells us that it has a strong dependency on this systematic uncertainty. In fact, what we are measuring here is strongly related to the correlation coefficient between the signal strength and the nuisance parameter. The MultiDimFit method has an algorithm for calculating the impact for a given systematic: --algo impact -P [parameter name] , but it is typical to use a higher-level script, combineTool.py (part of the CombineHarvester package you checked out at the beginning) to automatically run the impacts for all parameters. Full documentation on this is given here . There is a three step process for running this. We will demonstrate this with a similar analysis to what we were using before, but at a lower mass point (200 GeV) as this region is more sensitive to the background uncertainties. The datacard is provided for you (datacard_part3.txt). Make the workspace first to be able to perform the steps for the impacts. First we perform an initial fit for the signal strength and its uncertainty: combineTool.py -M Impacts -d workspace_part3.root -m 200 --rMin -1 --rMax 2 --robustFit 1 --doInitialFit Then we run the impacts for all the nuisance parameters: combineTool.py -M Impacts -d workspace_part3.root -m 200 --rMin -1 --rMax 2 --robustFit 1 --doFits This will take a little bit of time. When finished we collect all the output and convert it to a json file: combineTool.py -M Impacts -d workspace_part3.root -m 200 --rMin -1 --rMax 2 --robustFit 1 --output impacts.json We can then make a plot showing the pulls and parameter impacts, sorted by the largest impact: plotImpacts.py -i impacts.json -o impacts Tasks and questions: Identify the most important uncertainties using the impacts tool. B: Post-fit distributions Another thing the FitDiagnostics mode can help us with is visualising the distributions we are fitting, and the uncertainties on those distributions, both before the fit is performed (\"pre-fit\") and after (\"post-fit\"). The pre-fit can give us some idea of how well our uncertainties cover any data-MC discrepancy, and the post-fit if discrepancies remain after the fit to data (as well as possibly letting us see the presence of a significant signal!). To produce these distributions add the --saveShapes and --saveWithUncertainties options when running FitDiagnostics : combine -M FitDiagnostics workspace_part3.root -m 200 --rMin -1 --rMax 2 --saveShapes --saveWithUncertainties Combine will produce pre- and post-fit distributions (for fit_s and fit_b) in the fitdiagnostics.root output file: Tasks and questions: Make a plot showing the expected background and signal contributions using the output from FitDiagnostics - do this for both the pre-fit and post-fit. You will find a script postFitPlot.py in the combinetutorial-2020 repository that can help you get started. The bin errors on the TH1s in the fitdiagnostics file are determined from the systematic uncertainties. In the post-fit these take into account the additional constraints on the nuisance parameters as well as any correlations. Why is the uncertainty on the post-fit so much smaller than on the pre-fit? Part 4: Setting limits You can watch the introductory presentation to part 4 here A: Asymptotic limits As we are searching for a signal process that does not exist in the standard model, it's natural to set an upper limit on the cross section times branching fraction of the process (assuming our dataset does not contain a significant discovery of new physics). Combine has dedicated method for calculating upper limits. The most commonly used one is AsymptoticLimits , which implements the CLs criterion and uses the profile likelihood ratio as the test statistic. As the name implies, the test statistic distributions are determined analytically in the asymptotic approximation, so there is no need for more time-intensive toy throwing and fitting. First we will go back to the first datacard we looked at, which is a one-bin counting experiment again. Try running the following command: combine -M AsymptoticLimits datacard_counting_part2.txt You should see the results of the observed and expected limit calculations printed to the screen. They are also saved to the usual output file. The expected limit is given under the background-only hypothesis. The median value under this hypothesis as well as the quantiles needed to give the 68% and 95% intervals are also calculated. These are all the ingredients needed to produce the standard limit plots you will see in many CMS results, for example the \\sigma \\times \\mathcal{B} \\sigma \\times \\mathcal{B} limits for the \\text{bb}\\phi\\rightarrow\\tau\\tau \\text{bb}\\phi\\rightarrow\\tau\\tau process: In this case we only computed the values for one signal mass hypothesis, indicated by a red dashed line. Tasks and questions: Try changing the values of some uncertainties (up or down, or removing them altogether) - how do the expected and observed limits change? Now try changing the number of observed events. The observed limit will naturally change, but the expected does too - why might this be? Most analyses are developed and optimised while we are \"blind\" to the region of data where we expect our signal to be. With AsymptoticLimits we can choose just to run the expected limit ( --run expected ), so as not to calculate the observed. However the data is still used, even for the expected, since in the frequentist approach a background-only fit to the data is performed to define the Asimov dataset used to calculate the expected limits. To skip this fit to data and use the pre-fit state of the model the option --run blind or --noFitAsimov can be used. Task: Compare the expected limits calculated with --run expected and --run blind . Why are they different? A more general way of blinding is to use combine's toy and Asimov dataset generating functionality. You can read more about this here . These options can be used with any method in combine, not just AsymptoticLimits . Task: Calculate a blind limit by generating a background-only Asimov with the -t -1 option instead of using the AsymptoticLimits specific options. You should find the observed limit is the same (up to numerical differences) as the expected. Then see what happens if you inject a signal into the Asimov dataset using the --expectSignal [X] option. B: Computing limits with toys Now we will look at computing limits without the asymptotic approximation, so instead using toy datasets to determine the test statistic distributions under the signal+background and background-only hypotheses. This can be necessary if we are searching for signal in bins with a small number of events expected. In combine we will use the HybridNew method to calculate limits using toys. This mode is capable of calculating limits with several different test statistics and with fine-grained control over how the toy datasets are generated internally. To calculate LHC-style profile likelihood limits (i.e. the same as we did with the asymptotic) we set the option --LHCmode LHC-limits . You can read more about the different options in the Combine documentation . Run the following command: combine -M HybridNew datacard_counting_part2.txt --LHCmode LHC-limits -n .part4B --saveHybridResult --fork 0 In contrast to AsymptoticLimits this will only determine the observed limit, and will take around five minutes. There will not be much output to the screen while combine is running. You can add the option -v 1 to get a better idea of what is going on. You should see combine stepping around in r , trying to find the value for which CLs = 0.05, i.e. the 95% CL limit. The --saveHybridResult option will cause the test statistic distributions that are generated at each tested value of r to be saved in the output ROOT file. To get an expected limit add the option --expectedFromGrid X , where X is the desired quantile, e.g. for the median: combine -M HybridNew datacard_counting_part2.txt --LHCmode LHC-limits -n .part4B --saveHybridResult --fork 0 --expectedFromGrid 0.500 Calculate the median expected limit and the 68% range. The 95% range could also be done, but note it will take much longer to run the 0.025 quantile. While combine is running you can move on to the next steps below. Tasks and questions: - In contrast to AsymptoticLimits , with HybridNew each limit comes with an uncertainty. What is the origin of this uncertainty? - How good is the agreement between the asymptotic and toy-based methods? - Why does it take longer to calculate the lower expected quantiles (e.g. 0.025, 0.16)? Think about how the statistical uncertainty on the CLs value depends on CLs+b and CLb. Next plot the test statistic distributions stored in the output file: python $CMSSW_BASE/src/HiggsAnalysis/CombinedLimit/test/plotTestStatCLs.py --input higgsCombine.part4B.HybridNew.mH120.root --poi r --val all --mass 120 This produces a new ROOT file cls_qmu_distributions.root containing the plots, to save them as pdf/png files run this small script and look at the resulting figures: python printTestStatPlots.py cls_qmu_distributions.root Advanced exercises These distributions can be useful in understanding features in the CLs limits, especially in the low statistics regime. To explore this, try reducing the observed and expected yields in the datacard by a factor of 10, and rerun the above steps to compare the observed and expected limits with the asymptotic approach, and plot the test statistic distributions. Tasks and questions: Is the asymptotic limit still a good approximation? You might notice that the test statistic distributions are not smooth but rather have several \"bump\" structures? Where might this come from? Try reducing the size of the systematic uncertainties to make them more pronounced. Note that for more complex models the fitting time can increase significantly, making it infeasible to run all the toy-based limits interactively like this. An alternative strategy is documented here Advanced tasks If you have time, you can go back to your shape analysis from part 2B and: Compare the expected sensitivities of the counting experiment and the shape-based analysis. Which one is more sensitive? Check the pulls and constraints on a b-only and s+b asimov dataset instead. This check is required for all analyses in the Higgs PAG. It serves both as a closure test (do we fit exactly what signal strength we input?) and a way to check whether there are any infeasibly strong constraints while the analysis is still blind (typical example: something has probably gone wrong if we constrain the luminosity uncertainty to 10% of the input!) Sometimes there are problems in the fit model that aren't apparent from only fitting the Asimov dataset, but will appear when fitting randomised data. Follow the exercise on toy-by-toy diagnostics here to explore the tools available for this.","title":"Exercise"},{"location":"tutorial2020/exercise/#getting-started","text":"We need to set up a new CMSSW area and checkout the combine package: export SCRAM_ARCH=slc7_amd64_gcc700 cmsrel CMSSW_10_2_13 cd CMSSW_10_2_13/src cmsenv git clone https://github.com/cms-analysis/HiggsAnalysis-CombinedLimit.git HiggsAnalysis/CombinedLimit cd HiggsAnalysis/CombinedLimit cd $CMSSW_BASE/src/HiggsAnalysis/CombinedLimit git fetch origin git checkout v8.1.0 cd $CMSSW_BASE/src We will also make use another package, CombineHarvester , which contains some high-level tools for working with combine. The following command will download the repository and checkout just the parts of it we need for this tutorial: bash <(curl -s https://raw.githubusercontent.com/cms-analysis/CombineHarvester/master/CombineTools/scripts/sparse-checkout-https.sh) Now make sure the CMSSW area is compiled: scramv1 b clean; scramv1 b Finally we will checkout the working directory for these tutorials - this contains all the inputs needed to run the exercises below: cd $CMSSW_BASE/src git clone https://gitlab.cern.ch/adewit/combinetutorial-2020 cd combinetutorial-2020","title":"Getting started"},{"location":"tutorial2020/exercise/#session-structure","text":"These exercises are accompanied by introductory presentations given live during the tutorial mornings (and made available for later viewing for those who cannot follow them live) The introductory presentations that are scheduled appear: Before part 1 (recording available here ) Before part 1E (recording available here ) Before part 2 (recording available here ) Before part 4 (recording available here ) All presentations introduce the methods you'll learn about in the following sections and in addition the presentations in bold contain some theoretical background. If you get stuck while working on the exercises, or have specific questions about them, you can ask for help either directly in the zoom room during tutorial hours, or on this mattermost channel . The mattermost channel will remain available after the tutorial, so if you're working on the exercises at a later date you can still ask questions about them there.","title":"Session structure"},{"location":"tutorial2020/exercise/#part-1-roofit","text":"You can watch the introductory presentation here RooFit is an object-oriented analysis environment built on ROOT, with a collection of classes designed to augment ROOT for data modelling. Combine is in turn built on RooFit, so before learning about Combine, it is useful to get to grips with a few RooFit basics. We will do that in this section. We will use python syntax in this section; you can either call the commands in an interactive python session, or just put them in a .py script. Make sure to do from ROOT import * at the top of your script (or in the interactive session)","title":"Part 1: RooFit"},{"location":"tutorial2020/exercise/#a-variables","text":"In RooFit, any variable, data point, function, PDF (etc.) is represented by a c++ object. The most basic of these is the RooRealVar. Let's create one which will represent the mass of some hypothetical particle, name it, and give a hypothetical starting value and range. MH = RooRealVar(\"MH\",\"mass of the Hypothetical Boson (H-boson) in GeV\",125,120,130) MH.Print() The output of this is RooRealVar::MH = 125 L(120 - 130) So we now have a RooRealVar called MH, with default value 125 and range 120-130. We can now access the object and change for example its value: MH.setVal(130) print MH.getVal() Which should print the new value, 130. In particle detectors we typically don't observe this particle mass but usually define some observable which is sensitive to this mass. Lets assume we can detect and reconstruct the decay products of the H-boson and measure the invariant mass of those particles. We need to make another variable which represents that invariant mass. Make a RooRealVar \"mass\" with a default value of 100 GeV and a range 80-200 In a perfect world we would perfectly measure the exact mass of the particle in every single event. However, our detectors are usually far from perfect so there will be some resolution effect. Let's assume the resolution of our measurement of the invariant mass is 10 GeV (range 0-20 GeV) and call it \"sigma\" Create this variable \"sigma\"","title":"A: Variables"},{"location":"tutorial2020/exercise/#b-functions-and-pdfs","text":"More exotic variables can be constructed out of these RooRealVar s using RooFormulaVar s. For example, suppose we wanted to make a function out of the variables which represented the relative resolution as a function of the hypothetical mass MH. func = RooFormulaVar(\"R\",\"@0/@1\",RooArgList(sigma,mass)) func.Print(\"v\") The main objects we are interested in using from RooFit are probability density functions (PDFs). We can construct the PDF f(m|M_H,\\sigma) f(m|M_H,\\sigma) as a Gaussian shape, a RooGaussian : gauss = RooGaussian(\"gauss\",\"f(m|M_{H},#sigma)\",mass,MH,sigma) gauss.Print(\"v\") Notice how the gaussian PDF, like the RooFormulaVar depends on our RooRealVar objects, these are its servers. Its evaluation will depend on their values. The main difference between PDFs and functions in RooFit is that PDFs are automatically normalised to unity, hence they represent a probability density, you don't need to normalise yourself. Let's plot it for the different values of M_H M_H .","title":"B: Functions and PDFs"},{"location":"tutorial2020/exercise/#c-plotting","text":"First we need to make a canvas and a RooPlot object. This object needs to know what observable is going to be on the x-axis: can = TCanvas() plot = mass.frame() Now we can plot the gaussian PDF for several mass values. We set M_H M_H to 130 GeV earlier on, so to plot this PDF for M_H M_H of 130 GeV we just do gauss.plotOn(plot,RooFit.LineColor(kGreen+3)) Where we're using kGreen+3 as the line colour. Notice that we need to tell RooFit on which RooPlot object we want to plot our PDF, even if we only have one such object. Let's also plot the PDF with M_H M_H at 120 GeV, in blue, and with M_H M_H 125 GeV, in red: MH.setVal(120) gauss.plotOn(plot,RooFit.LineColor(kBlue)) MH.setVal(125) gauss.plotOn(plot,RooFit.LineColor(kRed)) Finally, let's try adding this PDF for M_H M_H at 115 GeV in bright green. We'll use a dashed line for this one. Afterwards we'll draw the plot and save the canvas. MH.setVal(115) gauss.plotOn(plot,RooFit.LineColor(kGreen),RooFit.LineStyle(2)) plot.Draw() can.Draw() can.SaveAs(\"gaussians.pdf\") Why do the blue and bright green lines overlap?","title":"C: Plotting"},{"location":"tutorial2020/exercise/#d-workspaces","text":"Next we'll look at how to store RooFit objects and links between them. We can do this with a RooWorkspace . Let's create one and import our PDF, and the RooFormulaVar we created earlier: w = RooWorkspace(\"w\") getattr(w,'import')(gauss) getattr(w,'import')(func) w.writeToFile(\"workspace.root\") Notice that the RooRealVar s we created are also getting imported into the workspace as our gaussian PDF depends on all three of them. Now we can open the file that we've created in root: root workspace.root .ls You should see that our workspace, named w is in the file: TFile** workspace.root TFile* workspace.root KEY: RooWorkspace w;1 w KEY: TProcessID ProcessID0;1 94b05638-d0c4-11ea-a5b3-84978a89beef We can inspect its contents: w->Print() Show output RooWorkspace(w) w contents variables --------- (MH,m,resolution) p.d.f.s ------- RooGaussian::gauss[ x=m mean=MH sigma=resolution ] = 0.135335 functions -------- RooFormulaVar::R[ actualVars=(resolution,m) formula=\"@0/@1\" ] = 0.1 Now we can check the properties of some of the objects, for example: w->pdf(\"gauss\")->Print(\"v\") Show output --- RooAbsArg --- Value State: DIRTY Shape State: DIRTY Attributes: Address: 0x63b62e0 Clients: Servers: (0x6781400,V-) RooRealVar::m \"m (GeV)\" (0x687edd0,V-) RooRealVar::MH \"mass of the Hypothetical Boson (H-boson) in GeV\" (0x6795740,V-) RooRealVar::resolution \"#sigma\" Proxies: x -> m mean -> MH sigma -> resolution --- RooAbsReal --- Plot label is \"gauss\" --- RooAbsPdf --- Cached value = 0 We can also check and change the values of our RooRealVar s w->var(\"MH\")->Print() w->var(\"MH\")->setVal(123) w->var(\"MH\")->getVal() Gives: RooRealVar::MH = 120 L(120 - 130) and (double) 123.00000 Note that if you close the file containing the workspace, open it again and call w->var(\"MH\")->getVal() You will get the value as set when the workspace was created again, in our case that's 120 GeV: (double) 120.00000 Have a look at how to use the workspace syntax in this tutorial and try to create the gaussian pdf and the formula directly in a workspace","title":"D: Workspaces"},{"location":"tutorial2020/exercise/#e-fitting","text":"You can watch the introductory presentation to part 1E here Note: from this point on if you were using the interactive python session, you should switch to a python script. Of course we don't just want to create variables and pdfs, we want to do fits with them. Models that we define in combine make use of likelihood fitting, so let's build a likelihood \\mathcal{L} \\propto p(\\text{data}|\\text{parameters}) \\mathcal{L} \\propto p(\\text{data}|\\text{parameters}) where our parameters are parameters of interest, \\mu \\mu and nuisance parameters \\theta \\theta . The nuisance parameters are constrained by external measurements, so we add constraint terms \\pi(\\vec{\\theta}_0|\\vec{\\theta}) \\pi(\\vec{\\theta}_0|\\vec{\\theta}) So we have \\mathcal{L} \\propto p(\\text{data}|\\mu,\\vec{\\theta})\\cdot \\pi(\\vec{\\theta}_0|\\vec{\\theta}) \\mathcal{L} \\propto p(\\text{data}|\\mu,\\vec{\\theta})\\cdot \\pi(\\vec{\\theta}_0|\\vec{\\theta}) let's try to build the likelihood by hand for a 1-bin counting experiment. The data is the number of observed events N N , and the probability is just a poisson probability p(N|\\lambda) = \\frac{\\lambda^N e^{-\\lambda}}{N!} p(N|\\lambda) = \\frac{\\lambda^N e^{-\\lambda}}{N!} , where \\lambda \\lambda is the number of events expected in our signal+background model: \\lambda = \\mu\\cdot s(\\vec{\\theta}) + b(\\vec{\\theta}) \\lambda = \\mu\\cdot s(\\vec{\\theta}) + b(\\vec{\\theta}) . In the expression, s and b are the numbers of expected signal- and background events, which both depend on the nuisance parameters. Let's start by building a simple likelihood function with one signal process and one background process. We'll assume there are no nuisance parameters for now. The number of observed events in data is 15, the expected number of signal events is 5 and the expected number of background events 8.1. It's easiest to use the RooFit workspace factory to build our model from ROOT import * w = RooWorkspace(\"w\") We need to create an expression for the number of events in our model, \\mu s +b \\mu s +b : w.factory('expr::n(\"mu*s +b\", mu[1.0,0,4], s[5],b[8.1])') Now we can build the likelihood which is just our poisson pdf: w.factory('Poisson::poisN(N[15],n)') To find the best-fit value for our parameter of interest \\mu \\mu we need to maximize the likelihood. In practice it's actually easier to minimize the N egative l og of the l ikelihood, or NLL: w.factory('expr::NLL(\"-log(@0)\",poisN)') We can now use the RooMinimizer to find the minimum of the NLL nll = w.function(\"NLL\") minim = RooMinimizer(nll) minim.setErrorLevel(0.5) minim.minimize(\"Minuit2\",\"migrad\") bestfitnll = nll.getVal() Notice that we need to set the error level to 0.5 to get the uncertainties (relying on Wilks' theorem!) - note that there is a more reliable way of extracting the confidence interval (explicitly rather than relying on migrad). We will discuss this a bit later in this section. Now let's add a nuisance parameter, lumi , which represents the luminosity uncertainty. It has a 2.5% effect on both the signal and the background. The parameter will be log-normally distributed: when it's 0, the normalization of the signal and background are not modified; at +1\\sigma +1\\sigma the signal and background normalizations will be multiplied by 1.025 and at -1\\sigma -1\\sigma they will be divided by 1.025. We should modify the expression for the number of events in our model: w.factory('expr::n(\"mu*s*pow(1.025,lumi) +b*pow(1.025,lumi)\", mu[1.0,0,4], s[5],b[8.1],lumi[0,-4,4])') And we add a unit gaussian constraint w.factory('Gaussian::lumiconstr(lumi,0,1)') Our full likelihood will now be w.factory('PROD::likelihood(poisN,lumiconstr)') and the NLL w.factory('expr::NLL(\"-log(@0)\",likelihood)') Which we can minimize in the same way as before. Now let's extend our model a bit. Don't hesitate to ask for help if you get stuck! Expanding on what was demonstrated above, build the likelihood for N=15 N=15 , a signal process s with expectation 5 events, a background ztt with expectation 3.7 events and a background tt with expectation 4.4 events. The luminosity uncertainty applies to all three processes. The signal process is further subject to a 5% log-normally distributed uncertainty sigth , tt is subject to a 6% log-normally distributed uncertainty ttxs , and ztt is subject to a 4% log-normally distributed uncertainty zttxs . Find the best-fit value and the associated uncertainty Also perform an explicit scan of the \\Delta \\Delta NLL ( = log of profile likelihood ratio) and make a graph of the scan. Some example code can be found below to get you started. Hint: you'll need to perform fits for different values of mu, where mu is fixed. In RooFit you can set a variable to be constant as var(\"VARNAME\").setConstant(True) From the curve that you've created by performing an explicit scan, we can extract the 68% CL interval. You can do so by eye or by writing some code to find the relevant intersections of the curve. gr = TGraph() npoints = 0 for i in range(0,60): npoints+=1 mu=0.05*i ... [perform fits for different values of mu with mu fixed] ... deltanll = ... gr.SetPoint(npoints,mu,deltanll) canv = TCanvas(); gr.Draw(\"ALP\"); canv.SaveAs(\"likelihoodscan.pdf\") Well, this is doable - but we were only looking at a simple one-bin counting experiment. This might become rather cumbersome for large models... [*] [*] We'll now switch to Combine which will make it a lot easier to set up your model and do the statistical analysis than trying to build the likelihood yourself. [*] [*] Side note - RooFit does have additional functionality to help with statistical model building, but we won't go into detail today.","title":"E: Fitting"},{"location":"tutorial2020/exercise/#part-2-measurements-of-parameters-in-combine","text":"You can watch the introductory presentation for parts 2 & 3 here For the parts of the tutorial that follow we will work with a simplified version of a real analysis, that nonetheless will have many features of the full analysis. The analysis is a search for an additional heavy neutral Higgs boson decaying to tau lepton pairs. Such a signature is predicted in many extensions of the standard model, in particular the minimal supersymmetric standard model (MSSM). You can read about the analysis in the paper here . The statistical inference makes use of a variable called the total transverse mass ( M_{\\mathrm{T}}^{\\mathrm{tot}} M_{\\mathrm{T}}^{\\mathrm{tot}} ) that provides good discrimination between the resonant high-mass signal and the main backgrounds, which have a falling distribution in this high-mass region. The events selected in the analysis are split into a several categories which target the main di-tau final states as well as the two main production modes: gluon-fusion (ggH) and b-jet associated production (bbH). One example is given below for the fully-hadronic final state in the b-tag category which targets the bbH signal:","title":"Part 2 Measurements of parameters in Combine"},{"location":"tutorial2020/exercise/#a-a-one-bin-counting-experiment","text":"We will begin by looking at a simplified version of a datacard from the MSSM \\phi\\rightarrow\\tau\\tau \\phi\\rightarrow\\tau\\tau analysis that has been converted to a one-bin counting experiment, as discussed in the introductory presentation to this section. While the full analysis considers a range of signal mass hypotheses, we will start by considering just one: m_{\\phi} m_{\\phi} =800GeV. Click the text below to study the datacard ( datacard_counting_part2.txt in the combinetutorial-2020 directory). It might bear some similarities to the model you just set up by hand. Show datacard imax 1 number of bins jmax 4 number of processes minus 1 kmax * number of nuisance parameters -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- bin signal_region observation 10.0 -------------------------------------------------------------------------------- bin signal_region signal_region signal_region signal_region signal_region process ttbar diboson Ztautau jetFakes bbHtautau process 1 2 3 4 0 rate 4.43803 3.18309 3.7804 1.63396 0.711064 -------------------------------------------------------------------------------- CMS_eff_b lnN 1.02 1.02 1.02 - 1.02 CMS_eff_t lnN 1.12 1.12 1.12 - 1.12 CMS_eff_t_highpt lnN 1.1 1.1 1.1 - 1.1 acceptance_Ztautau lnN - - 1.08 - - acceptance_bbH lnN - - - - 1.05 acceptance_ttbar lnN 1.005 - - - - lumi_13TeV lnN 1.025 1.025 1.025 - 1.025 norm_jetFakes lnN - - - 1.2 - xsec_Ztautau lnN - - 1.04 - - xsec_diboson lnN - 1.05 - - - xsec_ttbar lnN 1.06 - - - - The layout of the datacard is as follows: At the top are the numbers imax , jmax and kmax representing the number of bins, processes and nuisance parameters respectively. Here a \"bin\" can refer to a literal single event count as in this example, or a full distribution we are fitting, in general with many histogram bins, as we will see later. We will refer to both as \"channels\" from now on. It is possible to replace these numbers with * and they will be deduced automatically. The first line starting with bin gives a unique label to each channel, and the following line starting with observation gives the number of events observed in data. In the remaining part of the card there are several columns: each one represents one process in one channel. The first four lines labelled bin , process , process and rate give the channel label, the process label, a process identifier ( <=0 for signal, >0 for background) and the number of expected events respectively. The remaining lines describe sources of systematic uncertainty. Each line gives the name of the uncertainty, (which will become the name of the nuisance parameter inside our RooFit model), the type of uncertainty (\"lnN\" = log-normal normalisation uncertainty) and the effect on each process in each channel. E.g. a 20% uncertainty on the yield is written as 1.20. It is also possible to add a hash symbol ( # ) at the start of a line, which combine will then ignore when it reads the card. We can now run combine directly using this datacard as input. The general format for running combine is: combine -M [method] [datacard] [additional options...] By default, combine will report measurements on a parameter called r , which is the default parameter of interest (POI) that is added to the model automatically. It is a linear scaling of the normalisation of all signal processes given in the datacard, i.e. if s_{i,j} s_{i,j} is the nominal number of signal events in channel i i for signal process j j , then the normalisation of that signal in the model is given as r\\cdot s_{i,j}(\\vec{\\theta}) r\\cdot s_{i,j}(\\vec{\\theta}) , where \\vec{\\theta} \\vec{\\theta} represents the set of nuisance parameters which may also affect the signal normalisation. So r is the equivalent of the parameter \\mu \\mu we introduced in the RooFit section. We have some choice in the interpretation of r: for the measurement of a process with a well defined SM prediction we may enter this as the nominal yield in the datacard, such that r=1 r=1 corresponds to this SM expectation, whereas for setting limits on BSM processes we may choose the nominal yield to correspond to some cross section, e.g. 1 pb, such that we can interpret the limit as a cross section limit directly. In this example the signal has been normalised to a cross section times branching fraction of 1 fb. We will now explore one of the most commonly used modes of combine: FitDiagnostics . As well as allowing us to make a measurement of some physical quantity, this method is useful to gain additional information about the model and the behaviour of the fit. It performs two fits: A \"background-only\" (b-only) fit: first POI (usually \"r\") fixed to zero A \"signal+background\" (s+b) fit: all POIs are floating With the s+b fit combine will report the best-fit value of our signal strength modifier r . Two ouput files will be produced. One of these will be produced regardless of which combine method we use, and the file name depends on the options we ran with. It is of the form: higgsCombine[name].[method].mH[mass].root. The default name is Test , but you can change this with the --name (or -n ) option. The file contains a TTree called limit which stores the numerical values returned by FitDiagnostics. Note that in our case we did not set a signal mass when running combine (i.e. -m 800), so the output file just uses the default value of 120. This does not affect our result in any way though, just the label that is used on the output file. The second file that is produced is a file named fitdiagnostics.root , which contains additional information. In particular it includes two RooFitResult objects, one for the b-only and one for the s+b fit, which store the fitted values of all the nuisance parameters (NPs) and POIs as well as estimates of their uncertainties. The covariance matrix from both fits is also included, from which we can learn about the correlations between parameters. Task: Make a datacard for the simple model that you previously set up directly in RooFit Now run the FitDiagnostics method on this datacard: combine -M FitDiagnostics datacard.txt --forceRecreateNLL Note that the --forceRecreateNLL option is only needed for counting experiments; we will soon look at a shape-based analysis instead where this is not needed. There are other command line options we can supply to combine which will change its behaviour when run. You can see the full set of supported options by doing combine -h . Many options are specific to a given method, but others are more general and are applicable to all methods. Throughout this tutorial we will highlight some of the most useful options you may need to use, for example: The range on the signal strength modifier: --rMin=X and --rMax=Y : In RooFit parameters can optionally have a range specified. The implication of this is that their values cannot be adjusted beyond the limits of this range. The min and max values can be adjusted though, and we might need to do this for our POI r if the order of magnitude of our measurement is different from the default range of [0, 20] . This will be discussed again later in the tutorial. Verbosity: -v X : By default combine does not usually produce much output on the screen other the main result at the end. However, much more detailed information can be printed by setting the -v N with N larger than zero. For example at -v 3 the logs from the minimizer, Minuit, will also be printed. These are very useful for debugging problems with the fit. Task: Verify that the best-fit value and uncertainty on r are the same as what you extracted from the likelihood scan you did by hand in RooFit. Now open the resulting fitDiagnostics.root file interactively and print the contents of the s+b RooFit result: root [1] fit_s->Print() Show output RooFitResult: minimized FCN value: 3.54419, estimated distance to minimum: 4.84873e-07 covariance matrix quality: Full, accurate covariance matrix Status : MINIMIZE=0 HESSE=0 Floating Parameter FinalValue +/- Error -------------------- -------------------------- lumi 1.6988e-04 +/- 9.94e-01 r 1.3792e+00 +/- 7.76e-01 sigth -7.5538e-05 +/- 9.94e-01 ttxs 3.2043e-05 +/- 9.94e-01 zttxs -2.9580e-05 +/- 9.94e-01 There are several useful pieces of information here. At the top the status codes from the fits that were performed is given. In this case we can see that two algorithms were run: MINIMIZE and HESSE , both of which returned a successful status code (0). Both of these are routines in the Minuit2 minimization package - the default minimizer used in RooFit. The first performs the main fit to the data, and the second calculates the covariance matrix at the best-fit point. It is important to always check this second step was successful and the message \"Full, accurate covariance matrix\" is printed, otherwise the parameter uncertainties can be very inaccurate, even if the fit itself was successful. Underneath this the best-fit values ( \\theta \\theta ) and symmetrised uncertainties for all the floating parameters are given. For all the constrained nuisance parameters a convention is used by which the nominal value ( \\theta_I \\theta_I ) is zero, corresponding to the mean of a Gaussian constraint PDF with width 1.0, such that the parameter values \\pm 1.0 \\pm 1.0 correspond to the \\pm 1\\sigma \\pm 1\\sigma input uncertainties. A more useful way of looking at this is to compare the pre- and post-fit values of the parameters, to see how much the fit to data has shifted and constrained these parameters with respect to the input uncertainty. The script diffNuisances.py can be used for this: python diffNuisances.py fitDiagnostics.root --all Show output name b-only fit s+b fit rho lumi +0.17, 0.99 +0.00, 0.99 -0.09 sigth +0.00, 0.99 -0.00, 0.99 -0.09 ttxs +0.22, 0.99 +0.00, 0.99 -0.07 zttxs +0.12, 0.99 -0.00, 0.99 -0.04 The numbers in each column are respectively \\frac{\\theta-\\theta_I}{\\sigma_I} \\frac{\\theta-\\theta_I}{\\sigma_I} (often called the pull , though note that more than one definition is in use for this), where \\sigma_I \\sigma_I is the input uncertainty; and the ratio of the post-fit to the pre-fit uncertainty \\frac{\\sigma}{\\sigma_I} \\frac{\\sigma}{\\sigma_I} . Question: Can you explain why none of the nuisance parameters are pulled in the s+b fit, but in the b-only fit some are? So far we've just been looking at our original single-bin counting experiment; let's make things a bit more interesting and move to a shape-based analysis","title":"A: A one-bin counting experiment"},{"location":"tutorial2020/exercise/#b-setting-up-the-datacard-for-a-shape-based-analysis","text":"Now we move to the next step: instead of a one-bin counting experiment we will fit a binned distribution. In a typical analysis we will produce TH1 histograms of some variable sensitive to the presence of signal: one for the data and one for each signal and background process. Then we add a few extra lines to the datacard to link the declared processes to these shapes which are saved in a ROOT file, for example: Show datacard imax 1 jmax 1 kmax * --------------- shapes * * simple-shapes-TH1_input.root $PROCESS $PROCESS_$SYSTEMATIC shapes sig * simple-shapes-TH1_input.root $PROCESS$MASS $PROCESS$MASS_$SYSTEMATIC --------------- bin bin1 observation 85 ------------------------------ bin bin1 bin1 process sig background process 0 1 rate 10 100 -------------------------------- lumi lnN 1.10 1.0 bgnorm lnN 1.00 1.3 alpha shape - 1 Note that as with the one-bin card, the total nominal rate of a given process must be specified in the rate line of the datacard. This should agree with the value returned by TH1::Integral . However, we can also put a value of -1 and the Integral value will be substituted automatically. There are two other differences with respect to the one-bin card: A new block of lines at the top defining how channels and processes are mapped to the histograms (more than one line can be used) In the list of systematic uncertainties some are marked as shape instead of lnN The syntax of the \"shapes\" line is: shapes [process] [channel] [file] [histogram] [histogram_with_systematics] . It is possible to use the * wildcard to map multiple processes and/or channels with one line. The histogram entries can contain the $PROCESS , $CHANNEL and $MASS place-holders which will be substituted when searching for a given (process, channel) combination. The value of $MASS is specified by the -m argument when combine. By default the observed data process name will be data_obs . Shape uncertainties can be added by supplying two additional histograms for a process, corresponding to the distribution obtained by shifting that parameter up and down by one standard deviation. These shapes will be interpolated with a 6th order polynomial for shifts below 1\\sigma 1\\sigma and linearly beyond. The normalizations are interpolated linearly in log scale just like we do for log-normal uncertainties. The final argument of the \"shapes\" line above should contain the $SYSTEMATIC place-holder which will be substituted by the systematic name given in the datacard. In the list of uncertainties the interpretation of the values for shape lines is a bit different from lnN . The effect can be \"-\" or 0 for no effect, 1 for normal effect, and possibly something different from 1 to test larger or smaller effects (in that case, the unit Gaussian is scaled by that factor before using it as parameter for the interpolation). In this section we will use a datacard corresponding to the full distribution that was shown at the start of section 1, not just the high mass region. Have a look at datacard_shape_part2.txt : this is still currently a one-bin counting experiment, however the yields are much higher since we now consider the full range of M_{\\mathrm{T}}^{\\mathrm{tot}} M_{\\mathrm{T}}^{\\mathrm{tot}} . If you run the asymptotic limit calculation on this you should find the sensitivity is significantly worse than before. The first task is to convert this to a shape analysis: the file datacard_part2.shapes.root contains all the necessary histograms, including those for the relevant shape systematic uncertainties. Add the relevant shapes lines to the top of the datacard (after the kmax line) to map the processes to the correct TH1s in this file. Hint: you will need a different line for the signal process. Compared to the counting experiment we must also consider the effect of uncertainties that change the shape of the distribution. Some, like CMS_eff_t_highpt , were present before, as it has both a shape and normalisation effect. Others are primarily shape effects so were not included before. Add the following shape uncertainties: top_pt_ttbar_shape affecting ttbar ,the tau energy scale uncertainties CMS_scale_t_1prong0pi0_13TeV , CMS_scale_t_1prong1pi0_13TeV and CMS_scale_t_3prong0pi0_13TeV affecting all processes except jetFakes , and CMS_eff_t_highpt also affecting the same processes. Once this is done you can run FitDiagnostics on the datacard. From now on we will convert the text datacard into a RooFit workspace ourselves instead of combine doing it internally every time we run. This is a good idea for more complex analyses since the conversion step can take a notable amount of time. For this we use the text2workspace.py command: text2workspace.py datacard_shape_part2.txt -m 800 -o workspace_part2.root And then we can use this as input to combine instead of the text datacard: combine -M FitDiagnostics --rMin -20 --rMax 20 workspace_part2.root -m 800 Note that we have changed the range of the parameter of interest, r, by using --rMin and --rMax . The default range is 0-20. Tasks and Questions: - Run FitDiagnostics on this new workspace and re-run the diffNuisances script on the output fitDiagnostics.root file. - Which parameter has the largest pull? Which has the tightest constraint? - Should we be concerned when a parameter is more strongly constrained than the input uncertainty (i.e. \\frac{\\sigma}{\\sigma_I}<1.0 \\frac{\\sigma}{\\sigma_I}<1.0 )?","title":"B: Setting up the datacard for a shape-based analysis"},{"location":"tutorial2020/exercise/#c-mc-statistical-uncertainties","text":"So far there is an important source of uncertainty we have neglected. Our estimates of the backgrounds come either from MC simulation or from sideband regions in data, and in both cases these estimates are subject to a statistical uncertainty on the number of simulated or data events. In principle we should include an independent statistical uncertainty for every bin of every process in our model. It's important to note that combine/RooFit does not take this into account automatically - statistical fluctuations of the data are implicitly accounted for in the likelihood formalism, but statistical uncertainties in the model must be specified by us. One way to implement these uncertainties is to create a shape uncertainty for each bin of each process, in which the up and down histograms have the contents of the bin shifted up and down by the 1\\sigma 1\\sigma uncertainty. However this makes the likelihood evaluation computationally inefficient, and can lead to a large number of nuisance parameters in more complex models. Instead we will use a feature in combine called autoMCStats that creates these automatically from the datacard, and uses a technique called \"Barlow-Beeston-lite\" to reduce the number of systematic uncertainties that are created. This works on the assumption that for high MC event counts we can model the uncertainty with a Gaussian distribution. Given the uncertainties in different bins are independent, the total uncertainty of several processes in a particular bin is just the sum of N N individual Gaussians, which is itself a Gaussian distribution. So instead of N N nuisance parameters we need only one. This breaks down when the number of events is small and we are not in the Gaussian regime. The autoMCStats tool has a threshold setting on the number of events below which the the Barlow-Beeston-lite approach is not used, and instead a Poisson PDF is used to model per-process uncertainties in that bin. After reading the full documentation on autoMCStats here , add the corresponding line to your datacard. Start by setting a threshold of 0, i.e. [channel] autoMCStats 0 , to force the use of Barlow-Beeston-lite in all bins. Tasks and questions: Check how much the cross section measurement and uncertainties change using FitDiagnostics .","title":"C: MC statistical uncertainties"},{"location":"tutorial2020/exercise/#part-3-further-investigation-of-shape-based-models","text":"","title":"Part 3: Further investigation of shape-based models"},{"location":"tutorial2020/exercise/#a-nuisance-parameter-impacts","text":"It is often useful to examine in detail the effects the systematic uncertainties have on the signal strength measurement. This is often referred to as calculating the \"impact\" of each uncertainty. What this means is to determine the shift in the signal strength, with respect to the best-fit, that is induced if a given nuisance parameter is shifted by its \\pm1\\sigma \\pm1\\sigma post-fit uncertainty values. If the signal strength shifts a lot, it tells us that it has a strong dependency on this systematic uncertainty. In fact, what we are measuring here is strongly related to the correlation coefficient between the signal strength and the nuisance parameter. The MultiDimFit method has an algorithm for calculating the impact for a given systematic: --algo impact -P [parameter name] , but it is typical to use a higher-level script, combineTool.py (part of the CombineHarvester package you checked out at the beginning) to automatically run the impacts for all parameters. Full documentation on this is given here . There is a three step process for running this. We will demonstrate this with a similar analysis to what we were using before, but at a lower mass point (200 GeV) as this region is more sensitive to the background uncertainties. The datacard is provided for you (datacard_part3.txt). Make the workspace first to be able to perform the steps for the impacts. First we perform an initial fit for the signal strength and its uncertainty: combineTool.py -M Impacts -d workspace_part3.root -m 200 --rMin -1 --rMax 2 --robustFit 1 --doInitialFit Then we run the impacts for all the nuisance parameters: combineTool.py -M Impacts -d workspace_part3.root -m 200 --rMin -1 --rMax 2 --robustFit 1 --doFits This will take a little bit of time. When finished we collect all the output and convert it to a json file: combineTool.py -M Impacts -d workspace_part3.root -m 200 --rMin -1 --rMax 2 --robustFit 1 --output impacts.json We can then make a plot showing the pulls and parameter impacts, sorted by the largest impact: plotImpacts.py -i impacts.json -o impacts Tasks and questions: Identify the most important uncertainties using the impacts tool.","title":"A: Nuisance parameter impacts"},{"location":"tutorial2020/exercise/#b-post-fit-distributions","text":"Another thing the FitDiagnostics mode can help us with is visualising the distributions we are fitting, and the uncertainties on those distributions, both before the fit is performed (\"pre-fit\") and after (\"post-fit\"). The pre-fit can give us some idea of how well our uncertainties cover any data-MC discrepancy, and the post-fit if discrepancies remain after the fit to data (as well as possibly letting us see the presence of a significant signal!). To produce these distributions add the --saveShapes and --saveWithUncertainties options when running FitDiagnostics : combine -M FitDiagnostics workspace_part3.root -m 200 --rMin -1 --rMax 2 --saveShapes --saveWithUncertainties Combine will produce pre- and post-fit distributions (for fit_s and fit_b) in the fitdiagnostics.root output file: Tasks and questions: Make a plot showing the expected background and signal contributions using the output from FitDiagnostics - do this for both the pre-fit and post-fit. You will find a script postFitPlot.py in the combinetutorial-2020 repository that can help you get started. The bin errors on the TH1s in the fitdiagnostics file are determined from the systematic uncertainties. In the post-fit these take into account the additional constraints on the nuisance parameters as well as any correlations. Why is the uncertainty on the post-fit so much smaller than on the pre-fit?","title":"B: Post-fit distributions"},{"location":"tutorial2020/exercise/#part-4-setting-limits","text":"You can watch the introductory presentation to part 4 here","title":"Part 4: Setting limits"},{"location":"tutorial2020/exercise/#a-asymptotic-limits","text":"As we are searching for a signal process that does not exist in the standard model, it's natural to set an upper limit on the cross section times branching fraction of the process (assuming our dataset does not contain a significant discovery of new physics). Combine has dedicated method for calculating upper limits. The most commonly used one is AsymptoticLimits , which implements the CLs criterion and uses the profile likelihood ratio as the test statistic. As the name implies, the test statistic distributions are determined analytically in the asymptotic approximation, so there is no need for more time-intensive toy throwing and fitting. First we will go back to the first datacard we looked at, which is a one-bin counting experiment again. Try running the following command: combine -M AsymptoticLimits datacard_counting_part2.txt You should see the results of the observed and expected limit calculations printed to the screen. They are also saved to the usual output file. The expected limit is given under the background-only hypothesis. The median value under this hypothesis as well as the quantiles needed to give the 68% and 95% intervals are also calculated. These are all the ingredients needed to produce the standard limit plots you will see in many CMS results, for example the \\sigma \\times \\mathcal{B} \\sigma \\times \\mathcal{B} limits for the \\text{bb}\\phi\\rightarrow\\tau\\tau \\text{bb}\\phi\\rightarrow\\tau\\tau process: In this case we only computed the values for one signal mass hypothesis, indicated by a red dashed line. Tasks and questions: Try changing the values of some uncertainties (up or down, or removing them altogether) - how do the expected and observed limits change? Now try changing the number of observed events. The observed limit will naturally change, but the expected does too - why might this be? Most analyses are developed and optimised while we are \"blind\" to the region of data where we expect our signal to be. With AsymptoticLimits we can choose just to run the expected limit ( --run expected ), so as not to calculate the observed. However the data is still used, even for the expected, since in the frequentist approach a background-only fit to the data is performed to define the Asimov dataset used to calculate the expected limits. To skip this fit to data and use the pre-fit state of the model the option --run blind or --noFitAsimov can be used. Task: Compare the expected limits calculated with --run expected and --run blind . Why are they different? A more general way of blinding is to use combine's toy and Asimov dataset generating functionality. You can read more about this here . These options can be used with any method in combine, not just AsymptoticLimits . Task: Calculate a blind limit by generating a background-only Asimov with the -t -1 option instead of using the AsymptoticLimits specific options. You should find the observed limit is the same (up to numerical differences) as the expected. Then see what happens if you inject a signal into the Asimov dataset using the --expectSignal [X] option.","title":"A: Asymptotic limits"},{"location":"tutorial2020/exercise/#b-computing-limits-with-toys","text":"Now we will look at computing limits without the asymptotic approximation, so instead using toy datasets to determine the test statistic distributions under the signal+background and background-only hypotheses. This can be necessary if we are searching for signal in bins with a small number of events expected. In combine we will use the HybridNew method to calculate limits using toys. This mode is capable of calculating limits with several different test statistics and with fine-grained control over how the toy datasets are generated internally. To calculate LHC-style profile likelihood limits (i.e. the same as we did with the asymptotic) we set the option --LHCmode LHC-limits . You can read more about the different options in the Combine documentation . Run the following command: combine -M HybridNew datacard_counting_part2.txt --LHCmode LHC-limits -n .part4B --saveHybridResult --fork 0 In contrast to AsymptoticLimits this will only determine the observed limit, and will take around five minutes. There will not be much output to the screen while combine is running. You can add the option -v 1 to get a better idea of what is going on. You should see combine stepping around in r , trying to find the value for which CLs = 0.05, i.e. the 95% CL limit. The --saveHybridResult option will cause the test statistic distributions that are generated at each tested value of r to be saved in the output ROOT file. To get an expected limit add the option --expectedFromGrid X , where X is the desired quantile, e.g. for the median: combine -M HybridNew datacard_counting_part2.txt --LHCmode LHC-limits -n .part4B --saveHybridResult --fork 0 --expectedFromGrid 0.500 Calculate the median expected limit and the 68% range. The 95% range could also be done, but note it will take much longer to run the 0.025 quantile. While combine is running you can move on to the next steps below. Tasks and questions: - In contrast to AsymptoticLimits , with HybridNew each limit comes with an uncertainty. What is the origin of this uncertainty? - How good is the agreement between the asymptotic and toy-based methods? - Why does it take longer to calculate the lower expected quantiles (e.g. 0.025, 0.16)? Think about how the statistical uncertainty on the CLs value depends on CLs+b and CLb. Next plot the test statistic distributions stored in the output file: python $CMSSW_BASE/src/HiggsAnalysis/CombinedLimit/test/plotTestStatCLs.py --input higgsCombine.part4B.HybridNew.mH120.root --poi r --val all --mass 120 This produces a new ROOT file cls_qmu_distributions.root containing the plots, to save them as pdf/png files run this small script and look at the resulting figures: python printTestStatPlots.py cls_qmu_distributions.root","title":"B: Computing limits with toys"},{"location":"tutorial2020/exercise/#advanced-exercises","text":"These distributions can be useful in understanding features in the CLs limits, especially in the low statistics regime. To explore this, try reducing the observed and expected yields in the datacard by a factor of 10, and rerun the above steps to compare the observed and expected limits with the asymptotic approach, and plot the test statistic distributions. Tasks and questions: Is the asymptotic limit still a good approximation? You might notice that the test statistic distributions are not smooth but rather have several \"bump\" structures? Where might this come from? Try reducing the size of the systematic uncertainties to make them more pronounced. Note that for more complex models the fitting time can increase significantly, making it infeasible to run all the toy-based limits interactively like this. An alternative strategy is documented here","title":"Advanced exercises"},{"location":"tutorial2020/exercise/#advanced-tasks","text":"If you have time, you can go back to your shape analysis from part 2B and: Compare the expected sensitivities of the counting experiment and the shape-based analysis. Which one is more sensitive? Check the pulls and constraints on a b-only and s+b asimov dataset instead. This check is required for all analyses in the Higgs PAG. It serves both as a closure test (do we fit exactly what signal strength we input?) and a way to check whether there are any infeasibly strong constraints while the analysis is still blind (typical example: something has probably gone wrong if we constrain the luminosity uncertainty to 10% of the input!) Sometimes there are problems in the fit model that aren't apparent from only fitting the Asimov dataset, but will appear when fitting randomised data. Follow the exercise on toy-by-toy diagnostics here to explore the tools available for this.","title":"Advanced tasks"}]}